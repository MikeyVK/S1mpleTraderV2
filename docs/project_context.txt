--- START FILE: GEMINI.md ---
# S1mpleTrader V2 - AI Assistent Instructies

Hallo! Ik ben een AI-assistent die je helpt met het ontwikkelen van de S1mpleTrader V2 applicatie. Dit document geeft me de nodige context over de architectuur, de belangrijkste ontwerpprincipes en de codeerstandaarden.

## 1. Visie & Kernprincipes

Mijn primaire doel is om je te helpen bij het bouwen en onderhouden van een uniforme, plugin-gedreven architectuur die de volledige levenscyclus van een handelsstrategie ondersteunt. Ik houd me aan de volgende vier kernprincipes:

* **Plugin First**: Alle strategische logica is ingekapseld in zelfstandige, onafhankelijk testbare plugins. Dit is de kern van het systeem.
* **Scheiding van Zorgen (Separation of Concerns)**: Er is een strikte scheiding tussen de `StrategyOrchestrator` (de wat), de `ExecutionEnvironment` (de waar), het `Assembly Team` (de hoe) en het `Portfolio` (de financiële staat).
* **Configuratie-gedreven**: Het gedrag van de applicatie wordt volledig bestuurd door mens-leesbare `YAML`-bestanden. De code is de motor, de configuratie is de bestuurder.
* **Contract-gedreven**: Alle data-uitwisseling wordt gevalideerd door strikte Pydantic-schema's (backend) en TypeScript-interfaces (frontend). Dit zorgt voor voorspelbaarheid en type-veiligheid.

## 2. Architectuur Overzicht

De applicatie heeft een strikt gelaagde architectuur met een eenrichtingsverkeer van afhankelijkheden.

+-------------------------------------------------------------+
|  Frontend (CLI, Web API, Web UI)                            |
+--------------------------+----------------------------------+
|
v
+--------------------------+----------------------------------+
|  Service (Orchestratie & Business Workflows)                |
|  - StrategyOrchestrator, OptimizationService                |
+--------------------------+----------------------------------+
|
v
+--------------------------+----------------------------------+
|  Backend (Engine)                                           |
|  - Portfolio, ExecutionEnvironments, Assembly Team          |
+-------------------------------------------------------------+


* **Backend (`/backend`)**: De "engine". Bevat alle kernlogica en is ontworpen als een onafhankelijke library.
* **Service (`/services`)**: De "lijm". Orkestreert backend-componenten tot complete business workflows. Hier leeft de `StrategyOrchestrator`.
* **Frontend (`/frontends`)**: De gebruikersinterface (Web UI, API, CLI).

## 3. De 6-Fasen Quant Workflow

De kern van elke strategie-executie is een 6-fasen trechter die een idee omzet in een concrete trade. Ik moet deze flow begrijpen en respecteren bij het schrijven van code.

1.  **Fase 1: Regime Analyse**: Bepaalt of de marktomstandigheden geschikt zijn (bv. trending).
2.  **Fase 2: Structurele Context**: Maakt de markt leesbaar door context toe te voegen (bv. marktstructuur, trends).
3.  **Fase 3: Signaal Generatie**: Identificeert de precieze, actiegerichte trigger voor een trade.
4.  **Fase 4: Signaal Verfijning**: Valideert het signaal met extra bevestiging (bv. volume).
5.  **Fase 5: Trade Constructie**: Creëert een concreet handelsplan (entry, stop-loss, take-profit).
6.  **Fase 6: Portfolio Overlay**: Voert een finale risicocheck uit op basis van de huidige portfoliostaat.

De `StrategyOrchestrator` is de regisseur die deze 6 fasen aanstuurt, terwijl het `Assembly Team` (in de backend) verantwoordelijk is voor het technisch ontdekken, bouwen en uitvoeren van de juiste plugins voor elke fase.

## 4. Anatomie van een Plugin

Plugins zijn de fundamentele bouwstenen. Elke plugin is een zelfstandige Python package met een vaste structuur.

* `plugins/[plugin_naam]/`:
    * `plugin_manifest.yaml`: De "ID-kaart" die de plugin vindbaar maakt. Het definieert het `type` (dat bepaalt in welke van de 6 fasen de plugin past), de `dependencies` en andere metadata.
    * `worker.py`: Bevat de Python-klasse met de daadwerkelijke businesslogica.
    * `schema.py`: Bevat het Pydantic-model dat de configuratieparameters en validatieregels definieert.
    * `state.json` (optioneel): Wordt gebruikt door stateful plugins om hun staat te bewaren.

## 5. Codeerstandaarden & Best Practices

Ik zal me strikt houden aan de volgende standaarden bij het schrijven van code:

1.  **Code Stijl**:
    * Alle Python-code moet **PEP 8 compliant** zijn.
    * **Volledige Type Hinting** is verplicht.
    * Commentaar en docstrings zijn in het **Engels**.
    * Gebruik **Google Style Python Docstrings** voor alle functies en klassen.

2.  **Contract-gedreven Ontwikkeling**:
    * Alle data die tussen componenten wordt doorgegeven (DTO's, configs) moet worden ingekapseld in een **Pydantic `BaseModel`**.

3.  **Logging**:
    * De primaire output voor analyse is een gestructureerd **`run.log.json`**-bestand.
    * Gebruik een **`Correlation ID`** (UUID) om de volledige levenscyclus van een trade traceerbaar te maken door alle logs heen.

4.  **Testen**:
    * Code zonder tests is incompleet. Elke plugin is **verplicht** om een `tests/test_worker.py` te hebben.

5.  **Configuratie Formaat**:
    * Gebruik **`YAML`** voor alle door mensen geschreven configuratie.
    * Gebruik **`JSON`** voor machine-naar-machine data-uitwisseling (bv. API's, state-bestanden).

## 6. Snelle Referentie: Kernterminologie

* **Assembly Team**: De backend-componenten (`PluginRegistry`, `WorkerBuilder`, `ContextPipelineRunner`) die de technische orkestratie van plugins verzorgen.
* **Run**: Een `YAML`-bestand (`run_schema.yaml`) dat een complete strategie-configuratie beschrijft.
* **DTO (Data Transfer Object)**: Een Pydantic-model (`Signal`, `Trade`) dat als strikt contract dient voor data-uitwisseling.
* **ExecutionEnvironment**: De backend-laag die de "wereld" definieert waarin een strategie draait (`Backtest`, `Paper`, `Live`).
* **StrategyOrchestrator**: De "regisseur" in de Service-laag die de 6-fasen trechter uitvoert voor één enkele run.

Door deze principes en structuren te volgen, help ik je om een consistente, robuuste en onderhoudbare codebase te bouwen. Laten we beginnen!

--- END FILE: GEMINI.md ---

--- START FILE: requirements.txt ---
# requirements.txt

# --- Data & Analysis ---
# Kernbibliotheken voor dataverwerking en analyse.
pandas
pandas-stubs
numpy
scipy
pyarrow
openpyxl

# --- Configuration ---
# Voor het inlezen van YAML-configuratiebestanden.
PyYAML

# --- Web & API Server ---
# Benodigdheden voor de web-interface en de API.
fastapi
uvicorn[standard]
python-multipart
Jinja2

# --- User Interface (CLI) ---
# Tools voor de command-line interface.
tqdm
rich
prompt-toolkit

# --- Code Quality & Contracts ---
# Voor het afdwingen van code-stijl en datavalidatie.
pylint
pydantic

# --- Testing ---
# Framework en tools voor het schrijven en uitvoeren van tests.
pytest
pytest-mock

# --- Visualization ---
# Voor het genereren van grafieken en visuele rapportages.
plotly

# --- Documentation Generation ---
# Tools voor het genereren van de projectdocumentatie.
sphinx
sphinx-rtd-theme
--- END FILE: requirements.txt ---

--- START FILE: run_backtest_cli.py ---
# run_backtest_cli.py
"""
Entrypoint: For automated (headless) runs
"""

--- END FILE: run_backtest_cli.py ---

--- START FILE: run_supervisor.py ---
# run_supervisor.py
"""
Entrypoint: Starts the live trading supervisor
"""

--- END FILE: run_supervisor.py ---

--- START FILE: run_web.py ---
# run_web.py
"""
Entrypoint: Starts the Web UI and API
"""

--- END FILE: run_web.py ---

--- START FILE: __init__.py ---

--- END FILE: __init__.py ---

--- START FILE: .pytest_cache/README.md ---
# pytest cache directory #

This directory contains data from the pytest's cache plugin,
which provides the `--lf` and `--ff` options, as well as the `cache` fixture.

**Do not** commit this to version control.

See [the docs](https://docs.pytest.org/en/stable/how-to/cache.html) for more information.

--- END FILE: .pytest_cache/README.md ---

--- START FILE: backend/__init__.py ---
# backend/__init__.py
"""
Exposes the public API of the Backend package.
"""
__all__ = [
    # from .core
    "StrategyEngine",
    "Portfolio",
    "BaseStrategyWorker",
    "ContextRecorder",
    # from .dtos
    "Signal",
    "EntrySignal",
    "RiskDefinedSignal",
    "TradePlan",
    "RoutedTradePlan",
    "CriticalEvent",
    "ExecutionDirective",
    "EngineCycleResult",
    "ClosedTrade",
    "TradingContext",
    "BacktestResult",
    # from .environments
    "BacktestEnvironment",
#    "LiveEnvironment",
#    "PaperEnvironment",
    # from .assembly
    "ContextBuilder",
    "DependencyValidator",
    "PluginRegistry",
    "WorkerBuilder",
]

from .core import (
    StrategyEngine,
    Portfolio,
    BaseStrategyWorker,
    ContextRecorder,
)
from .dtos import (
    Signal,
    EntrySignal,
    RiskDefinedSignal,
    TradePlan,
    RoutedTradePlan,
    CriticalEvent,
    ExecutionDirective,
    EngineCycleResult,
    ClosedTrade,
    TradingContext,
    BacktestResult,
)
from .environments import (
    BacktestEnvironment,
#    LiveEnvironment,
#    PaperEnvironment,
)
from .assembly import (
    ContextBuilder,
    DependencyValidator,
    PluginRegistry,
    WorkerBuilder,
)

--- END FILE: backend/__init__.py ---

--- START FILE: backend/assembly/context_builder.py ---
# backend/assembly/context_builder.py
"""
Contains the ContextBuilder, responsible for executing a sequence of
context-providing plugins to enrich a DataFrame.

@layer: Backend (Assembly)
@dependencies: [pandas]
@responsibilities:
    - Sequentially applies a list of instantiated context workers to a DataFrame.
    - Ensures the original DataFrame is not modified (works on a copy).
    - Returns the final, enriched DataFrame.
"""
from typing import List

import pandas as pd

from backend.core.interfaces import ContextWorker

class ContextBuilder:
    """Executes a pipeline of context workers to enrich a DataFrame."""

    def build(self,
              initial_df: pd.DataFrame,
              context_pipeline: List[ContextWorker]
        ) -> pd.DataFrame:
        """
        Applies a list of context workers sequentially to a DataFrame.

        This method takes a starting DataFrame and a list of worker objects
        (which are expected to have a 'process' method). It creates a copy of
        the DataFrame and then passes it through each worker in order, with the
        output of one worker becoming the input for the next.

        Args:
            initial_df (pd.DataFrame): The raw OHLCV DataFrame.
            context_pipeline (List[object]): An ordered list of instantiated
                                             context worker objects.

        Returns:
            pd.DataFrame: The final, enriched DataFrame after all workers
                          have been executed.
        """
        # Werk altijd op een kopie om onverwachte bijeffecten te voorkomen.
        enriched_df = initial_df.copy()

        for worker in context_pipeline:
            # We gaan ervan uit dat elke 'worker' een .process(df) methode heeft.
            # De test die we hebben geschreven, valideert deze aanname.
            enriched_df = worker.process(enriched_df)

        return enriched_df

--- END FILE: backend/assembly/context_builder.py ---

--- START FILE: backend/assembly/dependency_validator.py ---
# backend/assembly/dependency_validator.py
"""
Contains the DependencyValidator, responsible for ensuring the integrity
of a context pipeline before execution.

@layer: Backend (Assembly)
@dependencies: [.plugin_registry]
@responsibilities:
    - Validates that the data dependencies of each plugin in a sequence are met
      by the outputs of the preceding plugins.
"""
from typing import List
from backend.assembly.plugin_registry import PluginRegistry

class DependencyValidator:
    """Validates the dataflow integrity of a context worker pipeline."""

    def __init__(self, plugin_registry: PluginRegistry):
        """Initializes the DependencyValidator.

        Args:
            plugin_registry (PluginRegistry): The registry to fetch manifests from.
        """
        self._registry = plugin_registry

    def validate(self, context_pipeline: List[str]) -> bool:
        """
        Validates a sequential pipeline of context workers.

        It checks if the `dependencies` of each worker are satisfied by the
        initial `DataFrame` columns or the `provides` of the workers that
        run before it.

        Args:
            context_pipeline (List[str]): An ordered list of context worker names.

        Returns:
            True if the pipeline is valid.

        Raises:
            ValueError: If a dependency is not met, with a descriptive error.
        """
        # Start met de basiskolommen die altijd aanwezig zijn in de ruwe DataFrame.
        available_columns = {"open", "high", "low", "close", "volume"}

        for plugin_name in context_pipeline:
            plugin_data = self._registry.get_plugin_data(plugin_name)
            if not plugin_data:
                raise ValueError(f"Plugin '{plugin_name}' not found in registry.")

            manifest, _ = plugin_data

            # Controleer de dependencies van de huidige plugin.
            if manifest.dependencies:
                for dep in manifest.dependencies:
                    if dep not in available_columns:
                        raise ValueError(
                            f"Dependency '{dep}' for plugin '{plugin_name}' not met. "
                            f"Available columns: {sorted(list(available_columns))}"
                        )

            # Voeg de output van deze plugin toe aan de set van beschikbare kolommen.
            if manifest.provides:
                available_columns.update(manifest.provides)

        return True

--- END FILE: backend/assembly/dependency_validator.py ---

--- START FILE: backend/assembly/engine_builder.py ---
# backend/assembly/engine_builder.py
"""
Contains the EngineBuilder, a specialist for assembling a StrategyEngine.
"""
from typing import Any, Dict, List

from backend.assembly.worker_builder import WorkerBuilder
from backend.config.schemas.run_schema import RunBlueprint, WorkerDefinition
from backend.core.enums import PipelinePhase
from backend.core.strategy_engine import StrategyEngine
from backend.core.interfaces.worker import ContextWorker

class EngineBuilder:
    """Assembles a StrategyEngine with all its required workers."""

    def __init__(self, worker_builder: WorkerBuilder):
        self._worker_builder = worker_builder

    def build_context_pipeline(
        self, run_conf: RunBlueprint
    ) -> List[ContextWorker]:
        """Builds the initial pipeline of context workers."""
        context_plugin_names: List[str] = run_conf.taskboard.root.get(
            PipelinePhase.STRUCTURAL_CONTEXT, []
        )

        built_workers = [
            self._worker_builder.build(
                name=name,
                user_params=run_conf.workforce.get(name, WorkerDefinition()).params
            ) for name in context_plugin_names if name
        ]
        # Filter out any workers that failed to build
        return [worker for worker in built_workers if worker is not None]

    def build_engine(self, run_conf: RunBlueprint) -> StrategyEngine:
        """Builds and returns a configured StrategyEngine."""
        active_workers: Dict[str, Any] = {}
        for phase, plugin_names in run_conf.taskboard.root.items():
            if phase != PipelinePhase.STRUCTURAL_CONTEXT:
                worker_list = [
                    self._worker_builder.build(
                        name=name,
                        user_params=run_conf.workforce.get(name, WorkerDefinition()).params
                    ) for name in plugin_names if name
                ]
                active_workers[phase.value] = [w for w in worker_list if w is not None]

        return StrategyEngine(active_workers=active_workers)

--- END FILE: backend/assembly/engine_builder.py ---

--- START FILE: backend/assembly/plugin_creator.py ---
# backend/assembly/plugin_creator.py
"""
Contains the PluginCreator, a service for generating plugin boilerplate code.

@layer: Backend (Assembly)
@dependencies: [pathlib, backend.utils.app_logger]
@responsibilities:
    - Creates the directory structure for a new plugin.
    - Generates all required files from templates (manifest, worker, etc.).
    - Provides a simple interface for creating new plugins.
"""

from pathlib import Path
import shutil
from backend.utils.app_logger import LogEnricher

class PluginCreator:
    """
    A service class responsible for creating the boilerplate structure for a new plugin.
    It uses template files to generate the necessary Python and YAML files.
    """

    def __init__(self, plugins_root: Path, logger: LogEnricher):
        """Initializes the PluginCreator.

        Args:
            plugins_root (Path): The root directory where all plugins are stored.
            logger (LogEnricher): The logger instance, passed via dependency injection.
        """
        self._logger = logger
        self.plugins_root = plugins_root
        self.template_root = Path(__file__).parent / "templates"

        if not self.plugins_root.is_dir():
            self._logger.error(f"Plugins root directory does not exist: {self.plugins_root}")
            raise FileNotFoundError(f"Plugins root directory does not exist: {self.plugins_root}")

        if not self.template_root.is_dir():
            self._logger.error(f"Template directory not found at: {self.template_root}")
            raise FileNotFoundError(f"Template directory not found at: {self.template_root}")

    def create(self, name: str, plugin_type: str) -> bool:
        """Creates a new plugin skeleton from templates.

        Args:
            name (str): The name of the new plugin (e.g., "my_test_plugin").
            plugin_type (str): The type of the plugin (e.g., "signal_generator").

        Returns:
            bool: True if creation was successful, False otherwise.
        """
        plugin_path = self.plugins_root / plugin_type / name
        tests_path = plugin_path / "tests"

        try:
            self._logger.info(f"Creating plugin '{name}' at: {plugin_path}")
            tests_path.mkdir(parents=True, exist_ok=True)

            template_files = {
                "manifest.yaml.tpl": "plugin_manifest.yaml",
                "schema.py.tpl": "schema.py",
                "worker.py.tpl": "worker.py",
                "visualization_schema.py.tpl": "visualization_schema.py",
                "test/test_worker.py.tpl": "tests/test_worker.py"
            }

            for template_name, target_name in template_files.items():
                source_path = self.template_root / template_name
                target_path = plugin_path / target_name

                # TODO: Implement actual template rendering (e.g., with Jinja2)
                # For now, we are just copying the files.
                shutil.copy(source_path, target_path)

            self._logger.info(f"Successfully created plugin '{name}'.")
            return True

        except Exception as e:
            self._logger.error(f"Failed to create plugin '{name}': {e}", exc_info=True)
            if plugin_path.exists():
                shutil.rmtree(plugin_path) # Cleanup partial creation
            return False

--- END FILE: backend/assembly/plugin_creator.py ---

--- START FILE: backend/assembly/plugin_registry.py ---
# backend/assembly/plugin_registry.py
"""
Contains the PluginRegistry, responsible for discovering, validating, and indexing
all available plugins within the ecosystem.

@layer: Backend (Assembly)
@dependencies: [Pydantic, PyYAML, backend.config.schemas]
@responsibilities:
    - Scans plugin directories for manifests.
    - Validates manifest schemas against the PluginManifest contract.
    - Builds and maintains the central in-memory plugin registry.
"""

from pathlib import Path
from typing import Dict, Optional, Tuple

import yaml

from pydantic import ValidationError
from backend.config.schemas.platform_schema import PlatformConfig
from backend.config.schemas.plugin_manifest_schema import PluginManifest
from backend.utils.app_logger import LogEnricher

class PluginRegistry:
    """
    Discovers all valid plugins and holds their manifest data in an
    in-memory dictionary for fast retrieval by other components.
    """

    def __init__(self, platform_config: PlatformConfig, logger: LogEnricher):
        """
        Initializes the registry by scanning and validating all plugins.

        Args:
            platform_config (PlatformConfig): The validated platform configuration object.
            logger (LogEnricher): The logger instance.
        """
        self._logger = logger
        self._plugins_root_path = Path(platform_config.plugins_root_path)
        self._registry: Dict[str, Tuple[PluginManifest, Path]] = {}

        self._scan_and_register_plugins()

    def _scan_and_register_plugins(self):
        """
        Scans the plugin directory, validates each manifest, and populates the registry.
        """
        self._logger.info(f"Scanning for plugins in '{self._plugins_root_path}'...")

        if not self._plugins_root_path.is_dir():
            self._logger.error(f"Plugin root path '{self._plugins_root_path}' not found.")
            return

        for manifest_path in self._plugins_root_path.rglob("plugin_manifest.yaml"):
            try:
                with open(manifest_path, 'r', encoding='utf-8') as f:
                    manifest_data = yaml.safe_load(f)

                # Valideer de data tegen ons Pydantic-contract
                manifest = PluginManifest(**manifest_data)

                # Controleer op dubbele namen
                if manifest.name in self._registry:
                    self._logger.warning(
                        f"Duplicate plugin name '{manifest.name}' found at '{manifest_path}'. "
                        "Skipping."
                    )
                    continue

                # Voeg de gevalideerde manifest toe aan de registry
                plugin_directory = manifest_path.parent
                self._registry[manifest.name] = (manifest, plugin_directory)

            except yaml.YAMLError as e:
                self._logger.warning(f"Could not parse manifest at '{manifest_path}': {e}")
            except ValidationError as e:
                self._logger.warning(f"Invalid manifest at '{manifest_path}':\n{e}")

        self._logger.info(
            f"Scan complete. Found and registered {len(self._registry)} valid plugins."
        )

    def get_plugin_data(self, plugin_name: str) -> Optional[Tuple[PluginManifest, Path]]:
        """
        Retrieves the validated manifest for a single plugin by its unique name.

        Args:
            plugin_name (str): The unique name of the plugin.

        Returns:
            Optional[PluginManifest]: The Pydantic model of the manifest, or None if not found.
        """
        return self._registry.get(plugin_name)

    def get_all_manifests(self) -> Dict[str, PluginManifest]:
        """
        Returns the entire registry of validated plugin manifests.

        Returns:
            Dict[str, PluginManifest]: A dictionary of all registered plugins.
        """
        return {name: data[0] for name, data in self._registry.items()}

--- END FILE: backend/assembly/plugin_registry.py ---

--- START FILE: backend/assembly/worker_builder.py ---
# backend/assembly/worker_builder.py
"""
Contains the WorkerBuilder, responsible for instantiating a single plugin worker
based on its manifest and user-provided configuration.

@layer: Backend (Assembly)
@dependencies:
    - .plugin_registry: To get the manifest (the "blueprint") for a worker.
    - backend.utils.dynamic_loader: To dynamically import plugin code.
    - backend.utils.app_logger: To create and inject a specific logger for the worker.
@responsibilities:
    - Dynamically loads a worker's code and its Pydantic schema.
    - Validates user-provided parameters against the plugin's schema.
    - Injects dependencies (like a logger) into the worker instance.
    - Returns a fully instantiated and validated worker object.
"""
from typing import Any, Dict, Optional, cast

from pydantic import ValidationError

from backend.assembly.plugin_registry import PluginRegistry
from backend.utils.dynamic_loader import load_class_from_module
from backend.utils.app_logger import LogEnricher


class WorkerBuilder:
    """Constructs a single, validated plugin worker instance."""

    def __init__(self, plugin_registry: PluginRegistry, logger: LogEnricher):
        """Initializes the WorkerBuilder.

        Args:
            plugin_registry (PluginRegistry): The registry containing all discovered plugins.
            logger (LogEnricher): The main logger, used to create child loggers.
        """
        self._registry = plugin_registry
        self._logger = logger

    def build(self, name: str, user_params: Dict[str, Any]) -> Optional[Any]:
        """Builds, validates, and instantiates a single worker.

        This method orchestrates the entire lifecycle of creating a worker, from
        finding its definition to validating user input and injecting dependencies.

        Args:
            name (str): The unique name of the worker to build.
            user_params (Dict[str, Any]): The parameter dictionary from the
                                           run_blueprint's 'workforce' section.

        Returns:
            An instantiated and validated worker object if successful, otherwise None.
        """
        # 1. Vraag Manifest en Pad op
        plugin_data = self._registry.get_plugin_data(name)
        if not plugin_data:
            self._logger.error(f"Cannot build worker: plugin '{name}' not found in registry.")
            return None

        manifest, plugin_path = plugin_data

        try:
            # Converteer het file path (bv. "plugins\\signal_generators\\fvg")
            # naar een Python module path (bv. "plugins.signal_generators.fvg")
            plugin_module_path = ".".join(plugin_path.parts)

            # 2. Dynamisch Laden met expliciete paden
            schema_module_path = f"{plugin_module_path}.{manifest.schema_path.replace('.py', '')}"
            worker_module_path = f"{plugin_module_path}.worker"

            schema_class = load_class_from_module(schema_module_path, manifest.params_class)
            worker_class = load_class_from_module(worker_module_path, manifest.entry_class)

            # 3. Valideer Parameters
            validated_params = schema_class(**user_params)

            # 4. Creëer & Injecteer Logger
            # Haal de onderliggende standaard logger op om een child te maken.
            indent_val = self._logger.extra.get('indent', 0) if self._logger.extra else 0
            current_indent = cast(int, indent_val)
            child_logger = self._logger.logger.getChild(name)
            worker_logger = LogEnricher(
                child_logger,
                indent=current_indent + 1
            )

            # 5. Instantieer de Worker
            worker_instance = worker_class(
                name=name,
                params=validated_params,
                logger=worker_logger
            )

            self._logger.info(f"Successfully built worker '{name}'.")
            return worker_instance

        except (ImportError, AttributeError) as e:
            self._logger.error(
                f"Failed to load code for worker '{name}': {e}"
            )
        except ValidationError as e:
            self._logger.error(
                f"Invalid parameters for worker '{name}':\n{e}"
            )

        return None

--- END FILE: backend/assembly/worker_builder.py ---

--- START FILE: backend/assembly/__init__.py ---
# backend/assembly/__init__.py
"""
Exposes the public API of the Assembly sub-package.
"""
__all__ = [
    "ContextBuilder",
    "DependencyValidator",
    "PluginRegistry",
    "WorkerBuilder",
]

from .context_builder import ContextBuilder
from .dependency_validator import DependencyValidator
from .plugin_registry import PluginRegistry
from .worker_builder import WorkerBuilder

--- END FILE: backend/assembly/__init__.py ---

--- START FILE: backend/assembly/templates/__init__.py ---

--- END FILE: backend/assembly/templates/__init__.py ---

--- START FILE: backend/assembly/templates/test/__init__.py ---

--- END FILE: backend/assembly/templates/test/__init__.py ---

--- START FILE: backend/config/__init__.py ---

--- END FILE: backend/config/__init__.py ---

--- START FILE: backend/config/schemas/app_schema.py ---
# backend/config/schemas/app_schema.py
"""
Contains the final, composed Pydantic model for a complete application run.

@layer: Backend (Config)
@dependencies: [Pydantic, .platform_schema, .run_schema]
@responsibilities:
    - Composes platform-level and run-level configurations into a single,
      unified, and immutable AppConfig object.
"""
from pydantic import BaseModel
from .platform_schema import PlatformConfig
from .run_schema import RunBlueprint

class AppConfig(BaseModel):
    """
    The final, composed configuration object for a run. It explicitly
    combines platform-wide settings (PlatformConfig) with the blueprint for a
    specific run (RunBlueprint), creating a single source of truth.
    """
    platform: PlatformConfig
    run: RunBlueprint

--- END FILE: backend/config/schemas/app_schema.py ---

--- START FILE: backend/config/schemas/platform_schema.py ---
# backend/config/schemas/platform_schema.py
"""
Contains Pydantic models that define the structure of the platform.yaml file.
This is the foundational contract for the entire application's configuration.

@layer: Backend (Config)
@dependencies: [Pydantic]
@responsibilities:
    - Defines the schema for global, platform-wide settings.
"""

# --- Sub-models ---

from typing import Dict, List, Literal
from pydantic import BaseModel, Field
from backend.core.enums import LogLevel

class PlatformDataConfig(BaseModel):
    """Defines the structure for the 'data' section."""
    source_dir: str = "source_data"

class PortfolioConfig(BaseModel):
    """Defines the structure for the 'portfolio' section."""
    initial_capital: float = 10000.0
    fees_pct: float = 0.001

class LoggingConfig(BaseModel):
    """Defines the structure for the 'logging' section."""
    profile: Literal['developer', 'analysis'] = 'analysis'
    profiles: Dict[str, List[LogLevel]] # Gebruikt de LogLevel Enum

# --- Main model ---

class PlatformConfig(BaseModel):
    """
    The main Pydantic model that validates the entire platform.yaml file.
    It defines only the highest-level, essential configurations.
    """
    language: Literal['en', 'nl'] = 'nl'
    plugins_root_path: str = "plugins" # De enige verantwoordelijkheid t.o.v. plugins

    data: PlatformDataConfig = Field(default_factory=PlatformDataConfig)
    portfolio: PortfolioConfig = Field(default_factory=PortfolioConfig)

--- END FILE: backend/config/schemas/platform_schema.py ---

--- START FILE: backend/config/schemas/plugin_manifest_schema.py ---
# backend/config/schemas/plugin_manifest_schema.py
"""
Pydantic schema for validating the plugin_manifest.yaml file.

This schema acts as the single source of truth for the structure and data
types required for a plugin to be considered valid by the platform.

@layer: Backend (Config)
@dependencies: [Pydantic, backend.core.enums]
@responsibilities:
    - Defines the data contract for a plugin's manifest.
    - Enables automatic validation of manifests during plugin registration.
"""
from typing import Annotated, List, Literal

from pydantic import BaseModel, Field, StringConstraints

from backend.core.enums import PipelinePhase

# --- Nested Models for Grouping and Clarity ---

class CoreIdentity(BaseModel):
    """System-level fields for versioning and schema identification."""
    apiVersion: Literal["s1mpletrader.io/v1"] = Field(
        description="manifest.core_identity.apiVersion.desc"
    )
    kind: Literal["PluginManifest"] = Field(
        description="manifest.core_identity.kind.desc"
    )

class PluginIdentification(BaseModel):
    """Descriptive metadata for identifying the plugin."""
    name: Annotated[
        str,
        StringConstraints(strip_whitespace=True, to_lower=True, pattern=r"^[a-z0-9_]+$")
    ] = Field(description="manifest.identification.name.desc")

    display_name: str = Field(
        description="manifest.identification.display_name.desc"
    )
    type: PipelinePhase = Field(
        description="manifest.identification.type.desc"
    )
    version: Annotated[
        str,
        StringConstraints(pattern=r"^\d+\.\d+\.\d+$")
    ] = Field(description="manifest.identification.version.desc")

    description: str = Field(
        description="manifest.identification.description.desc"
    )
    author: str = Field(
        description="manifest.identification.author.desc"
    )

class Dependencies(BaseModel):
    """Defines the data contract for the plugin's interaction with context."""
    requires: List[str] = Field(
        description="manifest.dependencies.requires.desc",
        default_factory=list
    )
    provides: List[str] = Field(
        description="manifest.dependencies.provides.desc",
        default_factory=list
    )

class Permissions(BaseModel):
    """Defines the security permissions required by the plugin."""
    network_access: List[str] = Field(
        description="manifest.permissions.network_access.desc",
        default_factory=list
    )
    filesystem_access: List[str] = Field(
        description="manifest.permissions.filesystem_access.desc",
        default_factory=list
    )

# --- The Main Manifest Model ---

class PluginManifest(BaseModel):
    """The complete, validated Pydantic model for a plugin's manifest.yaml."""
    core_identity: CoreIdentity
    identification: PluginIdentification
    dependencies: Dependencies
    permissions: Permissions

--- END FILE: backend/config/schemas/plugin_manifest_schema.py ---

--- START FILE: backend/config/schemas/run_schema.py ---
# backend/config/schemas/run_schema.py
"""
Contains Pydantic models that define the structure of a run_schema.yaml file.
This schema defines how a user composes a strategy from available plugins.

@layer: Backend (Config)
@dependencies: [Pydantic]
@responsibilities:
    - Defines the schema for a single strategy configuration.
    - Validates the assignment of plugins to taskboard phases.
    - Validates the parameter definitions for the used plugins.
"""

from typing import List, Dict, Any
from pydantic import BaseModel, Field, RootModel
from backend.core.enums import PipelinePhase

class RunDataConfig(BaseModel):
    """Defines the data settings specific to this run."""
    trading_pair: str
    timeframe: str

class TaskboardConfig(RootModel[Dict[PipelinePhase, List[str]]]):
    """
    Defines which plugins are assigned to each phase.
    This is a flexible dictionary where keys must be valid PipelinePhase members.
    By inheriting from RootModel, this class instance acts directly as a dictionary.
    """

class WorkerDefinition(BaseModel):
    """
    Defines the user-provided parameters for a single plugin.
    The system will validate this 'params' dict against the plugin's own schema.py.
    """
    params: Dict[str, Any] = Field(default_factory=dict)

class RunBlueprint(BaseModel):
    """
    The main Pydantic model that validates a complete run_blueprint.yaml file.
    """
    data: RunDataConfig
    taskboard: TaskboardConfig
    workforce: Dict[str, WorkerDefinition] = Field(default_factory=dict)

--- END FILE: backend/config/schemas/run_schema.py ---

--- START FILE: backend/config/schemas/__init__.py ---

--- END FILE: backend/config/schemas/__init__.py ---

--- START FILE: backend/core/base_worker.py ---
# backend/core/base_worker.py
"""
Contains optional, concrete base classes for Strategy Workers to simplify
plugin development by automating DTO nesting and providing direct access
to key identifiers like the correlation_id.

@layer: Backend (Core)
@dependencies: [abc, typing, uuid]
@responsibilities:
    - Provide a generic BaseStrategyWorker that handles DTO nesting.
    - Provide specific, inheritable base classes for each worker category
      to minimize boilerplate code in plugins.
    - Automate the propagation of the correlation_id.
"""
from __future__ import annotations

from abc import ABC, abstractmethod
from typing import TYPE_CHECKING, Any, List, Dict, Generic, Optional, TypeVar
from uuid import UUID

if TYPE_CHECKING:
    from backend.dtos import (
        EntrySignal,
        RiskDefinedSignal,
        RoutedTradePlan,
        Signal,
        TradePlan,
        TradingContext,
        CriticalEvent,
    )


# --- Generieke Type Variabelen (volgens PEP 484 conventie) ---
InputDTO_T = TypeVar("InputDTO_T")  # pylint: disable=invalid-name
OutputDTO_T = TypeVar("OutputDTO_T")  # pylint: disable=invalid-name


class BaseStrategyWorker(ABC, Generic[InputDTO_T, OutputDTO_T]):
    """
    A generic base class that automates DTO creation and `correlation_id` handling.

    This class should typically not be inherited from directly. Use the
    specific, category-based classes below (e.g., BaseEntryPlanner) instead,
    as they pre-configure the DTO types and field names, resulting in
    minimal boilerplate for the plugin developer.
    """

    def __init__(self, params: Any):
        self.params = params

    def proces(
        self, input_dto: InputDTO_T, context: "TradingContext"
    ) -> Optional[OutputDTO_T]:
        """
        Public method called by the StrategyEngine. It extracts the correlation_id,
        calls the plugin's specific logic, and wraps the result in the
        correct output DTO.
        """
        # Haal de gepromote correlation_id direct van de input DTO
        correlation_id = getattr(input_dto, "correlation_id", None)
        if not isinstance(correlation_id, UUID):
            return None  # Veiligheid: stop als er geen ID is in de keten

        # Roep de kernlogica van de plugin aan
        new_data = self._process_internal(input_dto, correlation_id, context)

        if new_data is None:
            return None

        output_dto_class = self._get_output_dto_class()
        source_field_name = self._get_source_field_name()

        # Bouw de argumenten voor de constructor van de nieuwe DTO
        constructor_args: Dict[str, Any] = {
            "correlation_id": correlation_id,
            source_field_name: input_dto,
            **new_data,
        }

        return output_dto_class(**constructor_args)

    @abstractmethod
    def _process_internal(
        self,
        input_dto: InputDTO_T,
        correlation_id: UUID,
        context: "TradingContext",
    ) -> Optional[Dict[str, Any]]:
        """
        Plugin-specific logic must be implemented here by the developer.

        Args:
            input_dto: The DTO from the previous pipeline stage.
            correlation_id: The unique ID of the signal chain, provided for convenience.
            context: The full trading context.

        Returns:
            A dictionary with the new fields for the output DTO,
            or None if no output should be generated.
        """
        raise NotImplementedError

    @abstractmethod
    def _get_output_dto_class(self) -> type[OutputDTO_T]:
        """Must specify the output DTO type."""
        raise NotImplementedError

    @abstractmethod
    def _get_source_field_name(self) -> str:
        """Must specify the field name for the nested source DTO."""
        raise NotImplementedError


# --- Categorie-Specifieke Basisklassen ---

class BaseSignalGenerator(ABC):
    """Base class for SignalGenerator plugins (Fase 3)."""
    def __init__(self, params: Any):
        self.params = params

    @abstractmethod
    def process(self, context: "TradingContext") -> List["Signal"]:
        """
        Generates a list of raw Signal DTOs based on the market context.

        Args:
            context: The full trading context, including the enriched DataFrame.

        Returns:
            A list of Signal DTOs, or an empty list if no opportunities are found.
        """
        raise NotImplementedError

class BaseSignalRefiner(BaseStrategyWorker["Signal", "Signal"]):
    """Base class for SignalRefiner plugins (Fase 4)."""

    def execute(
        self, input_dto: "Signal", context: "TradingContext"
    ) -> Optional["Signal"]:
        """
        Overrides the base execute method for the specific case of a refiner,
        which acts as a filter (1-to-1 or 1-to-0).
        """
        is_valid = self._process(input_dto, input_dto.correlation_id, context)
        return input_dto if is_valid else None

    @abstractmethod
    def _process(  # type: ignore
        self,
        input_dto: "Signal",
        correlation_id: UUID,
        context: "TradingContext",
    ) -> bool:
        """Return True to keep the signal, False to discard it."""
        raise NotImplementedError


class BaseEntryPlanner(BaseStrategyWorker["Signal", "EntrySignal"]):
    """Base class for EntryPlanner plugins (Fase 5)."""

    def _get_output_dto_class(self) -> type["EntrySignal"]:
        # pylint: disable=import-outside-toplevel
        from backend.dtos import EntrySignal

        return EntrySignal

    def _get_source_field_name(self) -> str:
        return "signal"


class BaseExitPlanner(BaseStrategyWorker["EntrySignal", "RiskDefinedSignal"]):
    """Base class for ExitPlanner plugins (Fase 6)."""

    def _get_output_dto_class(self) -> type["RiskDefinedSignal"]:
        # pylint: disable=import-outside-toplevel
        from backend.dtos import RiskDefinedSignal

        return RiskDefinedSignal

    def _get_source_field_name(self) -> str:
        return "entry_signal"


class BaseSizePlanner(BaseStrategyWorker["RiskDefinedSignal", "TradePlan"]):
    """Base class for SizePlanner plugins (Fase 7)."""

    def _get_output_dto_class(self) -> type["TradePlan"]:
        # pylint: disable=import-outside-toplevel
        from backend.dtos import TradePlan

        return TradePlan

    def _get_source_field_name(self) -> str:
        return "risk_defined_signal"


class BaseOrderRouter(BaseStrategyWorker["TradePlan", "RoutedTradePlan"]):
    """Base class for OrderRouter plugins (Fase 8)."""

    def _get_output_dto_class(self) -> type["RoutedTradePlan"]:
        # pylint: disable=import-outside-toplevel
        from backend.dtos import RoutedTradePlan

        return RoutedTradePlan

    def _get_source_field_name(self) -> str:
        return "trade_plan"

class BaseCriticalEventDetector(ABC):
    """Base class for CriticalEventDetector plugins (Fase 9)."""
    def __init__(self, params: Any):
        self.params = params

    @abstractmethod
    def process(
        self, routed_trade_plans: List["RoutedTradePlan"], context: "TradingContext"
    ) -> List["CriticalEvent"]:
        """
        Detects and returns a list of critical events based on the final context.

        Args:
            routed_trade_plans: The list of proposed trades for the current cycle.
            context: The full trading context.

        Returns:
            A list of CriticalEvent DTOs, or an empty list if no events are detected.
        """
        raise NotImplementedError

--- END FILE: backend/core/base_worker.py ---

--- START FILE: backend/core/constants.py ---
# backend/core/constants.py
"""
Application-wide constants
"""

--- END FILE: backend/core/constants.py ---

--- START FILE: backend/core/context_recorder.py ---
# backend/core/context_recorder.py
"""
Contains the ContextRecorder, a class that acts as a central in-memory database
for storing contextual data produced by various strategy components during a run.

@layer: Backend (Core)
@dependencies: [pydantic, pandas, uuid]
@responsibilities:
    - Provides a single, unified interface for plugins to log contextual data.
    - Stores data in a structured way, indexed by timestamp and plugin name.
    - Serializes Pydantic models into JSON-compatible dictionaries for storage.
"""
import uuid
from typing import Any, Dict
import pandas as pd
from pydantic import BaseModel

class ContextRecorder:
    """A central, in-memory database for recording contextual data from specialists."""

    def __init__(self):
        """Initializes the ContextRecorder with an empty data log."""
        self._data_log: Dict[pd.Timestamp, Dict[str, Any]] = {}

    def add_data(
        self,
        correlation_id: uuid.UUID,
        timestamp: pd.Timestamp,
        specialist_name: str,
        context_object: BaseModel
    ):
        """
        Records a Pydantic context object from a specialist at a specific timestamp.

        The object is immediately serialized to a JSON-compatible dictionary to ensure
        immutability and prevent downstream side effects.

        Args:
            correlation_id (uuid.UUID): The unique ID of the trade lifecycle.
            timestamp (pd.Timestamp): The timestamp of the event to log.
            specialist_name (str): The name of the component logging the data.
            context_object (BaseModel): The Pydantic model with the context data.
        """
        if timestamp not in self._data_log:
            self._data_log[timestamp] = {}

        # Gebruik model_dump() om direct een dictionary te krijgen
        serializable_context = context_object.model_dump()

        # We voegen de correlation_id toe aan de gelogde data voor traceability
        serializable_context['correlation_id'] = str(correlation_id)

        self._data_log[timestamp][specialist_name] = serializable_context

    def get_all_data(self) -> Dict[pd.Timestamp, Dict[str, Any]]:
        """
        Returns the complete, raw data log.

        Returns:
            The nested dictionary containing all recorded context data.
        """
        return self._data_log

--- END FILE: backend/core/context_recorder.py ---

--- START FILE: backend/core/directive_flattener.py ---
# backend/core/directive_flattener.py
"""
Contains a utility class to flatten a deeply nested RoutedTradePlan DTO
into a simple, flat ExecutionDirective.

@layer: Backend (Core)
@dependencies: [backend.dtos, pydantic]
@responsibilities:
    - Decouples the StrategyEngine's complex data structure from the simple
      contract required by the ExecutionHandler.
    - Dynamically unnests DTOs to create a flat data structure.
"""
from typing import Any, Dict, cast
from backend.dtos.routed_trade_plan import RoutedTradePlan
from backend.dtos.execution_directive import ExecutionDirective

class DirectiveFlattener:
    """
    A utility responsible for dynamically flattening the nested trade plan
    structure into a final, flat execution directive.
    """

    def _flatten_recursively(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Recursively unnests Pydantic models within a dictionary.
        """
        flat_dict: Dict[str, Any] = {}
        for key, value in data.items():
            # --- DE FIX: Controleer op 'dict' in plaats van 'BaseModel' ---
            if isinstance(value, dict):
                # We willen de geneste container-keys (zoals 'signal', 'trade_plan') niet meenemen.
                # We pakken alleen de inhoud ervan uit.
                flat_dict.update(self._flatten_recursively(cast(Dict[str, Any], value)))
            else:
                flat_dict[key] = value
        return flat_dict

    def flatten(self, routed_trade_plan: RoutedTradePlan) -> ExecutionDirective:
        """
        Transforms a deeply nested RoutedTradePlan into a flat ExecutionDirective
        using a dynamic, recursive approach.

        Args:
            routed_trade_plan (RoutedTradePlan): The complete, nested output
                                                 from the OrderRouter (Fase 8).

        Returns:
            ExecutionDirective: A flat DTO containing all necessary data for execution.
        """
        # 1. Converteer het toplevel DTO naar een dictionary.
        nested_dict = routed_trade_plan.model_dump()

        # 2. Roep de recursieve functie aan om de dictionary plat te slaan.
        flat_data = self._flatten_recursively(nested_dict)

        # 3. Rename 'timestamp' from Signal to 'entry_time' for the directive
        if 'timestamp' in flat_data:
            flat_data['entry_time'] = flat_data.pop('timestamp')

        # 4. Creëer de uiteindelijke, platte DTO. Pydantic negeert
        #    automatisch alle overbodige velden (zoals de geneste objecten zelf).
        return ExecutionDirective(**flat_data)

--- END FILE: backend/core/directive_flattener.py ---

--- START FILE: backend/core/enums.py ---
# backend/core/enums.py
"""
Contains application-wide enumerations to provide type-safety and a single
source of truth for specific sets of values.

@layer: Core
"""
from enum import Enum

class LogLevel(str, Enum):
    """Defines all valid logging levels, including custom ones."""
    DEBUG = "DEBUG"
    INFO = "INFO"
    WARNING = "WARNING"
    ERROR = "ERROR"
    CRITICAL = "CRITICAL"
    SETUP = "SETUP"
    MATCH = "MATCH"
    FILTER = "FILTER"
    POLICY = "POLICY"
    RESULT = "RESULT"
    TRADE = "TRADE"

class PipelinePhase(str, Enum):
    """Defines the valid phases of the 6-phase strategy funnel."""
    REGIME_FILTER = "regime_filter"
    STRUCTURAL_CONTEXT = "structural_context"
    SIGNAL_GENERATOR = "signal_generator"
    SIGNAL_REFINER = "signal_refiner"
    ENTRY_PLANNER = "entry_planner"
    EXIT_PLANNER = "exit_planner"
    SIZE_PLANNER = "size_planner"
    ORDER_ROUTER = "order_router"
    CRITICAL_EVENT_DETECTOR = "critical_event_detector"

--- END FILE: backend/core/enums.py ---

--- START FILE: backend/core/execution.py ---
"""Contains the concrete implementation of an execution handler for backtests.

This module provides the `BacktestExecutionHandler`, which is a concrete
implementation of the `ExecutionHandler` interface. It is responsible for
simulating the execution of trading directives within a backtesting environment.

@layer: Backend (Core)
@dependencies: [backend.core.interfaces, backend.dtos, backend.utils]
@responsibilities:
    - Implement the `ExecutionHandler` interface for simulated backtests.
    - Receive `ExecutionDirective` objects and translate them into trade actions.
    - Interact with a `Tradable` component (e.g., Portfolio) to open trades.
    - Log all execution activities.
"""
from typing import List

# --- CORRECTIE: Importeer de GECENTRALISEERDE interface ---
from backend.core.interfaces.execution import ExecutionHandler
from backend.core.interfaces.portfolio import Tradable
from backend.dtos import ExecutionDirective
from backend.utils.app_logger import LogEnricher

class BacktestExecutionHandler(ExecutionHandler):
    """
    Handles the execution of directives within a simulated backtest environment.
    """
    def __init__(self, tradable: Tradable, logger: LogEnricher):
        self._tradable = tradable
        self._logger = logger

    def execute_plan(self, directives: List[ExecutionDirective]):
        """
        Processes a list of execution directives by calling the appropriate
        methods on the tradable entity (Portfolio).
        """
        for directive in directives:
            # Hier kun je in de toekomst logica toevoegen voor verschillende directive types
            # De huidige implementatie geeft de directive direct door.
            self._tradable.open_trade(directive)

--- END FILE: backend/core/execution.py ---

--- START FILE: backend/core/performance_analyzer.py ---
# backend/core/performance_analyzer.py
"""
Docstring for performance_analyzer.py.

@layer: TODO
@dependencies: TODO
@responsibilities: TODO
"""

class PerformanceAnalyzer:
    """Docstring for PerformanceAnalyzer."""
    pass

--- END FILE: backend/core/performance_analyzer.py ---

--- START FILE: backend/core/portfolio.py ---
# backend/core/portfolio.py
"""
Contains the Portfolio class, which manages the financial state of a backtest.

@layer: Backend
@dependencies:
    - backend.core.interfaces.portfolio: Implements the Tradable protocol.
    - backend.dtos: Uses ExecutionDirective and ClosedTrade DTOs.
@responsibilities:
    - Manages the account balance based on a starting capital.
    - Executes fully-formed ExecutionDirective objects without strategic validation.
    - Holds the state for multiple active trades.
    - Maintains a list of all closed trades as ClosedTrade DTOs.
@inputs:
    - `initial_capital` (float) and `fees_pct` (float) on initialization.
    - `ExecutionDirective` DTOs to be executed.
@outputs:
    - A list of `ClosedTrade` DTOs.
"""
from typing import Any, Dict, List
from uuid import UUID
import pandas as pd

from backend.core.interfaces.portfolio import Tradable
from backend.dtos.execution_directive import ExecutionDirective
from backend.dtos.closed_trade import ClosedTrade
from backend.utils.app_logger import LogEnricher
from backend.core.context_recorder import ContextRecorder


class Portfolio(Tradable):
    """
    Manages account capital, active trades, and a list of closed trades.

    This class acts as a stateful ledger. Its primary responsibility is to
    maintain the financial state of the simulation by executing pre-calculated
    ExecutionDirective objects and updating the balance. It is a concrete implementation
    of the Tradable protocol, capable of managing multiple concurrent trades.
    """

    def __init__(self,
                 initial_capital: float,
                 fees_pct: float,
                 logger: LogEnricher,
                 context_recorder: ContextRecorder):
        """
        Initializes the Portfolio.
        """
        self._initial_capital: float = initial_capital
        self._balance: float = initial_capital
        self._fees_pct: float = fees_pct
        self.logger = logger
        self.context_recorder = context_recorder

        self._closed_trades: List[ClosedTrade] = []
        self._active_trades: Dict[UUID, Dict[str, Any]] = {}

    @property
    def initial_capital(self) -> float:
        """Returns the starting capital of the portfolio."""
        return self._initial_capital

    @property
    def balance(self) -> float:
        """Returns the current account balance."""
        return self._balance

    @property
    def closed_trades(self) -> List[ClosedTrade]:
        """Returns the list of all closed trades."""
        return self._closed_trades

    @property
    def active_trades(self) -> Dict[UUID, Dict[str, Any]]:
        """A dictionary of all currently open trades, keyed by correlation_id."""
        return self._active_trades

    @property
    def active_trade_count(self) -> int:
        """Returns the number of active trades."""
        return len(self._active_trades)

    def get_active_trades(self) -> Dict[UUID, Dict[str, Any]]:
        """Returns the dictionary of active trades."""
        return self.active_trades

    def open_trade(self, execution_directive: ExecutionDirective):
        """
        Opens a new trade based on a pre-calculated ExecutionDirective object.
        """
        for trade in self._active_trades.values():
            if trade['asset'] == execution_directive.asset:
                self.logger.error(
                    "Attempted to open a trade on an asset with an existing position.",
                    values={'asset': execution_directive.asset}
                )
                return

        if execution_directive.position_value_quote > self._balance:
            self.logger.error(
                "Insufficient capital to open trade.",
                values={'required': execution_directive.position_value_quote,
                        'available': self._balance}
            )
            return

        # Sla ALLE benodigde data op, inclusief correlation_id en signal_type
        self._active_trades[execution_directive.correlation_id] = {
            "correlation_id": execution_directive.correlation_id,
            "signal_type": execution_directive.signal_type,
            "entry_time": execution_directive.entry_time,
            "asset": execution_directive.asset,
            "direction": execution_directive.direction,
            "entry_price": execution_directive.entry_price,
            "sl_price": execution_directive.sl_price,
            "tp_price": execution_directive.tp_price,
            "position_size_asset": execution_directive.position_size_asset,
            "position_value_eur": execution_directive.position_value_quote,
        }

        self.logger.trade(
            'portfolio.open_trade',
            values={
                'direction': execution_directive.direction.upper(),
                'price': f"{execution_directive.entry_price:,.2f}",
                'sl': f"{execution_directive.sl_price:,.2f}",
                'tp': f"{execution_directive.tp_price:,.2f}"if execution_directive.tp_price else "N/A"
            }
        )

    def process_candle(self, candle: pd.Series):
        """
        Processes the latest market data candle to check for SL/TP hits
        for all active trades.
        """
        if not self._active_trades:
            return

        # FIX: Controleer of de index van de candle (candle.name) een geldig Timestamp-object is
        if not isinstance(candle.name, pd.Timestamp):
            # Log een waarschuwing of negeer de candle
            return

        exit_timestamp = candle.name # Nu weten we zeker dat het een Timestamp is

        trade_ids_to_check = list(self._active_trades.keys())

        for correlation_id in trade_ids_to_check:
            trade = self._active_trades.get(correlation_id)
            if not trade:
                continue

            exit_price = None

            # TODO: In a multi-asset scenario, the candle should contain the asset
            # it belongs to, to match against the trade's asset.

            if trade['direction'] == 'long':
                if candle['low'] <= trade['sl_price']:
                    exit_price = trade['sl_price']
                elif trade['tp_price'] and candle['high'] >= trade['tp_price']:
                    exit_price = trade['tp_price']

            elif trade['direction'] == 'short':
                if candle['high'] >= trade['sl_price']:
                    exit_price = trade['sl_price']
                elif trade['tp_price'] and candle['low'] <= trade['tp_price']:
                    exit_price = trade['tp_price']

            if exit_price:
                self._close_trade(correlation_id, exit_timestamp, exit_price)

    def _close_trade(self, correlation_id: UUID, exit_timestamp: pd.Timestamp, exit_price: float):
        """
        Closes an active trade, calculates PnL, updates the balance, and archives
        the transaction.
        """
        trade_to_close = self._active_trades.pop(correlation_id, None)
        if not trade_to_close:
            return

        price_delta = exit_price - trade_to_close['entry_price']
        if trade_to_close['direction'] == 'short':
            price_delta *= -1

        gross_pnl = price_delta * trade_to_close['position_size_asset']
        fees = trade_to_close['position_value_eur'] * self._fees_pct * 2
        net_pnl = gross_pnl - fees

        self._balance += net_pnl

        # FIX: Voeg de ontbrekende correlation_id en signal_type velden toe
        closed_trade = ClosedTrade(
            correlation_id=trade_to_close['correlation_id'],
            signal_type=trade_to_close['signal_type'],
            entry_time=trade_to_close['entry_time'],
            exit_time=exit_timestamp,
            asset=trade_to_close['asset'],
            direction=trade_to_close['direction'],
            entry_price=trade_to_close['entry_price'],
            exit_price=exit_price,
            sl_price=trade_to_close['sl_price'],
            tp_price=trade_to_close['tp_price'],
            position_value_quote=trade_to_close['position_value_eur'],
            position_size_asset=trade_to_close['position_size_asset'],
            pnl_quote=net_pnl,
        )
        self._closed_trades.append(closed_trade)

        self.logger.trade(
            'portfolio.close_trade',
            values={
                'direction': closed_trade.direction.upper(),
                'price': f"{closed_trade.exit_price:,.2f}",
                'pnl': f"{closed_trade.pnl_quote:,.2f}",
                'result': "WIN" if closed_trade.pnl_quote > 0 else "LOSS"
            }
        )

--- END FILE: backend/core/portfolio.py ---

--- START FILE: backend/core/strategy_engine.py ---
# backend/core/strategy_engine.py
"""
Contains the StrategyEngine, the core component for executing the
signal-driven phases (3-9) of the trading strategy pipeline.

@layer: Backend (Core)
@dependencies:
    - backend.core.interfaces: For adhering to the Environment contract.
    - backend.dtos: For processing the DTO chain.
    - .directive_flattener: To flatten the final trade plan.
@responsibilities:
    - Orchestrates the event-driven loop, timed by the Environment's Clock.
    - Manages the DTO dataflow from Signal generation to final RoutedTradePlan.
    - Flattens approved plans into ExecutionDirectives.
    - Detects critical system-wide events.
    - Bundles all results into a final EngineCycleResult for each tick.
"""
from typing import Dict, List, Any, Generator

from backend.core.interfaces import (
    Clock, BaseStrategyEngine, SignalGenerator, SignalRefiner, EntryPlanner,
    ExitPlanner, SizePlanner, OrderRouter, CriticalEventDetector
)
from backend.dtos import (
    TradingContext, Signal, RoutedTradePlan, EngineCycleResult, CriticalEvent
)
from .directive_flattener import DirectiveFlattener


class StrategyEngine(BaseStrategyEngine):
    """
    The engine that orchestrates the signal-driven workflow (Fase 3-9).
    """

    def __init__(self, active_workers: Dict[str, Any]):
        """Initializes the StrategyEngine with a pre-built set of workers."""
        super().__init__(active_workers=active_workers)

        self._signal_generators: List[SignalGenerator] = active_workers.get('signal_generator', [])
        self._signal_refiners: List[SignalRefiner] = active_workers.get('signal_refiner', [])
        self._entry_planner: EntryPlanner | None = active_workers.get('entry_planner')
        self._exit_planner: ExitPlanner | None = active_workers.get('exit_planner')
        self._size_planner: SizePlanner | None = active_workers.get('size_planner')
        self._order_routers: List[OrderRouter] = active_workers.get('order_router', [])
        self._critical_event_detectors: List[CriticalEventDetector] = active_workers.get(
            'critical_event_detector', []
        )
        self._flattener = DirectiveFlattener()

    def run(self,
            trading_context: TradingContext,
            clock: Clock) -> Generator[EngineCycleResult, None, None]:
        """
        Starts the main event loop and yields a complete result for each cycle.
        """
        for _timestamp, _row in clock.tick():
            final_routed_plans: List[RoutedTradePlan] = []

            raw_signals: List[Signal] = []
            for generator in self._signal_generators:
                raw_signals.extend(generator.process(context=trading_context))

            for signal in raw_signals:
                routed_plan = self._process_single_signal(signal, trading_context)
                if routed_plan:
                    final_routed_plans.append(routed_plan)

            directives = [self._flattener.flatten(plan) for plan in final_routed_plans]

            events: List[CriticalEvent] = []
            for detector in self._critical_event_detectors:
                events.extend(detector.process(final_routed_plans, trading_context))

            yield EngineCycleResult(
                execution_directives=directives,
                critical_events=events
            )

    def _process_single_signal(
        self, signal: Signal, context: TradingContext
    ) -> RoutedTradePlan | None:
        """Leidt één enkel signaal door de trechter van Fase 4 tot 8."""

        approved_signal: Signal | None = signal
        for refiner in self._signal_refiners:
            if not (approved_signal := refiner.process(approved_signal, context)):
                return None

        if not self._entry_planner:
            return None
        if not (entry_signal := self._entry_planner.process(approved_signal, context)):
            return None

        if not self._exit_planner:
            return None
        if not (risk_defined_signal := self._exit_planner.process(entry_signal, context)):
            return None

        if not self._size_planner:
            return None
        if not (trade_plan := self._size_planner.process(risk_defined_signal, context)):
            return None

        final_routed_plan: RoutedTradePlan | None = None
        for router in self._order_routers:
            if final_routed_plan := router.process(trade_plan, context):
                break

        return final_routed_plan

--- END FILE: backend/core/strategy_engine.py ---

--- START FILE: backend/core/__init__.py ---
# backend/core/__init__.py
"""
Exposes the public API of the Core sub-package, making key components
available for other layers of the application, such as the Service layer
and test suites.

This centralization allows for cleaner imports, as consumers can import
directly from `backend.core` without needing to know the internal
file structure.

@layer: Backend (Core)
"""
__all__ = [
    "StrategyEngine",
    "Portfolio",
    "BaseStrategyWorker",
    "ContextRecorder",
    "BacktestExecutionHandler"
]

from .strategy_engine import StrategyEngine
from .portfolio import Portfolio
from .base_worker import BaseStrategyWorker
from .context_recorder import ContextRecorder
from .execution import BacktestExecutionHandler

--- END FILE: backend/core/__init__.py ---

--- START FILE: backend/core/interfaces/engine.py ---
# backend/core/interfaces/engine.py
"""
Contains the behavioral contracts (ABCs) for the core strategy execution engine.

@layer: Backend (Core Interfaces)
@dependencies: [abc, typing, backend.dtos]
@responsibilities:
    - Defines the abstract contract for any component that can execute the
      signal-driven portion of a strategy.
"""
from __future__ import annotations

from abc import ABC, abstractmethod
from typing import Any, Dict, Generator, TYPE_CHECKING

if TYPE_CHECKING:
    from backend.core.interfaces import Clock
    from backend.dtos import TradingContext, EngineCycleResult

class BaseStrategyEngine(ABC):
    """
    Abstract contract for a Strategy Engine.

    This interface defines the "motor" that drives the core trading logic.
    It is designed to be a pure, high-performance generator of TradePlans,
    completely decoupled from the environment in which it operates.
    """
    def __init__(self, active_workers: Dict[str, Any]):
        """Initializes the engine with its pre-built "toolbox" of workers."""
        ...

    @abstractmethod
    def run(self,
            trading_context: 'TradingContext',
            clock: 'Clock') -> Generator['EngineCycleResult', None, None]:
        """
        Starts the main event loop and yields approved TradePlans.

        Args:
            trading_context (TradingContext): The shared context object.
            clock (Clock): The clock that controls the flow of time.

        Yields:
            TradePlan: A fully validated and approved trade plan, ready for execution.
        """
        ...

--- END FILE: backend/core/interfaces/engine.py ---

--- START FILE: backend/core/interfaces/environment.py ---
# backend/core/interfaces/environment.py
"""
Contains the behavioral contracts (ABCs) for the Execution Environment
and its sub-components.

@layer: Backend (Core Interfaces)
@dependencies: [abc, typing, pandas, backend.dtos]
@responsibilities:
    - Defines the abstract contracts for the operational "world".
"""
from __future__ import annotations
from abc import ABC, abstractmethod
from typing import Generator, Tuple, TYPE_CHECKING
import pandas as pd

# --- CORRECTIE: Importeer de ExecutionHandler interface HIER ---
if TYPE_CHECKING:
    from backend.core.interfaces.execution import ExecutionHandler

class DataSource(ABC):
    """Abstract contract for any component that provides market data."""
    @abstractmethod
    def get_data(self) -> pd.DataFrame:
        """Returns the complete historical dataset for the environment."""
        ...

class Clock(ABC):
    """Abstract contract for any component that controls the flow of time."""
    @abstractmethod
    def tick(self) -> Generator[Tuple[pd.Timestamp, pd.Series], None, None]:
        """Yields the next moment in time (timestamp and data row)."""
        ...

class BaseEnvironment(ABC):
    """
    Abstract contract for an Execution Environment.
    This interface defines the "world" in which a strategy operates.
    """
    @property
    @abstractmethod
    def source(self) -> DataSource:
        """The data source for this environment."""
        ...

    @property
    @abstractmethod
    def clock(self) -> Clock:
        """The clock that controls time in this environment."""
        ...

    @property
    @abstractmethod
    def handler(self) -> "ExecutionHandler":
        """The execution handler for this environment."""
        ...

--- END FILE: backend/core/interfaces/environment.py ---

--- START FILE: backend/core/interfaces/execution.py ---
"""Defines the abstract contract for all execution handlers.

This module contains the `ExecutionHandler` Abstract Base Class (ABC), which
enforces a standard interface for any component that executes trading
directives.

@layer: Backend (Core Interfaces)
@dependencies: [abc, backend.dtos]
@responsibilities:
    - Define the `ExecutionHandler` abstract base class.
    - Specify the `execute_plan` method as the required contract for all execution environments.
"""
from __future__ import annotations
from abc import ABC, abstractmethod
from typing import List, TYPE_CHECKING

# CORRECTIE: Importeer DTOs alleen binnen een TYPE_CHECKING block
if TYPE_CHECKING:
    from backend.dtos import ExecutionDirective

class ExecutionHandler(ABC):
    """
    Abstract Base Class that defines the contract for any component capable
    of executing trade directives.
    """

    @abstractmethod
    def execute_plan(self, directives: List["ExecutionDirective"]) -> None:
        """
        Processes a list of execution directives.

        Args:
            directives (List[ExecutionDirective]): The directives to be executed.
        """
        raise NotImplementedError

--- END FILE: backend/core/interfaces/execution.py ---

--- START FILE: backend/core/interfaces/portfolio.py ---
# backend/core/interfaces/portfolio.py
# pylint: disable=unnecessary-ellipsis
"""
Contains the abstract contract (Protocol) for any component that can manage
and execute trades within the S1mpleTrader ecosystem.

@layer: Backend (Core Interfaces)
"""
from __future__ import annotations
from typing import Protocol, List, Dict, Any, runtime_checkable, TYPE_CHECKING
from uuid import UUID
import pandas as pd

# CORRECTIE: Importeer DTOs alleen binnen een TYPE_CHECKING block
if TYPE_CHECKING:
    from backend.dtos.execution_directive import ExecutionDirective
    from backend.dtos.closed_trade import ClosedTrade

@runtime_checkable
class Tradable(Protocol):
    """
    An interface for any object that can manage a financial state, open
    positions, and a history of closed trades. This contract ensures that
    high-level components like an ExecutionHandler can interact with any
    portfolio implementation in a consistent way.
    """

    @property
    def initial_capital(self) -> float:
        """The starting capital of the portfolio."""
        ...

    @property
    def balance(self) -> float:
        """The current, real-time balance of the portfolio."""
        ...

    @property
    def active_trades(self) -> Dict[UUID, Dict[str, Any]]:
        """A dictionary of all currently open trades."""
        ...

    @property
    def closed_trades(self) -> List["ClosedTrade"]:
        """A list of all closed trades."""
        ...

    def open_trade(self, execution_directive: "ExecutionDirective") -> None:
        """
        Receives a complete trade plan and processes it to open a new
        position, updating the internal state.
        """
        ...

    def process_candle(self, candle: pd.Series) -> None:
        """
        Processes the latest market data candle to check if any active
        trades should be closed based on their SL/TP levels.
        """
        ...

--- END FILE: backend/core/interfaces/portfolio.py ---

--- START FILE: backend/core/interfaces/worker.py ---
# backend/core/interfaces/worker.py
# pylint: disable=unnecessary-ellipsis
"""
Contains the behavioral contracts (Protocols) for all plugin worker types.
These interfaces are the "constitution" for the S1mpleTrader plugin
ecosystem, ensuring that any component created by a developer will correctly
integrate with the StrategyEngine.
@layer: Backend (Core Interfaces)
@dependencies: [typing, pandas]
@responsibilities:
    - Defines the structural contracts for all types of plugin workers.
    - Enforces the logical data flow of the 9-fase strategy pipeline.
"""
from __future__ import annotations
from typing import (Any, List, Optional, Protocol, TYPE_CHECKING, runtime_checkable)
import pandas as pd

# Use TYPE_CHECKING to prevent circular imports at runtime
if TYPE_CHECKING:
    from backend.dtos import (Signal, EntrySignal, RiskDefinedSignal,
                              TradePlan, RoutedTradePlan, CriticalEvent,
                              TradingContext)

# --- Specific Worker Contracts ---

@runtime_checkable
class ContextWorker(Protocol):
    """A contract for a data enrichment worker (Fase 1 & 2)."""
    def __init__(self, name: str, params: Any, logger: Any):
        """Initializes the worker with its name, params, and logger."""
        ...

    def process(self, df: pd.DataFrame, context: "TradingContext") -> pd.DataFrame:
        """Processes the DataFrame to add context."""
        ...

@runtime_checkable
class StrategyWorker(Protocol):
    """A base contract for any worker operating within the main DTO pipeline."""
    def __init__(self, name: str, params: Any, logger: Any):
        """Initializes the worker with its name, params, and logger."""
        ...

# --- Specific Strategy Worker Contracts (The Pipeline) ---

@runtime_checkable
class SignalGenerator(StrategyWorker, Protocol):
    """Fase 3: A contract for a worker that generates trading opportunities."""
    def process(self, context: "TradingContext") -> List["Signal"]:
        """Generates raw Signal DTOs based on the market context."""
        ...

@runtime_checkable
class SignalRefiner(StrategyWorker, Protocol):
    """Fase 4: A contract for a worker that filters raw signals."""
    def process(
        self, signal: "Signal", context: "TradingContext"
    ) -> Optional["Signal"]:
        """Processes a single signal and returns it if valid, otherwise None."""
        ...

@runtime_checkable
class EntryPlanner(StrategyWorker, Protocol):
    """Fase 5: A contract for a worker that defines the entry tactic."""
    def process(
        self, signal: "Signal", context: "TradingContext"
    ) -> Optional["EntrySignal"]:
        """Enriches a Signal with a concrete entry price."""
        ...

@runtime_checkable
class ExitPlanner(StrategyWorker, Protocol):
    """Fase 6: A contract for a worker that defines risk parameters (SL/TP)."""
    def process(
        self, entry_signal: "EntrySignal", context: "TradingContext"
    ) -> Optional["RiskDefinedSignal"]:
        """Enriches an EntrySignal with stop-loss and take-profit prices."""
        ...

@runtime_checkable
class SizePlanner(StrategyWorker, Protocol):
    """Fase 7: A contract for a worker that calculates position size."""
    def process(
        self, risk_defined_signal: "RiskDefinedSignal", context: "TradingContext"
    ) -> Optional["TradePlan"]:
        """Enriches a RiskDefinedSignal with the final position size."""
        ...

@runtime_checkable
class OrderRouter(StrategyWorker, Protocol):
    """Fase 8: A contract for a worker that translates a TradePlan."""
    def process(
        self, trade_plan: "TradePlan", context: "TradingContext"
    ) -> Optional["RoutedTradePlan"]:
        """Enriches a TradePlan with specific order execution instructions."""
        ...

@runtime_checkable
class CriticalEventDetector(StrategyWorker, Protocol):
    """Fase 9: A contract for a worker that scans for systemic events."""
    def process(
        self, routed_trade_plans: List["RoutedTradePlan"], context: "TradingContext"
    ) -> List["CriticalEvent"]:
        """Detects and returns a list of critical events."""
        ...

--- END FILE: backend/core/interfaces/worker.py ---

--- START FILE: backend/core/interfaces/__init__.py ---
# backend/core/interfaces/__init__.py
"""Exposes all interface contracts from a single, clean namespace."""

from .worker import *
from .environment import *
from .engine import *
from .portfolio import *

--- END FILE: backend/core/interfaces/__init__.py ---

--- START FILE: backend/data/loader.py ---
# backend/data/loader.py
"""
Handles loading raw data from CSV files and performing initial cleaning.

@layer: Backend (Data)
@dependencies: [pathlib, pandas, backend.utils.app_logger]
@responsibilities:
    - Loads raw OHLCV data from a specific CSV file.
    - Performs initial data cleaning, sets the timestamp index, and handles NA values.
"""
from pathlib import Path
import pandas as pd
from backend.utils.app_logger import LogEnricher

class DataLoader:
    """Loads and performs initial preparation of OHLCV data from a CSV file."""

    def __init__(self, file_path: str, logger: LogEnricher):
        """Initializes the DataLoader."""
        self.file_path = Path(file_path)
        self.logger = logger
        if not self.file_path.is_file():
            raise FileNotFoundError(f"Data file not found at path: {self.file_path}")

    def load(self) -> pd.DataFrame:
        """Loads data from the CSV, sets the index, and cleans the data."""
        self.logger.info('loader.loading_from', values={'filename': self.file_path.name})

        df: pd.DataFrame = pd.read_csv(self.file_path) # pyright: ignore[reportUnknownMemberType]

        # Converteer timestamp naar datetime objecten, stel in als index en sorteer.
        df['timestamp'] = pd.to_datetime(df['timestamp'], utc=True, errors='coerce')
        df = df.set_index('timestamp').sort_index()

        # Verwijder rijen met ontbrekende waarden om de datakwaliteit te garanderen.
        df = df.dropna() # pyright: ignore[reportUnknownMemberType]

        self.logger.info('loader.load_success')
        return df

--- END FILE: backend/data/loader.py ---

--- START FILE: backend/data/__init__.py ---

--- END FILE: backend/data/__init__.py ---

--- START FILE: backend/dtos/backtest_result.py ---
# backend/dtos/backtest_result.py
"""
Contains the data class for storing the complete result of a single backtest run.

@layer: Backend (DTO)
@dependencies: [pydantic, pandas]
@responsibilities:
    - Defines the standardized data structure for holding all results from a
      single backtest run, ready for analysis and presentation.
"""
from typing import Dict, Any
from pydantic import BaseModel, ConfigDict
import pandas as pd

class BacktestResult(BaseModel):
    """A container for all results of a single backtest run.

    This object acts as a standardized Data Transfer Object (DTO) that holds
    all the essential, aggregated outputs from a backtest analysis, produced
    by the PerformanceAnalyzer.

    Attributes:
        trades_df (pd.DataFrame): A DataFrame containing all ClosedTrade objects.
        equity_curve (pd.Series): A Series representing the portfolio's equity over time.
        drawdown_curve (pd.Series): A Series representing the portfolio's drawdown.
        metrics (Dict[str, Any]): A dictionary of calculated performance metrics.
        initial_capital (float): The starting capital for the run.
    """
    trades_df: pd.DataFrame
    equity_curve: pd.Series
    drawdown_curve: pd.Series
    metrics: Dict[str, Any]
    initial_capital: float

    model_config = ConfigDict(
        arbitrary_types_allowed=True
    )

--- END FILE: backend/dtos/backtest_result.py ---

--- START FILE: backend/dtos/closed_trade.py ---
# backend/dtos/closed_trade.py
"""
Contains the data class for a closed trade, representing a completed transaction.

@layer: Backend (DTO)
@dependencies: [pydantic, pandas, uuid]
@responsibilities:
    - Defines the standardized data structure for a trade that has been fully
      executed and resulted in a profit or loss.
"""
import uuid
from typing import Optional
from pydantic import BaseModel, ConfigDict
import pandas as pd

class ClosedTrade(BaseModel):
    """Represents the final, recorded result of a single completed trade.

    This DTO is created by the Portfolio after a position is closed. It serves
    as the definitive, historical record for performance analysis and reporting.
    It contains all information from the original TradePlan, enriched with the
    actual exit details and the resulting profit or loss.

    Attributes:
        correlation_id (uuid.UUID): The unique ID linking this record to its
                                    full context log. Inherited from the TradePlan.
        entry_time (pd.Timestamp): The timestamp when the trade was opened.
        exit_time (pd.Timestamp): The timestamp when the trade was closed.
        asset (str): The asset that was traded.
        direction (str): The direction of the trade ('long' or 'short').
        signal_type (str): The name of the logic that generated the original signal.
        entry_price (float): The actual entry price.
        exit_price (float): The actual exit price.
        sl_price (float): The original stop-loss price from the TradePlan.
        tp_price (Optional[float]): The original take-profit price, if any.
        position_value_quote (float): The initial value of the position.
        position_size_asset (float): The size of the position.
        pnl_quote (float): The net profit or loss of the trade, after fees.
    """
    correlation_id: uuid.UUID
    entry_time: pd.Timestamp
    exit_time: pd.Timestamp
    asset: str
    direction: str
    signal_type: str
    entry_price: float
    exit_price: float
    sl_price: float
    tp_price: Optional[float] = None
    position_value_quote: float
    position_size_asset: float
    pnl_quote: float

    model_config = ConfigDict(
        arbitrary_types_allowed=True
    )

--- END FILE: backend/dtos/closed_trade.py ---

--- START FILE: backend/dtos/critical_event.py ---
# backend/dtos/critical_event.py
"""
Contains the DTO for a critical event notification.

@layer: Backend (DTO)
@dependencies: [pydantic, pandas, uuid]
@responsibilities:
    - Defines the standardized data structure for a critical event detected
      by the StrategyEngine.
"""
import uuid
import pandas as pd
from pydantic import BaseModel, ConfigDict, Field

class CriticalEvent(BaseModel):
    """
    Represents a minimal, agnostic notification of a critical event.

    This DTO acts as an "alarm bell" generated by a CriticalEventDetector
    plugin (Fase 9). It signals that a significant systemic event has
    occurred. The detailed context that led to this event is stored
    separately in the ContextRecorder. The RunService interprets this
    event and decides on the appropriate action (e.g., halting trading).

    Attributes:
        correlation_id (uuid.UUID): A unique ID for this specific event.
        event_type (str): A string identifier for the event type (e.g., "MAX_DRAWDOWN_BREACHED").
        timestamp (pd.Timestamp): The timestamp of the candle on which the event was detected.
    """
    correlation_id: uuid.UUID = Field(default_factory=uuid.uuid4)
    event_type: str
    timestamp: pd.Timestamp

    model_config = ConfigDict(
        arbitrary_types_allowed=True
    )

--- END FILE: backend/dtos/critical_event.py ---

--- START FILE: backend/dtos/engine_cycle_result.py ---
# backend/dtos/engine_cycle_result.py
"""
Contains the DTO that represents the complete output of a single StrategyEngine cycle.

@layer: Backend (DTO)
@dependencies: [pydantic, .execution_directive, .critical_event]
@responsibilities:
    - Bundles all outcomes of a single engine tick into one object.
    - Decouples new trade instructions from systemic event notifications.
"""
from typing import List
from pydantic import BaseModel, ConfigDict
from .execution_directive import ExecutionDirective
from .critical_event import CriticalEvent

class EngineCycleResult(BaseModel):
    """
    Represents the complete output of a single processing cycle (tick)
    of the StrategyEngine. It decouples trade proposals from critical events.

    This object is yielded by the StrategyEngine on every tick and allows the
    consuming service (e.g., BacktestService) to intelligently decide on the
    next course of action.

    Attributes:
        execution_directives (List[ExecutionDirective]): A list of new trades to be executed.
        critical_events (List[CriticalEvent]): A list of detected systemic events.
    """
    execution_directives: List[ExecutionDirective]
    critical_events: List[CriticalEvent]

    model_config = ConfigDict(arbitrary_types_allowed=True)

--- END FILE: backend/dtos/engine_cycle_result.py ---

--- START FILE: backend/dtos/entry_signal.py ---
# backend/dtos/entry_signal.py
"""
Contains the data class for a signal enriched with an entry tactic.

@layer: Backend (DTO)
@dependencies: [pydantic, pandas, uuid]
@responsibilities:
    - Defines the data structure for a signal that has been enriched with
      a concrete entry tactic by an EntryPlanner.
"""
import uuid
from pydantic import BaseModel, ConfigDict
from .signal import Signal

class EntrySignal(BaseModel):
    """Represents a signal with a concrete entry tactic.

    This DTO is created by an EntryPlanner (Fase 5a). It enriches a raw
    Signal DTO with a calculated entry price and a descriptive entry method.
    It serves as the input for the ExitPlanner (Fase 5b).

    Attributes:
        correlation_id (uuid.UUID): The unique ID inherited from the source Signal.
        signal: the original Signal DTO.
        entry_price (float): The calculated entry price for the trade.
    """
    correlation_id: uuid.UUID
    signal: Signal
    entry_price: float

    model_config = ConfigDict(
        arbitrary_types_allowed=True
    )

--- END FILE: backend/dtos/entry_signal.py ---

--- START FILE: backend/dtos/execution_directive.py ---
# backend/dtos/execution_directive.py
"""
Contains the DTO for a final, flattened execution instruction.

@layer: Backend (DTO)
@dependencies: [pydantic, pandas, uuid]
@responsibilities:
    - Defines the flat, universal contract for an instruction sent to the
      ExecutionHandler.
"""
import uuid
from typing import Literal, Optional, Dict, Any
from pydantic import BaseModel, ConfigDict
import pandas as pd

class ExecutionDirective(BaseModel):
    """
    A flat, final, and universal instruction for the ExecutionHandler.

    This DTO is a flattened representation of a RoutedTradePlan and contains
    all necessary information to execute, manage, and track a trade. It serves
    as the definitive, simple contract between the StrategyEngine's output and
    the execution layer.

    Attributes:
        correlation_id (uuid.UUID): The unique ID from the source Signal.
        signal_type (str): The name of the logic that generated the original signal.
        asset (str): The asset to be traded.
        direction (Literal['long', 'short']): The direction of the trade.
        entry_price (float): The calculated entry price for the trade.
        sl_price (float): The absolute stop-loss price.
        tp_price (Optional[float]): The absolute take-profit price, if any.
        position_value_quote (float): The total value of the position in the quote currency.
        position_size_asset (float): The size of the position in the base asset.
        order_type (Literal['market', 'limit']): The fundamental order type.
        limit_price (Optional[float]): The price for a limit order.
        time_in_force (Literal['GTC', 'IOC', 'FOK']): How long the order remains valid.
        post_only (bool): Flag to ensure the order is a "maker" order.
        execution_strategy (Optional[Literal['twap']]): Label for an algorithmic strategy.
        strategy_params (Optional[Dict[str, Any]]): Parameters for the algorithmic strategy.
        preferred_exchange (Optional[str]): A hint for the ExecutionHandler.
        entry_time (pd.Timestamp): From the original signal.
    """
    # Traceability & Identity
    correlation_id: uuid.UUID
    signal_type: str

    # Core Trade Parameters
    asset: str
    direction: Literal['long', 'short']
    entry_price: float
    sl_price: float
    tp_price: Optional[float]

    # Sizing
    position_value_quote: float
    position_size_asset: float

    # Tactical Execution Instructions
    order_type: Literal['market', 'limit']
    limit_price: Optional[float] = None
    time_in_force: Literal['GTC', 'IOC', 'FOK'] = 'GTC'
    post_only: bool = False
    execution_strategy: Optional[Literal['twap']] = None
    strategy_params: Optional[Dict[str, Any]] = None
    preferred_exchange: Optional[str] = None

    # Timestamps
    entry_time: pd.Timestamp

    model_config = ConfigDict(
        arbitrary_types_allowed=True
    )

--- END FILE: backend/dtos/execution_directive.py ---

--- START FILE: backend/dtos/risk_defined_signal.py ---
# backend/dtos/risk_defined_signal.py
"""
Contains the data class for a signal enriched with exit prices (risk definition).

@layer: Backend (DTO)
@dependencies: [pydantic, uuid]
@responsibilities:
    - Defines the data structure for a signal that has been enriched with
      absolute stop-loss and take-profit prices by an ExitPlanner.
"""
import uuid
from typing import Optional
from pydantic import BaseModel, ConfigDict
from .entry_signal import EntrySignal

class RiskDefinedSignal(BaseModel):
    """Represents a signal with its risk parameters fully defined.

    This DTO is created by an ExitPlanner (Fase 5b). It enriches an
    EntrySignal with the absolute stop-loss and (optional) take-profit prices.
    It serves as the direct input for the SizePlanner (Fase 5c), providing all
    necessary information to calculate position size based on risk.

    Attributes:
        correlation_id (uuid.UUID): The unique ID from the source Signal.
        entry_signal: the original EntrySignal DTO.
        sl_price (float): The absolute stop-loss price, defining the risk boundary.
        tp_price (Optional[float]): The absolute take-profit price, if any.
    """
    correlation_id: uuid.UUID
    entry_signal: EntrySignal
    sl_price: float
    tp_price: Optional[float] = None

    model_config = ConfigDict(
        arbitrary_types_allowed=True
    )

--- END FILE: backend/dtos/risk_defined_signal.py ---

--- START FILE: backend/dtos/routed_trade_plan.py ---
# backend/dtos/routed_trade_plan.py
"""
Contains the DTO that represents a TradePlan decorated with execution tactics.

@layer: Backend (DTO)
@dependencies: [pydantic, uuid, backend.dtos.trade_plan]
@responsibilities:
    - Defines the universal blueprint for how an order should be executed.
"""
import uuid
from typing import Literal, Optional, Dict, Any
from pydantic import BaseModel, ConfigDict

from .trade_plan import TradePlan

class RoutedTradePlan(BaseModel):
    """
    The universal blueprint for how an order should be executed.

    This DTO is the output of an OrderRouter plugin (Fase 8). It takes the
    complete strategic intent (the TradePlan) and enriches it with concrete,
    technical execution instructions for the ExecutionHandler. It serves as the
    definitive contract between the strategy layer and the execution layer.

    Attributes:
        correlation_id (uuid.UUID): The unique ID from the source Signal.
        trade_plan (TradePlan): The nested strategic plan to be executed.
        order_type (Literal['market', 'limit']): The fundamental order type.
        limit_price (Optional[float]): The price for a limit order.
        time_in_force (Literal['GTC', 'IOC', 'FOK']): How long the order remains valid.
        post_only (bool): Flag to ensure the order is a "maker" order.
        execution_strategy (Optional[Literal['twap']]): Label for an algorithmic strategy.
        strategy_params (Optional[Dict[str, Any]]): Parameters for the algorithmic strategy.
        preferred_exchange (Optional[str]): A hint for the ExecutionHandler.
    """
    correlation_id: uuid.UUID
    trade_plan: TradePlan

    # --- Tactical Execution Instructions ---
    order_type: Literal['market', 'limit']
    limit_price: Optional[float] = None
    time_in_force: Literal['GTC', 'IOC', 'FOK'] = 'GTC'
    post_only: bool = False
    execution_strategy: Optional[Literal['twap']] = None
    strategy_params: Optional[Dict[str, Any]] = None
    preferred_exchange: Optional[str] = None

    model_config = ConfigDict(
        arbitrary_types_allowed=True
    )

--- END FILE: backend/dtos/routed_trade_plan.py ---

--- START FILE: backend/dtos/signal.py ---
# backend/dtos/signal.py
"""
Contains the data class for a raw, unfiltered signal event.

@layer: Backend (DTO)
@dependencies: [pydantic, pandas, uuid]
@responsibilities:
    - Defines the standardized data structure for a raw signal event, generated
      by a SignalGenerator plugin.
"""
import uuid
from typing import Literal
from pydantic import BaseModel, Field, ConfigDict
import pandas as pd

class Signal(BaseModel):
    """Represents a pure, unrefined signal event from a specific strategy logic.

    This DTO signifies that a pattern or condition was met at a specific time.
    It contains only the essential "what, where, and when" information, plus a
    unique identifier for traceability. This is the first DTO in the
    SignalOrchestrator's pipeline.

    Attributes:
        correlation_id (uuid.UUID): The unique ID that links this signal and all
                                    subsequent objects (Trade, ClosedTrade) to its
                                    full context log in the ContextRecorder.
                                    Generated by the SignalGenerator.
        timestamp (pd.Timestamp): The timestamp of the candle where the signal occurred.
        asset (str): The asset for which the signal was generated (e.g., 'BTC/EUR').
        direction (str): The directional bias of the signal ('long' or 'short').
        signal_type (str): The name of the logic that generated the signal (e.g.,
                           'golden_cross', 'fvg_entry_detector'). This defines the
                           identity of the signal.
    """
    correlation_id: uuid.UUID = Field(default_factory=uuid.uuid4)
    timestamp: pd.Timestamp
    asset: str
    direction: Literal['long', 'short']
    signal_type: str

    model_config = ConfigDict(
        arbitrary_types_allowed=True
    )

--- END FILE: backend/dtos/signal.py ---

--- START FILE: backend/dtos/trade_plan.py ---
# backend/dtos/trade_plan.py
"""
Contains the data class for a complete, executable trade plan.

@layer: Backend (DTO)
@dependencies: [pydantic, uuid, .risk_defined_signal]
@responsibilities:
    - Defines the standardized data structure for a fully planned trade, ready
      for the OrderRouter.
"""
import uuid
from pydantic import BaseModel, ConfigDict
from .risk_defined_signal import RiskDefinedSignal

class TradePlan(BaseModel):
    """
    Represents a complete strategic plan for a single trade.

    This DTO is created by a SizePlanner (Fase 7). It enriches a
    RiskDefinedSignal with the final position size and value. It contains
    all necessary strategic information before being passed to the
    OrderRouter (Fase 8) to be translated into tactical execution instructions.

    Attributes:
        correlation_id (uuid.UUID): The unique ID from the source Signal.
        risk_defined_signal (RiskDefinedSignal): The nested DTO from the previous phase.
        position_value_quote (float): The total value of the position in the quote currency.
        position_size_asset (float): The size of the position in the base asset.
    """
    correlation_id: uuid.UUID
    risk_defined_signal: RiskDefinedSignal
    position_value_quote: float
    position_size_asset: float

    model_config = ConfigDict(
        arbitrary_types_allowed=True
    )

--- END FILE: backend/dtos/trade_plan.py ---

--- START FILE: backend/dtos/trading_context.py ---
# backend/dtos/trading_context.py
"""
Contains the data class for the TradingContext.

@layer: Backend (DTO)
@dependencies: [pydantic, pandas, backend.core.interfaces]
@responsibilities:
    - Defines a standardized data structure to hold all shared, contextual
      information available during a single run.
"""
from __future__ import annotations
from typing import Dict, Any, TYPE_CHECKING
from pydantic import BaseModel, ConfigDict
import pandas as pd

from backend.core.context_recorder import ContextRecorder

# Gebruik TYPE_CHECKING om de circulaire import tijdens runtime te voorkomen
if TYPE_CHECKING:
    from backend.core.interfaces import Tradable


class TradingContext(BaseModel):
    """
    A container for all shared data available during a single run.
    This object is the single source of truth for the state of the world
    at the time a plugin is executed.

    Attributes:
        enriched_df (pd.DataFrame): The fully enriched DataFrame, containing all
                                    indicator and context columns.
        portfolio (Tradable): A reference to the active portfolio object.
        context_recorder (ContextRecorder): The recorder for logging detailed context.
        structural_context_registry (Dict[str, Any]): A registry for complex,
                                                      non-tabular context data.
    """
    enriched_df: pd.DataFrame
    # --- DE FIX: Gebruik een forward reference (string) voor Tradable ---
    portfolio: "Tradable"
    context_recorder: ContextRecorder
    structural_context_registry: Dict[str, Any] = {}

    def register_structural_data(self, source_plugin: str, data: Any):
        """Allows a plugin to register a complex data structure."""
        self.structural_context_registry[source_plugin] = data

    def get_structural_data(self, source_plugin: str) -> Any | None:
        """Allows a later plugin to retrieve a complex data structure."""
        return self.structural_context_registry.get(source_plugin)

    model_config = ConfigDict(
        arbitrary_types_allowed=True
    )

--- END FILE: backend/dtos/trading_context.py ---

--- START FILE: backend/dtos/__init__.py ---
# backend/dtos/__init__.py
"""
Exposes the public API of the DTOs sub-package.

This file centralizes all DTO imports, allowing other parts of the
application to import any DTO directly from `backend.dtos` without
needing to know the specific internal file structure.

@layer: Backend (DTO)
"""
__all__ = [
    "Signal",
    "EntrySignal",
    "RiskDefinedSignal",
    "TradePlan",
    "RoutedTradePlan",
    "CriticalEvent",
    "ExecutionDirective",
    "EngineCycleResult",
    "ClosedTrade",
    "TradingContext",
    "BacktestResult",
]

from .signal import Signal
from .entry_signal import EntrySignal
from .risk_defined_signal import RiskDefinedSignal
from .trade_plan import TradePlan
from .routed_trade_plan import RoutedTradePlan
from .critical_event import CriticalEvent
from .execution_directive import ExecutionDirective
from .engine_cycle_result import EngineCycleResult
from .closed_trade import ClosedTrade
from .backtest_result import BacktestResult
from .trading_context import TradingContext

--- END FILE: backend/dtos/__init__.py ---

--- START FILE: backend/environments/backtest_environment.py ---
# backend/environments/backtest_environment.py
"""
Contains the BacktestEnvironment and its specialized sub-components, providing
a complete, isolated world for running historical strategy tests.

@layer: Backend (Environment)
@dependencies: [pandas, backend.core.interfaces, backend.data.loader]
@responsibilities:
    - Implements the BaseEnvironment interface for backtesting.
    - Orchestrates the creation of CSV-based data sources, simulated clocks,
      and backtest execution handlers.
"""
import logging
from pathlib import Path
from typing import Generator, Tuple

import pandas as pd

from backend.config.schemas.app_schema import AppConfig
# --- CORRECTIE: Importeer de JUISTE, GECENTRALISEERDE interfaces ---
from backend.core.interfaces import BaseEnvironment, Clock, DataSource, Tradable
from backend.core.interfaces.execution import ExecutionHandler
from backend.data.loader import DataLoader
from backend.utils.app_logger import LogEnricher
# --- CORRECTIE: Importeer de CONCRETE handler ---
from backend.core.execution import BacktestExecutionHandler


# --- Sub-component Implementations ---
class CSVDataSource(DataSource):
    """A data source that loads market data from a CSV file."""

    def __init__(self, source_dir: str, trading_pair: str, timeframe: str, logger: LogEnricher):
        self._logger = logger
        base_path = Path(source_dir)
        pair_filename = trading_pair.replace('/', '_')
        filename = f"{pair_filename}_{timeframe}.csv"
        file_path = base_path / filename

        self._data_loader = DataLoader(str(file_path), self._logger)
        self._data: pd.DataFrame = pd.DataFrame()

    def get_data(self) -> pd.DataFrame:
        """Loads data if not already loaded, then returns it."""
        if self._data.empty:
            self._data = self._data_loader.load()
        return self._data

class SimulatedClock(Clock):
    """A clock that simulates the passage of time by iterating over a DataFrame."""

    def __init__(self, df: pd.DataFrame):
        self._df = df

    def tick(self) -> Generator[Tuple[pd.Timestamp, pd.Series], None, None]:
        """Yields each row of the DataFrame as a moment in time."""
        for timestamp, row in self._df.iterrows():
            assert isinstance(timestamp, pd.Timestamp)
            yield timestamp, row

# --- Main Environment Class ---
class BacktestEnvironment(BaseEnvironment):
    """
    The concrete implementation of a BaseEnvironment for running backtests.
    """
    def __init__(self, app_config: AppConfig, tradable: Tradable):
        """Initializes the environment and constructs its sub-components."""
        self._logger = LogEnricher(logging.getLogger(__name__))

        self._source = CSVDataSource(
            source_dir=app_config.platform.data.source_dir,
            trading_pair=app_config.run.data.trading_pair,
            timeframe=app_config.run.data.timeframe,
            logger=self._logger
        )
        self._clock = SimulatedClock(self._source.get_data())
        self._handler = BacktestExecutionHandler(tradable, self._logger)

    @property
    def source(self) -> DataSource:
        return self._source

    @property
    def clock(self) -> Clock:
        return self._clock

    @property
    def handler(self) -> ExecutionHandler:
        return self._handler

--- END FILE: backend/environments/backtest_environment.py ---

--- START FILE: backend/environments/live_environment.py ---
# backend/environments/live_environment.py
"""
Docstring for live_environment.py.

@layer: TODO
@dependencies: TODO
@responsibilities: TODO
"""

class LiveTradeEnvironment:
    """Docstring for LiveTradeEnvironment."""

--- END FILE: backend/environments/live_environment.py ---

--- START FILE: backend/environments/paper_environment.py ---
# backend/environments/paper_environment.py
"""
Docstring for paper_environment.py.

@layer: TODO
@dependencies: TODO
@responsibilities: TODO
"""

class PaperTradeEnvironment:
    """Docstring for PaperTradeEnvironment."""
    pass

--- END FILE: backend/environments/paper_environment.py ---

--- START FILE: backend/environments/__init__.py ---
# backend/environments/__init__.py
"""
Exposes the public API of the Environments sub-package.
"""
__all__ = [
    "BacktestEnvironment",
#    "LiveEnvironment",
#    "PaperEnvironment",
]

from .backtest_environment import BacktestEnvironment
#from .live_environment import LiveEnvironment
#from .paper_environment import PaperEnvironment

--- END FILE: backend/environments/__init__.py ---

--- START FILE: backend/utils/app_logger.py ---
# utils/app_logger.py
"""
Configures and provides the application's logging system.

This module is the single source of truth for all logging-related setup.
It is designed to be configured once at the application's entry point.

@layer: Utility
@dependencies:
    - Translator: The `LogFormatter` receives a `Translator` instance to translate log message keys.
    - Constants: Uses `core.constants` for log level definitions and default profile names.
@responsibilities:
    - Defines the custom `LogFormatter` to handle translation and indentation of log messages.
    - Defines the `LogEnricher` adapter, which is the standard logger interface for the application.
    - Defines the `LogProfiler` to filter logs based on the configured profile.
    - Provides the central `configure_logging` function to bootstrap the logging system.
@inputs:
    - The application `config` dictionary.
    - A `Translator` instance.
@outputs:
    - A fully configured root logger (side-effect).
"""

# 1. Standard Library Imports
import logging
import sys
from typing import Any, Dict, List, Literal, MutableMapping, Optional, Tuple

# 3. Our Application Imports
from backend.utils.translator import Translator
from backend.core.enums import LogLevel
from backend.config.schemas.platform_schema import LoggingConfig

class LogFormatter(logging.Formatter):
    """A custom log formatter that handles translation, value formatting, and indentation.

    This formatter intercepts the log record, translates the message key if
    applicable, formats the translated string with any provided values, and
    applies an indentation level based on the record's context.
    """

    def __init__(self,
                 fmt: Optional[str] = None,
                 datefmt: Optional[str] = None,
                 style: Literal['%', '{', '$'] = '%',
                 translator: Optional[Translator] = None):
        """Initializes the LogFormatter.

        Args:
            fmt (str, optional): The format string for the log. Defaults to None.
            datefmt (str, optional): The format string for dates. Defaults to None.
            style (str, optional): The formatting style. Defaults to '%'.
            translator (Translator, optional): The translator instance for
                                               translating log keys. Defaults to None.
        """
        super().__init__(fmt, datefmt, style)
        self.translator = translator

    def format(self, record: logging.LogRecord) -> str:
        """Formats the log record by translating, populating, and indenting it.

        Args:
            record (logging.LogRecord): The log record to format.

        Returns:
            The fully formatted log message string.
        """
        key = record.msg
        translated_template = key
        values_dict = getattr(record, 'values', {})

        # Step 1: Translate the message key, if it's a valid key.
        if self.translator and isinstance(key, str) and '.' in key and ' ' not in key:
            translated_template = self.translator.get(key, default=key)

        # Step 2: Format the template with any provided values.
        final_message = translated_template
        if values_dict:
            try:
                final_message = translated_template.format(**values_dict)
            except (KeyError, TypeError):
                final_message = f"{translated_template} [FORMATTING ERROR]"

        record.msg = final_message
        record.args = ()

        # Step 3: Apply our custom indentation directly to the message content.
        indent_level = getattr(record, 'indent', 0)
        indented_message = "  " * indent_level + final_message

        # Place the fully prepared (and indented) message back into the record.
        record.msg = indented_message

        # Step 4: Let the original Formatter handle the primary layout (e.g., [INFO   ]).
        return super().format(record)

class LogEnricher(logging.LoggerAdapter[logging.Logger]):
    """A logger adapter that enriches log records with indentation and context.

    This adapter provides the standard logging interface for the application. Its
    main purpose is to shuttle contextual data (like 'indent' or 'values') into
    the 'extra' payload of a log record, which can then be used by the
    `LogFormatter`. It also provides convenience methods for custom log levels.
    """

    def __init__(self, logger: logging.Logger, indent: int = 0):
        """Initializes the LogEnricher adapter.

        Args:
            logger: The logger instance to wrap.
            indent: The indentation level for messages from this logger.
        """
        super().__init__(logger, {'indent': indent})

    def process(
        self, msg: Any, kwargs: MutableMapping[str, Any]
    ) -> Tuple[Any, MutableMapping[str, Any]]:
        """Merges the adapter's contextual information into the kwargs.

        Args:
            msg: The log message.
            kwargs: Keyword arguments to the logging call.

        Returns:
            A tuple containing the message and the modified keyword arguments.
        """
        # Ensure 'extra' dictionary exists and merge adapter's context into it.
        kwargs["extra"] = kwargs.get("extra", {})
        kwargs["extra"].update(self.extra)

        # Move 'values' from kwargs into the 'extra' dict for the formatter.
        if 'values' in kwargs:
            kwargs['extra']['values'] = kwargs.pop('values')

        return msg, kwargs

    # --- Convenience methods for custom levels ---
    def setup(self, key: str, **values: Any) -> None:
        """Logs a message with the SETUP level."""
        self.log(CUSTOM_LEVELS[LogLevel.SETUP], key, values=values)

    def match(self, key: str, **values: Any) -> None:
        """Logs a message with the MATCH level."""
        self.log(CUSTOM_LEVELS[LogLevel.MATCH], key, values=values)

    def filter(self, key: str, **values: Any) -> None:
        """Logs a message with the FILTER level."""
        self.log(CUSTOM_LEVELS[LogLevel.FILTER], key, values=values)

    def policy(self, key: str, **values: Any) -> None:
        """Logs a message with the POLICY level."""
        self.log(CUSTOM_LEVELS[LogLevel.POLICY], key, values=values)

    def result(self, key: str, **values: Any) -> None:
        """Logs a message with the RESULT level."""
        self.log(CUSTOM_LEVELS[LogLevel.RESULT], key, values=values)

    def trade(self, key: str, **values: Any) -> None:
        """Logs a message with the TRADE level."""
        self.log(CUSTOM_LEVELS[LogLevel.TRADE], key, values=values)

class LogProfiler(logging.Filter):
    """A logging filter that allows messages based on the active profile."""

    def __init__(self, profile: str, profile_definitions: Dict[str, List[LogLevel]]):
        """Initializes the filter.

        Args:
            profile: The name of the active logging profile.
            profile_definitions: A dictionary defining all available profiles
                                 and their allowed log level names.
        """
        super().__init__()
        allowed_levels_for_profile = profile_definitions.get(profile, [])
        self.allowed_levels = {level.value for level in allowed_levels_for_profile}

    def filter(self, record: logging.LogRecord) -> bool:
        """Determines if a log record should be processed.

        Args:
            record: The log record to check.

        Returns:
            True if the record's level name is in the allowed set for the
            active profile, False otherwise.
        """
        return record.levelname in self.allowed_levels

# Define and register custom log levels. This mapping is an implementation
# detail of the logger and is therefore defined here, not in core.constants.
CUSTOM_LEVELS = {
    LogLevel.SETUP: 15,
    LogLevel.MATCH: 22,
    LogLevel.FILTER: 23,
    LogLevel.POLICY: 24,
    LogLevel.RESULT: 25,
    LogLevel.TRADE: 26,
}

def configure_logging(logging_config: LoggingConfig, translator: Translator):
    """Configures the central, root logger for the entire application.

    This function should be called only once from `main.py`. It sets up custom
    log levels, creates a handler, attaches the custom `LogFormatter` and
    `LogProfiler` filter, and adds the handler to the root logger.

    Args:
        logging_config: The Pydantic model for the logging configuration.
        translator: An existing translator instance to be used by the formatter.
    """
    for level_enum, level_value in CUSTOM_LEVELS.items():
        logging.addLevelName(level_value, level_enum.value)

    # Use dotted access on the Pydantic model
    log_profile = logging_config.profile
    profile_definitions = logging_config.profiles
    log_format = '[%(levelname)-8s] %(message)s'

    logger = logging.getLogger()
    logger.setLevel(logging.DEBUG)

    # Clear any existing handlers to prevent duplicate logs.
    if logger.hasHandlers():
        logger.handlers.clear()

    handler = logging.StreamHandler(sys.stdout)
    # The Formatter is the only component that needs the translator.
    handler.setFormatter(LogFormatter(log_format, translator=translator))
    handler.addFilter(LogProfiler(log_profile, profile_definitions))
    logger.addHandler(handler)

--- END FILE: backend/utils/app_logger.py ---

--- START FILE: backend/utils/data_utils.py ---
# backend/utils/data_utils.py
"""
Utility functions for data manipulation
"""

--- END FILE: backend/utils/data_utils.py ---

--- START FILE: backend/utils/dynamic_loader.py ---
# backend/utils/dynamic_loader.py
"""
Handles the dynamic loading of classes from plugin modules.

@layer: Backend (Utility)
@dependencies: [importlib]
@responsibilities:
    - Provides a single function to dynamically import a class from a module
      using a string-based path and class name.
"""

import importlib
from typing import Any

def load_class_from_module(module_path: str, class_name: str) -> Any:
    """
    Dynamically imports a module and returns a specific class from it.

    Args:
        module_path (str): The full, dot-separated path to the Python module
                           (e.g., "plugins.signal_generators.my_plugin.worker").
        class_name (str): The exact name of the class to load from the module.

    Raises:
        ImportError: If the module cannot be found.
        AttributeError: If the class does not exist within the module.

    Returns:
        The loaded class object (not an instance).
    """
    try:
        module = importlib.import_module(module_path)
        loaded_class = getattr(module, class_name)
        return loaded_class
    except ImportError as e:
        raise ImportError(f"Could not import module '{module_path}': {e}") from e
    except AttributeError as e:
        raise AttributeError(
            f"Class '{class_name}' not found in module '{module_path}': {e}"
        ) from e

--- END FILE: backend/utils/dynamic_loader.py ---

--- START FILE: backend/utils/translator.py ---
# utils/translator.py
"""
Handles loading and retrieving translated strings for the application.

@layer: Utility
@dependencies:
    - Constants: Uses `core.constants` to get the default language and locale directory path.
@responsibilities:
    - Loads the appropriate language file based on the application configuration.
    - Provides a `get` method to retrieve translated strings using dot-notation keys.
    - Provides a `get_param_name` method for the special case of parameter display names.
@inputs:
    - The application `config` dictionary on initialization.
    - Dot-notation keys and format values for getter methods.
@outputs:
    - Translated and formatted strings.
"""

# 1. Standard Library Imports
from pathlib import Path
from typing import Any, Dict

# 2. Third-Party Imports
import yaml

# 3. Our Application Imports
from backend.config.schemas.platform_schema import PlatformConfig

class Translator:
    """Loads and manages internationalization (i18n) strings from YAML files.

    This class is instantiated once at startup. It loads the appropriate
    language file based on the application configuration and provides methods
    to retrieve translated strings using a dot-notation key.
    """
    def __init__(self, platform_config: PlatformConfig):
        """Initializes the Translator by loading the appropriate language file.

        Args:
            app_config (AppConfig): The application Pydantic config object.
        """
        lang_path = Path('locales') / f"{platform_config.language}.yaml"
        self.strings: Dict[str, Any] = {}
        try:
            with open(lang_path, 'r', encoding='utf-8') as f:
                self.strings = yaml.safe_load(f)
        except FileNotFoundError:
            print(f"WARNING: Language file not found at {lang_path}")

    def get(self, key: str, default: str | None = None, **kwargs: Any) -> str:
        """Retrieves and formats a nested translated string using dot-notation.

        If the key is not found, it returns the default value, or the key
        itself if no default is provided.

        Args:
            key (str): The dot-notation key (e.g., 'app.start').
            default (str, optional): A fallback value to return if the key is
                                     not found. Defaults to None.
            **kwargs: Values to format into the translated string.

        Returns:
            The translated and formatted string.
        """
        try:
            value: Any = self.strings
            for part in key.split('.'):
                value = value[part]

            # A valid translation must be a string. If we resolved a dict
            # (incomplete key), it's invalid.
            if not isinstance(value, str):
                return default or key

            return value.format(**kwargs)
        except (KeyError, TypeError):
            return default or key

    def get_param_name(self, param_path: str, default: str | None = None) -> str:
        """Retrieves a display name for a full parameter path.

        This performs a direct lookup in the 'params_display_names' dictionary
        within the language file, which is a flat key-value map.

        Args:
            param_path (str): The full parameter path to look up.
            default (str, optional): The value to return if the path is not
                                     found. Defaults to the original param_path.

        Returns:
            The display name or a fallback value.
        """
        param_dict = self.strings.get('params_display_names', {})
        return param_dict.get(param_path, default or param_path)

--- END FILE: backend/utils/translator.py ---

--- START FILE: backend/utils/__init__.py ---

--- END FILE: backend/utils/__init__.py ---

--- START FILE: config/index.yaml ---
# Placeholder for config/index.yaml

--- END FILE: config/index.yaml ---

--- START FILE: config/platform.yaml ---
# config/platform.yaml
# De minimale, fundamentele configuratie voor S1mpleTrader V2.

language: 'en'
plugins_root_path: 'plugins'

data:
  source_dir: 'source_data'

portfolio:
  initial_capital: 10000.0
  fees_pct: 0.001

logging:
  profile: 'analysis'
  profiles:
    developer: ['WARNING', 'ERROR', 'CRITICAL']
    analysis: ['DEBUG', 'INFO', 'MATCH', 'FILTER', 'TRADE', 'RESULT', 'WARNING', 'ERROR', 'CRITICAL']

--- END FILE: config/platform.yaml ---

--- START FILE: config/__init__.py ---

--- END FILE: config/__init__.py ---

--- START FILE: config/optimizations/optimize_atr_params.yaml ---
# Placeholder for config/optimizations/optimize_atr_params.yaml

--- END FILE: config/optimizations/optimize_atr_params.yaml ---

--- START FILE: config/overrides/use_eth_pair.yaml ---
# Placeholder for config/overrides/use_eth_pair.yaml

--- END FILE: config/overrides/use_eth_pair.yaml ---

--- START FILE: config/runs/mss_fvg_strategy.yaml ---
# Placeholder for config/runs/mss_fvg_strategy.yaml

--- END FILE: config/runs/mss_fvg_strategy.yaml ---

--- START FILE: config/variants/robustness_test.yaml ---
# Placeholder for config/variants/robustness_test.yaml

--- END FILE: config/variants/robustness_test.yaml ---

--- START FILE: frontends/__init__.py ---

--- END FILE: frontends/__init__.py ---

--- START FILE: frontends/cli/presenters/optimization_presenter.py ---
# frontends/cli/presenters/optimization_presenter.py
"""
Docstring for optimization_presenter.py.

@layer: TODO
@dependencies: TODO
@responsibilities: TODO
"""

class OptimizationPresenter:
    """Docstring for OptimizationPresenter."""
    pass

--- END FILE: frontends/cli/presenters/optimization_presenter.py ---

--- START FILE: frontends/cli/reporters/cli_reporter.py ---
# frontends/cli/reporters/cli_reporter.py
"""
Docstring for cli_reporter.py.

@layer: TODO
@dependencies: TODO
@responsibilities: TODO
"""

class CliReporter:
    """Docstring for CliReporter."""
    pass

--- END FILE: frontends/cli/reporters/cli_reporter.py ---

--- START FILE: frontends/web/api/main.py ---
# frontends/web/api/main.py
"""
FastAPI application entry point
"""

--- END FILE: frontends/web/api/main.py ---

--- START FILE: frontends/web/api/__init__.py ---

--- END FILE: frontends/web/api/__init__.py ---

--- START FILE: frontends/web/api/routers/backtest_router.py ---
# frontends/web/api/routers/backtest_router.py
"""
API endpoints for running backtests
"""

--- END FILE: frontends/web/api/routers/backtest_router.py ---

--- START FILE: frontends/web/api/routers/plugins_router.py ---
# frontends/web/api/routers/plugins_router.py
"""
API endpoints for plugins
"""

--- END FILE: frontends/web/api/routers/plugins_router.py ---

--- START FILE: frontends/web/api/routers/__init__.py ---

--- END FILE: frontends/web/api/routers/__init__.py ---

--- START FILE: locales/en.yaml ---
# locales/en.yaml

# This file contains user-facing text for the application.
# For the MVP, we use English keys that map to English text.

app:
  start: "S1mpleTrader is starting..."

plugin_registry:
  scan_start: "Scanning for plugins in path: '{path}'..."
  scan_complete: "Scan complete. Found and registered {count} valid plugins."

loader:
  loading_from: "Loading data from {filename}..."
  load_success: "Data successfully loaded and prepared."

plugin_registry:
  scan_complete: "Scan complete. Found and registered {count} valid plugins."

worker_builder:
  build_success: "Successfully built worker '{name}'."
--- END FILE: locales/en.yaml ---

--- START FILE: locales/nl.yaml ---
app:
  start: "S1mpleTrader wordt gestart..."
--- END FILE: locales/nl.yaml ---

--- START FILE: plugins/__init__.py ---

--- END FILE: plugins/__init__.py ---

--- START FILE: plugins/critical_event_detectors/max_drawdown_overlay/plugin_manifest.yaml ---
name: max_drawdown_overlay
type: portfolio_overlay
...
--- END FILE: plugins/critical_event_detectors/max_drawdown_overlay/plugin_manifest.yaml ---

--- START FILE: plugins/critical_event_detectors/max_drawdown_overlay/schema.py ---
# plugins/portfolio_overlays/max_drawdown_overlay/schema.py
"""
Docstring for schema.py.

@layer: TODO
@dependencies: TODO
@responsibilities: TODO
"""

class MaxDrawdownOverlayParams:
    """Docstring for MaxDrawdownOverlayParams."""
    pass

--- END FILE: plugins/critical_event_detectors/max_drawdown_overlay/schema.py ---

--- START FILE: plugins/critical_event_detectors/max_drawdown_overlay/worker.py ---
# plugins/portfolio_overlays/max_drawdown_overlay/worker.py
"""
Docstring for worker.py.

@layer: TODO
@dependencies: TODO
@responsibilities: TODO
"""

class MaxDrawdownOverlay:
    """Docstring for MaxDrawdownOverlay."""
    pass

--- END FILE: plugins/critical_event_detectors/max_drawdown_overlay/worker.py ---

--- START FILE: plugins/critical_event_detectors/max_drawdown_overlay/__init__.py ---

--- END FILE: plugins/critical_event_detectors/max_drawdown_overlay/__init__.py ---

--- START FILE: plugins/critical_event_detectors/max_drawdown_overlay/tests/test_worker.py ---
# plugins/portfolio_overlays/max_drawdown_overlay/tests/test_worker.py
"""
Docstring for test_worker.py.

@layer: TODO
@dependencies: TODO
@responsibilities: TODO
"""

class TestMaxDrawdownOverlay:
    """Docstring for TestMaxDrawdownOverlay."""
    pass

--- END FILE: plugins/critical_event_detectors/max_drawdown_overlay/tests/test_worker.py ---

--- START FILE: plugins/critical_event_detectors/max_drawdown_overlay/tests/__init__.py ---

--- END FILE: plugins/critical_event_detectors/max_drawdown_overlay/tests/__init__.py ---

--- START FILE: plugins/entry_planners/__init__.py ---

--- END FILE: plugins/entry_planners/__init__.py ---

--- START FILE: plugins/entry_planners/liquidity_target_exit/plugin_manifest.yaml ---
name: liquidity_target_exit
type: trade_constructor
...
--- END FILE: plugins/entry_planners/liquidity_target_exit/plugin_manifest.yaml ---

--- START FILE: plugins/entry_planners/liquidity_target_exit/schema.py ---
# plugins/trade_constructors/liquidity_target_exit/schema.py
"""
Docstring for schema.py.

@layer: TODO
@dependencies: TODO
@responsibilities: TODO
"""

class LiquidityTargetExitParams:
    """Docstring for LiquidityTargetExitParams."""
    pass

--- END FILE: plugins/entry_planners/liquidity_target_exit/schema.py ---

--- START FILE: plugins/entry_planners/liquidity_target_exit/worker.py ---
# plugins/trade_constructors/liquidity_target_exit/worker.py
"""
Docstring for worker.py.

@layer: TODO
@dependencies: TODO
@responsibilities: TODO
"""

class LiquidityTargetExitPlanner:
    """Docstring for LiquidityTargetExitPlanner."""
    pass

--- END FILE: plugins/entry_planners/liquidity_target_exit/worker.py ---

--- START FILE: plugins/entry_planners/liquidity_target_exit/__init__.py ---

--- END FILE: plugins/entry_planners/liquidity_target_exit/__init__.py ---

--- START FILE: plugins/entry_planners/liquidity_target_exit/tests/test_worker.py ---
# plugins/trade_constructors/liquidity_target_exit/tests/test_worker.py
"""
Docstring for test_worker.py.

@layer: TODO
@dependencies: TODO
@responsibilities: TODO
"""

class TestLiquidityTargetExitPlanner:
    """Docstring for TestLiquidityTargetExitPlanner."""
    pass

--- END FILE: plugins/entry_planners/liquidity_target_exit/tests/test_worker.py ---

--- START FILE: plugins/entry_planners/liquidity_target_exit/tests/__init__.py ---

--- END FILE: plugins/entry_planners/liquidity_target_exit/tests/__init__.py ---

--- START FILE: plugins/order_routers/__init__.py ---

--- END FILE: plugins/order_routers/__init__.py ---

--- START FILE: plugins/regime_filters/__init__.py ---

--- END FILE: plugins/regime_filters/__init__.py ---

--- START FILE: plugins/regime_filters/adx_trend_filter/plugin_manifest.yaml ---
name: adx_trend_filter
type: regime_filter
...
--- END FILE: plugins/regime_filters/adx_trend_filter/plugin_manifest.yaml ---

--- START FILE: plugins/regime_filters/adx_trend_filter/schema.py ---
# plugins/regime_filters/adx_trend_filter/schema.py
"""
Docstring for schema.py.

@layer: TODO
@dependencies: TODO
@responsibilities: TODO
"""

class AdxTrendFilterParams:
    """Docstring for AdxTrendFilterParams."""
    pass

--- END FILE: plugins/regime_filters/adx_trend_filter/schema.py ---

--- START FILE: plugins/regime_filters/adx_trend_filter/worker.py ---
# plugins/regime_filters/adx_trend_filter/worker.py
"""
Docstring for worker.py.

@layer: TODO
@dependencies: TODO
@responsibilities: TODO
"""

class AdxTrendFilter:
    """Docstring for AdxTrendFilter."""
    pass

--- END FILE: plugins/regime_filters/adx_trend_filter/worker.py ---

--- START FILE: plugins/regime_filters/adx_trend_filter/__init__.py ---

--- END FILE: plugins/regime_filters/adx_trend_filter/__init__.py ---

--- START FILE: plugins/regime_filters/adx_trend_filter/tests/test_worker.py ---
# plugins/regime_filters/adx_trend_filter/tests/test_worker.py
"""
Docstring for test_worker.py.

@layer: TODO
@dependencies: TODO
@responsibilities: TODO
"""

class TestAdxTrendFilter:
    """Docstring for TestAdxTrendFilter."""
    pass

--- END FILE: plugins/regime_filters/adx_trend_filter/tests/test_worker.py ---

--- START FILE: plugins/regime_filters/adx_trend_filter/tests/__init__.py ---

--- END FILE: plugins/regime_filters/adx_trend_filter/tests/__init__.py ---

--- START FILE: plugins/signal_generators/__init__.py ---

--- END FILE: plugins/signal_generators/__init__.py ---

--- START FILE: plugins/signal_generators/fvg_entry_detector/plugin_manifest.yaml ---
name: fvg_entry_detector
type: signal_generator
...
--- END FILE: plugins/signal_generators/fvg_entry_detector/plugin_manifest.yaml ---

--- START FILE: plugins/signal_generators/fvg_entry_detector/schema.py ---
# plugins/signal_generators/fvg_entry_detector/schema.py
"""
Docstring for schema.py.

@layer: TODO
@dependencies: TODO
@responsibilities: TODO
"""

class FvgEntryDetectorParams:
    """Docstring for FvgEntryDetectorParams."""
    pass

--- END FILE: plugins/signal_generators/fvg_entry_detector/schema.py ---

--- START FILE: plugins/signal_generators/fvg_entry_detector/worker.py ---
# plugins/signal_generators/fvg_entry_detector/worker.py
"""
Docstring for worker.py.

@layer: TODO
@dependencies: TODO
@responsibilities: TODO
"""

class FvgEntryDetector:
    """Docstring for FvgEntryDetector."""
    pass

--- END FILE: plugins/signal_generators/fvg_entry_detector/worker.py ---

--- START FILE: plugins/signal_generators/fvg_entry_detector/__init__.py ---

--- END FILE: plugins/signal_generators/fvg_entry_detector/__init__.py ---

--- START FILE: plugins/signal_generators/fvg_entry_detector/tests/test_worker.py ---
# plugins/signal_generators/fvg_entry_detector/tests/test_worker.py
"""
Docstring for test_worker.py.

@layer: TODO
@dependencies: TODO
@responsibilities: TODO
"""

class TestFvgEntryDetector:
    """Docstring for TestFvgEntryDetector."""
    pass

--- END FILE: plugins/signal_generators/fvg_entry_detector/tests/test_worker.py ---

--- START FILE: plugins/signal_generators/fvg_entry_detector/tests/__init__.py ---

--- END FILE: plugins/signal_generators/fvg_entry_detector/tests/__init__.py ---

--- START FILE: plugins/signal_refiners/__init__.py ---

--- END FILE: plugins/signal_refiners/__init__.py ---

--- START FILE: plugins/signal_refiners/volume_spike_refiner/plugin_manifest.yaml ---
name: volume_spike_refiner
type: signal_refiner
...
--- END FILE: plugins/signal_refiners/volume_spike_refiner/plugin_manifest.yaml ---

--- START FILE: plugins/signal_refiners/volume_spike_refiner/schema.py ---
# plugins/signal_refiners/volume_spike_refiner/schema.py
"""
Docstring for schema.py.

@layer: TODO
@dependencies: TODO
@responsibilities: TODO
"""

class VolumeSpikeRefinerParams:
    """Docstring for VolumeSpikeRefinerParams."""
    pass

--- END FILE: plugins/signal_refiners/volume_spike_refiner/schema.py ---

--- START FILE: plugins/signal_refiners/volume_spike_refiner/worker.py ---
# plugins/signal_refiners/volume_spike_refiner/worker.py
"""
Docstring for worker.py.

@layer: TODO
@dependencies: TODO
@responsibilities: TODO
"""

class VolumeSpikeRefiner:
    """Docstring for VolumeSpikeRefiner."""
    pass

--- END FILE: plugins/signal_refiners/volume_spike_refiner/worker.py ---

--- START FILE: plugins/signal_refiners/volume_spike_refiner/__init__.py ---

--- END FILE: plugins/signal_refiners/volume_spike_refiner/__init__.py ---

--- START FILE: plugins/signal_refiners/volume_spike_refiner/tests/test_worker.py ---
# plugins/signal_refiners/volume_spike_refiner/tests/test_worker.py
"""
Docstring for test_worker.py.

@layer: TODO
@dependencies: TODO
@responsibilities: TODO
"""

class TestVolumeSpikeRefiner:
    """Docstring for TestVolumeSpikeRefiner."""
    pass

--- END FILE: plugins/signal_refiners/volume_spike_refiner/tests/test_worker.py ---

--- START FILE: plugins/signal_refiners/volume_spike_refiner/tests/__init__.py ---

--- END FILE: plugins/signal_refiners/volume_spike_refiner/tests/__init__.py ---

--- START FILE: plugins/structural_context/__init__.py ---

--- END FILE: plugins/structural_context/__init__.py ---

--- START FILE: plugins/structural_context/ema_detector/manifest.yaml ---
# plugins/structural_context/ema_detector/plugin_manifest.yaml
name: "ema_detector"
version: "1.0.0"
description: "Calculates and adds an Exponential Moving Average (EMA) to the DataFrame."
type: "structural_context" # Geeft aan dat dit een Fase 2 plugin is

# --- Code Contract ---
entry_class: "EmaDetector"          # De naam van de klasse in worker.py
schema_path: "schema.py"            # Het pad naar het Pydantic-schema
params_class: "EmaDetectorParams"   # De naam van de Pydantic-klasse in schema.py

# --- Data Contract ---
dependencies: ["close"]             # Deze plugin heeft de 'close' kolom nodig
provides: []                        # De output kolomnaam is dynamisch, dus hier leeg

--- END FILE: plugins/structural_context/ema_detector/manifest.yaml ---

--- START FILE: plugins/structural_context/ema_detector/schema.py ---
# plugins/structural_context/ema_detector/schema.py
"""
Contains the Pydantic validation schema for the EmaDetector plugin.

@layer: Plugin
@dependencies: [Pydantic]
@responsibilities:
    - Defines and validates the configuration parameters for the EmaDetector.
"""
from pydantic import BaseModel, Field

class EmaDetectorParams(BaseModel):
    """
    Validation schema for the parameters of the EmaDetector.
    It ensures that the 'period' is a positive integer.
    """
    period: int = Field(
        default=20,
        gt=0,
        description="The lookback period for the EMA calculation."
    )

--- END FILE: plugins/structural_context/ema_detector/schema.py ---

--- START FILE: plugins/structural_context/ema_detector/worker.py ---
# plugins/structural_context/ema_detector/worker.py
"""
Contains the main logic for the EmaDetector plugin.

@layer: Plugin
@dependencies: [pandas, backend.core.interfaces]
@responsibilities:
    - Implements the ContextWorker interface.
    - Calculates an EMA based on the provided period.
    - Adds the calculated EMA as a new column to the DataFrame.
"""
import pandas as pd

from backend.core.interfaces import ContextWorker
from backend.dtos import TradingContext
from backend.utils.app_logger import LogEnricher

from .schema import EmaDetectorParams


class EmaDetector(ContextWorker):
    """
    A context worker that calculates and adds an EMA column to a DataFrame.
    """

    def __init__(self, name: str, params: EmaDetectorParams, logger: LogEnricher):
        """
        Initializes the EmaDetector.

        Args:
            name (str): The unique name of this worker instance.
            params (EmaDetectorParams): A Pydantic object with validated parameters.
            logger (LogEnricher): The pre-configured logger instance.
        """
        self.name = name
        self.params = params
        self.logger = logger
        self.column_name = f"ema_{self.params.period}"

    # pylint: disable=unused-argument, arguments-differ
    def process(self, df: pd.DataFrame, context: TradingContext) -> pd.DataFrame:
        """
        Calculates the EMA and adds it as a new column to the DataFrame.

        Args:
            df (pd.DataFrame): The input DataFrame with OHLCV data.
            context (TradingContext): The trading context (not used by this worker).

        Returns:
            pd.DataFrame: The DataFrame with the new EMA column added.
        """
        if self.column_name in df.columns:
            return df

        self.logger.info(
            "Calculating EMA with period %s...", self.params.period
        )
        df[self.column_name] = df['close'].ewm(
            span=self.params.period,
            adjust=False
        ).mean()

        return df

--- END FILE: plugins/structural_context/ema_detector/worker.py ---

--- START FILE: plugins/structural_context/ema_detector/__init__.py ---

--- END FILE: plugins/structural_context/ema_detector/__init__.py ---

--- START FILE: plugins/structural_context/ema_detector/tests/test_worker.py ---
# plugins/structural_context/ema_detector/tests/test_worker.py
"""
Unit tests for the EmaDetector worker.

@layer: Plugin
@dependencies: [pandas, pytest]
@responsibilities:
    - Tests that the EmaDetector correctly calculates and adds an EMA column.
"""
import pandas as pd
from unittest.mock import MagicMock

# Import de componenten die we testen
from ..worker import EmaDetector
from ..schema import EmaDetectorParams
from backend.dtos import TradingContext

def test_ema_detector_adds_correct_column():
    """
    Tests if the worker adds a correctly named and calculated EMA column.
    """
    # Arrange
    params = EmaDetectorParams(period=3)
    logger = MagicMock()
    worker = EmaDetector(name="test_ema", params=params, logger=logger)

    # Maak een mock context object aan
    mock_context = MagicMock(spec=TradingContext)

    # Maak test-data
    data = {'close': [10, 20, 30, 40]}
    df = pd.DataFrame(data)

    # Handmatige berekening van de EMA (span=3 -> alpha=0.5)
    # 1: 10
    # 2: (20 * 0.5) + (10 * 0.5) = 15
    # 3: (30 * 0.5) + (15 * 0.5) = 22.5
    # 4: (40 * 0.5) + (22.5 * 0.5) = 31.25
    expected_ema = [10.0, 15.0, 22.5, 31.25]

    # Act
    result_df = worker.process(df, mock_context)

    # Assert
    expected_col = "ema_3"
    assert expected_col in result_df.columns

    # CORRECTIE: 'check_almost_equal' vervangen door 'check_exact=False' en 'atol'
    pd.testing.assert_series_equal(
        result_df[expected_col],
        pd.Series(expected_ema, name=expected_col),
        check_exact=False,
        atol=0.01  # Absolute tolerance voor floating point vergelijkingen
    )

--- END FILE: plugins/structural_context/ema_detector/tests/test_worker.py ---

--- START FILE: plugins/structural_context/ema_detector/tests/__init__.py ---

--- END FILE: plugins/structural_context/ema_detector/tests/__init__.py ---

--- START FILE: results/20250924_213000_mss_fvg_strategy/result_metrics.yaml ---
# Placeholder for results/20250924_213000_mss_fvg_strategy/result_metrics.yaml

--- END FILE: results/20250924_213000_mss_fvg_strategy/result_metrics.yaml ---

--- START FILE: results/20250924_213000_mss_fvg_strategy/run_config.yaml ---
# Placeholder for results/20250924_213000_mss_fvg_strategy/run_config.yaml

--- END FILE: results/20250924_213000_mss_fvg_strategy/run_config.yaml ---

--- START FILE: services/optimization_service.py ---
# services/optimization_service.py
"""
Docstring for optimization_service.py.

@layer: TODO
@dependencies: TODO
@responsibilities: TODO
"""

class OptimizationService:
    """Docstring for OptimizationService."""
    pass

--- END FILE: services/optimization_service.py ---

--- START FILE: services/parallel_run_service.py ---
# services/parallel_run_service.py
"""
Docstring for parallel_run_service.py.

@layer: TODO
@dependencies: TODO
@responsibilities: TODO
"""

class ParallelRunService:
    """Docstring for ParallelRunService."""
    pass

--- END FILE: services/parallel_run_service.py ---

--- START FILE: services/strategy_operator.py ---
# services/strategy_operator.py
"""
Contains the StrategyOperator, the service responsible for orchestrating a
single, complete strategy run.

@layer: Service
@dependencies:
    - backend.config.schemas.app_schema: To receive the complete run configuration.
    - backend.assembly: To build all necessary components (plugins, engine).
    - backend.environments: To create the world the strategy runs in.
    - backend.core: To instantiate the portfolio and the engine itself.
@responsibilities:
    - Acts as the main "conductor" for a single backtest or trading session.
    - Initializes all necessary backend components based on an AppConfig.
    - Runs the main event loop by calling the StrategyEngine.
    - Delegates the results from the engine (Directives and Events) to the
      appropriate handlers.
"""
from typing import Optional
import pandas as pd

from backend.config.schemas.app_schema import AppConfig
from backend.utils.app_logger import LogEnricher

from backend.assembly import PluginRegistry, WorkerBuilder, ContextBuilder
from backend.assembly.engine_builder import EngineBuilder  # Import the new builder
from backend.core import Portfolio, ContextRecorder
from backend.core.interfaces import Tradable #pyright: ignore[reportUnusedImport], pylint: disable=unused-import
from backend.core.interfaces.execution import ExecutionHandler
from backend.environments.backtest_environment import BacktestEnvironment
from backend.dtos import TradingContext

# Force Pydantic to resolve any forward-looking type references (like 'Tradable')
# This is crucial for models that use abstract base classes or protocols.
TradingContext.model_rebuild()

class StrategyOperator:  # pylint: disable=too-many-instance-attributes
    """Orchestrates a single, complete strategy run from setup to execution."""

    def __init__(self, app_config: AppConfig, logger: LogEnricher):
        """Initializes the StrategyOperator."""
        self._app_config = app_config
        self._logger = logger
        self._context_recorder = ContextRecorder()

        # Attributes to be initialized in _prepare_components
        self._engine_builder: Optional[EngineBuilder] = None
        self._context_builder: Optional[ContextBuilder] = None
        self._portfolio: Optional[Portfolio] = None
        self._environment: Optional[BacktestEnvironment] = None
        self._execution_handler: Optional[ExecutionHandler] = None

    def _prepare_components(self):
        """Phase 1: Prepares all long-lived components for the run."""
        self._logger.info("operator.setup_start")
        platform_conf = self._app_config.platform

        registry = PluginRegistry(platform_config=platform_conf, logger=self._logger)
        worker_builder = WorkerBuilder(plugin_registry=registry, logger=self._logger)

        # The EngineBuilder is now a dedicated specialist for engine assembly
        self._engine_builder = EngineBuilder(worker_builder=worker_builder)
        self._context_builder = ContextBuilder()

        self._portfolio = Portfolio(
            initial_capital=platform_conf.portfolio.initial_capital,
            fees_pct=platform_conf.portfolio.fees_pct,
            logger=self._logger,
            context_recorder=self._context_recorder
        )

        # TODO: MVP HACK - Environment should be injected by a factory.
        self._environment = BacktestEnvironment(app_config=self._app_config,
                                                tradable=self._portfolio)
        self._execution_handler = self._environment.handler

    def _prepare_data(self) -> pd.DataFrame:
        """Phase 2: Builds and runs the ContextBuilder to create enriched data."""
        assert self._engine_builder is not None, "Components not prepared"
        assert self._context_builder is not None, "Components not prepared"
        assert self._environment is not None, "Components not prepared"

        self._logger.info("operator.context_building_start")
        run_conf = self._app_config.run

        context_pipeline = self._engine_builder.build_context_pipeline(run_conf)

        enriched_df = self._context_builder.build(
            initial_df=self._environment.source.get_data(),
            context_pipeline=context_pipeline
        )
        self._logger.info("operator.context_building_complete")
        return enriched_df

    def _run_operational_cycle(self, enriched_df: pd.DataFrame):
        """Phase 3 & 4: Assembles the engine and runs the main event loop."""
        assert self._engine_builder is not None, "Components not prepared"
        assert self._portfolio is not None, "Components not prepared"
        assert self._environment is not None, "Components not prepared"
        assert self._execution_handler is not None, "Components not prepared"

        run_conf = self._app_config.run

        # --- Assemble Engine ---
        self._logger.info("operator.engine_assembly_start")
        engine = self._engine_builder.build_engine(run_conf)
        self._logger.info("operator.engine_assembly_complete")

        # --- Run Main Loop ---
        trading_context = TradingContext(
            enriched_df=enriched_df,
            portfolio=self._portfolio,
            context_recorder=self._context_recorder
        )
        self._logger.info("operator.engine_start")
        for result in engine.run(trading_context=trading_context, clock=self._environment.clock):
            if result.critical_events:
                # TODO: MVP HACK - Proper event handling belongs in a supervisor.
                self._logger.error("operator.critical_events_detected",
                                   values={'count': len(result.critical_events)})
                break

            if result.execution_directives:
                # TODO: ARCHITECTURE REFACTOR - Directive handling belongs in the caller.
                self._execution_handler.execute_plan(result.execution_directives)

    def run(self):
        """
        Executes the full workflow by composing the private helper methods.
        This method now only describes WHAT happens, not HOW.
        """
        self._logger.info("operator.run_start")

        self._prepare_components()
        enriched_df = self._prepare_data()
        self._run_operational_cycle(enriched_df)

        self._logger.info("operator.run_complete")

--- END FILE: services/strategy_operator.py ---

--- START FILE: services/variant_test_service.py ---
# services/variant_test_service.py
"""
Docstring for variant_test_service.py.

@layer: TODO
@dependencies: TODO
@responsibilities: TODO
"""

class VariantTestService:
    """Docstring for VariantTestService."""
    pass

--- END FILE: services/variant_test_service.py ---

--- START FILE: services/__init__.py ---
# services/__init__.py
"""
Exposes the public API of the Services package.
"""
__all__ = [
    # Top-level services
    "StrategyOperator",
    "OptimizationService",
    "ParallelRunService",
    "VariantTestService",
    # from .api_services
    "PluginQueryService",
    "VisualizationService",
]

# This assumes a file named strategy_operator.py exists with class StrategyOperator
from .strategy_operator import StrategyOperator
from .optimization_service import OptimizationService
from .parallel_run_service import ParallelRunService
from .variant_test_service import VariantTestService
from .api_services import (
    PluginQueryService,
    VisualizationService,
)

--- END FILE: services/__init__.py ---

--- START FILE: services/api_services/plugin_query_service.py ---
# services/api_services/plugin_query_service.py
"""
Docstring for plugin_query_service.py.

@layer: TODO
@dependencies: TODO
@responsibilities: TODO
"""

class PluginQueryService:
    """Docstring for PluginQueryService."""
    pass

--- END FILE: services/api_services/plugin_query_service.py ---

--- START FILE: services/api_services/visualization_service.py ---
# services/api_services/visualization_service.py
"""
Docstring for visualization_service.py.

@layer: TODO
@dependencies: TODO
@responsibilities: TODO
"""

class VisualizationService:
    """Docstring for VisualizationService."""
    pass

--- END FILE: services/api_services/visualization_service.py ---

--- START FILE: services/api_services/__init__.py ---
# services/api_services/__init__.py
"""
Exposes the public API of the API Services sub-package.
"""
__all__ = [
    "PluginQueryService",
    "VisualizationService",
]

from .plugin_query_service import PluginQueryService
from .visualization_service import VisualizationService

--- END FILE: services/api_services/__init__.py ---

--- START FILE: tests/__init__.py ---

--- END FILE: tests/__init__.py ---

--- START FILE: tests/backend/__init__.py ---

--- END FILE: tests/backend/__init__.py ---

--- START FILE: tests/backend/assembly/test_context_builder.py ---
# tests/backend/assembly/test_context_builder.py
"""Unit tests for the ContextBuilder."""

import pandas as pd
import pytest
from pytest_mock import MockerFixture

from backend.assembly.context_builder import ContextBuilder

# --- Test Setup: Maak een paar "nep"-workers ---

class MockWorkerAddColumn:
    """Een nep-worker die simpelweg een kolom toevoegt."""
    def __init__(self, new_col_name: str, value: int):
        self.new_col_name = new_col_name
        self.value = value

    def process(self, df: pd.DataFrame) -> pd.DataFrame:
        df[self.new_col_name] = self.value
        return df

class MockWorkerModifyColumn:
    """Een nep-worker die een bestaande kolom aanpast."""
    def __init__(self, target_col: str, multiplier: int):
        self.target_col = target_col
        self.multiplier = multiplier

    def process(self, df: pd.DataFrame) -> pd.DataFrame:
        df[self.target_col] = df[self.target_col] * self.multiplier
        return df

# --- De Tests ---

def test_context_builder_runs_pipeline_sequentially():
    """
    Tests of de ContextBuilder de workers in de juiste, sequentiële
    volgorde uitvoert.
    """
    # Arrange (De Voorbereiding)
    # 1. Maak een simpele start-DataFrame.
    initial_df = pd.DataFrame({'close': [10, 20, 30]})

    # 2. Maak instanties van onze nep-workers.
    worker1 = MockWorkerAddColumn(new_col_name="EMA_50", value=15)
    worker2 = MockWorkerModifyColumn(target_col="EMA_50", multiplier=2)

    # 3. Stel de ContextBuilder in met deze workers.
    context_builder = ContextBuilder()
    pipeline = [worker1, worker2]

    # Act (De Actie)
    # Voer de pijplijn uit.
    result_df = context_builder.build(initial_df, pipeline)

    # Assert (De Controle)
    # We controleren of het eindresultaat correct is.
    # Worker 1 zou de kolom "EMA_50" met waarde 15 moeten toevoegen.
    # Worker 2 zou die kolom moeten vermenigvuldigen met 2, dus de eindwaarde moet 30 zijn.
    assert "EMA_50" in result_df.columns
    assert result_df["EMA_50"].tolist() == [30, 30, 30]

def test_context_builder_returns_copy_of_dataframe():
    """
    Tests of de ContextBuilder een kopie van de DataFrame retourneert en
    de originele niet aanpast. Dit is belangrijk om onverwachte bijeffecten
    in de rest van de applicatie te voorkomen.
    """
    # Arrange
    initial_df = pd.DataFrame({'close': [10]})
    initial_df_copy = initial_df.copy() # Maak een kopie voor de controle achteraf.

    worker = MockWorkerAddColumn(new_col_name="EMA_50", value=15)
    context_builder = ContextBuilder()

    # Act
    result_df = context_builder.build(initial_df, [worker])

    # Assert
    # Controleer of de geretourneerde DataFrame de nieuwe kolom heeft.
    assert "EMA_50" in result_df.columns
    # Controleer of de *originele* DataFrame ongewijzigd is.
    assert "EMA_50" not in initial_df.columns
    # Vergelijk de originele DataFrame met zijn kopie om zeker te zijn.
    pd.testing.assert_frame_equal(initial_df, initial_df_copy)

--- END FILE: tests/backend/assembly/test_context_builder.py ---

--- START FILE: tests/backend/assembly/test_dependency_validator.py ---
# tests/backend/assembly/test_dependency_validator.py
"""Unit tests for the DependencyValidator."""

from typing import List, Optional
from pathlib import Path
import pytest
from pytest_mock import MockerFixture

from backend.assembly.dependency_validator import DependencyValidator
from backend.assembly.plugin_registry import PluginRegistry
from backend.config.schemas.plugin_manifest_schema import PluginManifest

# CORRECTIE: Type hints toegevoegd voor duidelijkheid en linter.
def create_mock_manifest(
    name: str,
    dependencies: Optional[List[str]] = None,
    provides: Optional[List[str]] = None
) -> PluginManifest:
    """Creates a mock PluginManifest for testing purposes."""
    return PluginManifest(
        name=name, version="1.0", type="structural_context",
        description="A test plugin.",
        entry_class="Dummy", schema_path="dummy.py", params_class="DummyParams",
        dependencies=dependencies or [],
        provides=provides or []
    )

def test_valid_pipeline_succeeds(mocker: MockerFixture):
    """Tests that a logically correct pipeline validates successfully."""
    # Arrange
    mock_registry = mocker.MagicMock(spec=PluginRegistry)
    mock_registry.get_plugin_data.side_effect = {
        "ema_50": (create_mock_manifest("ema_50", ["close"], ["EMA_50"]), Path("path1")),
        "rsi_14": (create_mock_manifest("rsi_14", ["close"], ["RSI_14"]), Path("path2")),
        "logic": (create_mock_manifest("logic", ["EMA_50", "RSI_14"]), Path("path3"))
    }.get

    validator = DependencyValidator(mock_registry)
    pipeline = ["ema_50", "rsi_14", "logic"]

    # Act & Assert
    assert validator.validate(pipeline) is True

def test_pipeline_with_missing_dependency_fails(mocker: MockerFixture):
    """Tests that a pipeline fails if a dependency is never provided."""
    # Arrange
    mock_registry = mocker.MagicMock(spec=PluginRegistry)
    mock_registry.get_plugin_data.side_effect = {
        "ema_50": (create_mock_manifest("ema_50", ["close"], ["EMA_50"]), Path("path1")),
        "logic": (create_mock_manifest("logic", ["COLUMN_THAT_DOES_NOT_EXIST"]), Path("path2"))
    }.get

    validator = DependencyValidator(mock_registry)
    pipeline = ["ema_50", "logic"]

    # Act & Assert
    error_msg = "Dependency 'COLUMN_THAT_DOES_NOT_EXIST' for plugin 'logic' not met."
    with pytest.raises(ValueError, match=error_msg):
        validator.validate(pipeline)

def test_pipeline_with_incorrect_order_fails(mocker: MockerFixture):
    """Tests that a pipeline fails if dependencies are not met due to wrong order."""
    # Arrange
    mock_registry = mocker.MagicMock(spec=PluginRegistry)
    mock_registry.get_plugin_data.side_effect = {
        "logic": (create_mock_manifest("logic", ["EMA_50"]), Path("path1")),
        "ema_50": (create_mock_manifest("ema_50", ["close"], ["EMA_50"]), Path("path2")),
    }.get

    validator = DependencyValidator(mock_registry)
    pipeline = ["logic", "ema_50"]

    # Act & Assert
    with pytest.raises(ValueError, match="Dependency 'EMA_50' for plugin 'logic' not met."):
        validator.validate(pipeline)

--- END FILE: tests/backend/assembly/test_dependency_validator.py ---

--- START FILE: tests/backend/assembly/test_engine_builder.py ---
# tests/backend/assembly/test_engine_builder.py
"""
Unit tests for the EngineBuilder class.
"""
from unittest.mock import call
from pytest_mock import MockerFixture

from backend.assembly.engine_builder import EngineBuilder
from backend.assembly.worker_builder import WorkerBuilder
from backend.config.schemas.run_schema import RunBlueprint, WorkerDefinition
from backend.core.enums import PipelinePhase

def test_build_context_pipeline(mocker: MockerFixture):
    """
    Tests if the context pipeline is built correctly by only requesting
    workers from the STRUCTURAL_CONTEXT phase.
    """
    # --- Arrange ---
    mock_worker_builder = mocker.MagicMock(spec=WorkerBuilder)
    mock_run_conf = mocker.MagicMock(spec=RunBlueprint)

    # --- DE FIX: Maak de geneste mock-structuur voor de taskboard ---
    mock_taskboard = mocker.MagicMock()
    mock_taskboard.root = {
        PipelinePhase.STRUCTURAL_CONTEXT: ["worker_a", "worker_b"],
        PipelinePhase.SIGNAL_GENERATOR: ["worker_c"]  # Should be ignored
    }
    mock_run_conf.taskboard = mock_taskboard
    # ----------------------------------------------------------------

    # --- DE FIX: Mock ook de workforce met een get methode ---
    mock_workforce = mocker.MagicMock()
    mock_workforce.get.return_value = WorkerDefinition()
    mock_run_conf.workforce = mock_workforce
    # ---------------------------------------------------------

    # --- Act ---
    builder = EngineBuilder(worker_builder=mock_worker_builder)
    builder.build_context_pipeline(run_conf=mock_run_conf)

    # --- Assert ---
    # Verify that 'build' was called only for the context workers
    expected_calls = [
        call(name="worker_a", user_params={}),
        call(name="worker_b", user_params={})
    ]
    mock_worker_builder.build.assert_has_calls(expected_calls, any_order=True)
    assert mock_worker_builder.build.call_count == 2

def test_build_engine(mocker: MockerFixture):
    """
    Tests if the StrategyEngine is assembled correctly with workers from all
    phases EXCEPT the STRUCTURAL_CONTEXT phase.
    """
    # --- Arrange ---
    mock_worker_builder = mocker.MagicMock(spec=WorkerBuilder)
    mock_strategy_engine_class = mocker.patch('backend.assembly.engine_builder.StrategyEngine')
    mock_run_conf = mocker.MagicMock(spec=RunBlueprint)

    # --- DE FIX: Maak de geneste mock-structuur voor de taskboard ---
    mock_taskboard = mocker.MagicMock()
    mock_taskboard.root = {
        PipelinePhase.STRUCTURAL_CONTEXT: ["context_worker"],  # Should be ignored
        PipelinePhase.SIGNAL_GENERATOR: ["signal_worker_1"],
        PipelinePhase.SIGNAL_REFINER: ["refiner_worker_1", "refiner_worker_2"]
    }
    mock_run_conf.taskboard = mock_taskboard
    # ----------------------------------------------------------------

    # --- DE FIX: Mock ook de workforce met een get methode ---
    mock_workforce = mocker.MagicMock()
    mock_workforce.get.return_value = WorkerDefinition()
    mock_run_conf.workforce = mock_workforce
    # ---------------------------------------------------------

    # --- Act ---
    builder = EngineBuilder(worker_builder=mock_worker_builder)
    builder.build_engine(run_conf=mock_run_conf)

    # --- Assert ---
    # Verify the worker_builder was called for the correct, non-context workers
    expected_build_calls = [
        call(name="signal_worker_1", user_params={}),
        call(name="refiner_worker_1", user_params={}),
        call(name="refiner_worker_2", user_params={})
    ]
    mock_worker_builder.build.assert_has_calls(expected_build_calls, any_order=True)
    assert mock_worker_builder.build.call_count == 3

    # Verify the StrategyEngine was instantiated with the correctly grouped workers
    active_workers_arg = mock_strategy_engine_class.call_args[1]['active_workers']

    assert PipelinePhase.SIGNAL_GENERATOR.value in active_workers_arg
    assert len(active_workers_arg[PipelinePhase.SIGNAL_GENERATOR.value]) == 1

    assert PipelinePhase.SIGNAL_REFINER.value in active_workers_arg
    assert len(active_workers_arg[PipelinePhase.SIGNAL_REFINER.value]) == 2

    # Crucially, assert that the context phase was NOT included in the engine's workers
    assert PipelinePhase.STRUCTURAL_CONTEXT.value not in active_workers_arg

--- END FILE: tests/backend/assembly/test_engine_builder.py ---

--- START FILE: tests/backend/assembly/test_plugin_creator.py ---
# tests/backend/assembly/test_plugin_creator.py
"""
Unit tests for the PluginCreator service.

@layer: Tests (Backend/Assembly)
@dependencies: [pytest, unittest.mock, backend.assembly.plugin_creator]
@responsibilities:
    - Verify that the plugin skeleton is generated correctly.
    - Ensure all required files are created in the specified location.
    - Confirm that the creator handles file system operations gracefully.
"""

from pathlib import Path
from unittest.mock import MagicMock

from backend.assembly.plugin_creator import PluginCreator

def test_plugin_creator_generates_correct_skeleton(tmp_path: Path):
    """Tests if the create method successfully generates all required files.

    This test uses a mock logger to isolate the PluginCreator from the actual
    logging infrastructure and verifies that appropriate log messages are called.
    
    Args:
        tmp_path (Path): A temporary directory path provided by the pytest fixture.
    """
    # Arrange
    plugins_root = tmp_path
    plugin_name = "my_test_plugin"
    plugin_type = "signal_generator"
    base_path = plugins_root / plugin_type / plugin_name

    expected_files = [
        base_path / "plugin_manifest.yaml",
        base_path / "schema.py",
        base_path / "worker.py",
        base_path / "visualization_schema.py",
        base_path / "tests/test_worker.py",
    ]

    # Maak een mock object aan voor de LogEnricher
    mock_logger = MagicMock()

    # Injecteer de mock logger in de PluginCreator
    creator = PluginCreator(plugins_root=plugins_root, logger=mock_logger)

    # Act
    success = creator.create(name=plugin_name, plugin_type=plugin_type)

    # Assert
    assert success is True
    assert (base_path / "tests").is_dir()
    for file_path in expected_files:
        assert file_path.is_file(), f"File not found: {file_path}"

    # Verifieer dat de logger correct is gebruikt
    mock_logger.info.assert_any_call(f"Creating plugin '{plugin_name}' at: {base_path}")
    mock_logger.info.assert_any_call(f"Successfully created plugin '{plugin_name}'.")
    mock_logger.error.assert_not_called()

--- END FILE: tests/backend/assembly/test_plugin_creator.py ---

--- START FILE: tests/backend/assembly/test_plugin_registry.py ---
# tests/backend/assembly/test_plugin_registry.py
"""Unit tests for the PluginRegistry."""

import yaml
from pathlib import Path
from unittest.mock import MagicMock

from backend.assembly.plugin_registry import PluginRegistry
from backend.config.schemas.platform_schema import PlatformConfig

def test_plugin_registry_discovers_and_validates_plugins(tmp_path: Path):
    """
    Tests if the registry correctly finds valid plugins and ignores invalid ones.
    """
    # Arrange (De Voorbereiding)
    # 1. Maak een tijdelijke, realistische plugin-structuur aan.
    plugins_root = tmp_path / "plugins"
    
    # Een valide plugin
    valid_plugin_path = plugins_root / "structural_context" / "valid_plugin"
    valid_plugin_path.mkdir(parents=True)
    valid_manifest = {
        "name": "valid_plugin", "version": "1.0", "type": "structural_context",
        "description": "A valid test plugin.", "entry_class": "ValidWorker",
        "schema_path": "schema.py", "params_class": "ValidParams"
    }
    with open(valid_plugin_path / "plugin_manifest.yaml", "w") as f:
        yaml.dump(valid_manifest, f)

    # Een plugin met een ongeldig manifest (mist 'type')
    invalid_plugin_path = plugins_root / "structural_context" / "invalid_plugin"
    invalid_plugin_path.mkdir(parents=True)
    invalid_manifest = {"name": "invalid_plugin", "version": "1.0"}
    with open(invalid_plugin_path / "plugin_manifest.yaml", "w") as f:
        yaml.dump(invalid_manifest, f)
        
    # 2. Configureer het systeem om onze tijdelijke map te gebruiken.
    mock_config = MagicMock(spec=PlatformConfig)
    mock_config.plugins_root_path = str(plugins_root)
    mock_logger = MagicMock()

    # Act (De Actie)
    registry = PluginRegistry(platform_config=mock_config, logger=mock_logger)

    # Assert (De Controle)
    all_manifests = registry.get_all_manifests()
    assert len(all_manifests) == 1, "Should only register the one valid plugin."
    assert "valid_plugin" in all_manifests, "The valid plugin should be registered."
    assert "invalid_plugin" not in all_manifests, "The invalid plugin should be ignored."
    
    # Controleer of er een waarschuwing is gelogd voor het ongeldige manifest.
    assert mock_logger.warning.call_count > 0

--- END FILE: tests/backend/assembly/test_plugin_registry.py ---

--- START FILE: tests/backend/assembly/test_worker_builder.py ---
# tests/backend/assembly/test_worker_builder.py
"""Unit tests for the WorkerBuilder."""

from pathlib import Path
from unittest.mock import MagicMock

from pydantic import BaseModel, ValidationError
from pytest_mock import MockerFixture

from backend.assembly.worker_builder import WorkerBuilder
from backend.assembly.plugin_registry import PluginRegistry

# --- Test Setup: Maak een nep-omgeving ---

class MockParams(BaseModel):
    """Een nep Pydantic-schema voor een plugin."""
    value: int

class MockWorker:
    """Een nep worker-klasse."""
    def __init__(self, name: str, params: MockParams, logger):
        self.name = name
        self.params = params
        self.logger = logger

def test_worker_builder_successfully_builds_worker(mocker: MockerFixture):
    """
    Tests the happy path: building a valid worker with correct parameters.
    """
    # Arrange
    mock_registry = mocker.MagicMock(spec=PluginRegistry)
    mock_logger = mocker.MagicMock()
    
    mock_manifest = mocker.MagicMock(
        params_class="MockParams", entry_class="MockWorker",
        schema_path="schema.py"
    )
    # CORRECTIE: Geef een Path-object terug, geen string.
    mock_registry.get_plugin_data.return_value = (mock_manifest, Path("dummy/path"))
    
    mocker.patch(
        "backend.assembly.worker_builder.load_class_from_module",
        side_effect=[MockParams, MockWorker]
    )
    
    builder = WorkerBuilder(mock_registry, mock_logger)
    
    # Act
    worker_instance = builder.build(name="my_worker", user_params={"value": 123})
    
    # Assert
    assert isinstance(worker_instance, MockWorker)
    assert worker_instance.name == "my_worker"
    assert worker_instance.params.value == 123
    mock_logger.info.assert_called_with("Successfully built worker 'my_worker'.")

def test_worker_builder_fails_on_invalid_params(mocker: MockerFixture):
    """
    Tests if the builder correctly fails when user parameters are invalid.
    """
    # Arrange
    mock_registry = mocker.MagicMock(spec=PluginRegistry)
    mock_logger = mocker.MagicMock()
    
    mock_manifest = mocker.MagicMock(
        params_class="MockParams", entry_class="MockWorker",
        schema_path="schema.py"
    )
    # CORRECTIE: Geef een Path-object terug, geen string.
    mock_registry.get_plugin_data.return_value = (mock_manifest, Path("dummy/path"))

    mocker.patch(
        "backend.assembly.worker_builder.load_class_from_module",
        side_effect=[MockParams, MockWorker]
    )
    
    builder = WorkerBuilder(mock_registry, mock_logger)
    
    # Act
    worker_instance = builder.build(name="my_worker", user_params={"value": "not-an-int"})

    # Assert
    assert worker_instance is None
    mock_logger.error.assert_called_once()
    assert "Invalid parameters for worker 'my_worker'" in mock_logger.error.call_args[0][0]

--- END FILE: tests/backend/assembly/test_worker_builder.py ---

--- START FILE: tests/backend/assembly/__init__.py ---

--- END FILE: tests/backend/assembly/__init__.py ---

--- START FILE: tests/backend/core/test_context_recoreder.py ---
# tests/backend/core/test_context_recorder.py
"""Unit tests for the ContextRecorder."""

import uuid
from datetime import datetime
import pandas as pd
from pydantic import BaseModel

from backend.core.context_recorder import ContextRecorder

# --- Test Setup ---

class MockContextObject(BaseModel):
    """Een simpele Pydantic-klasse om een context-object te simuleren."""
    value: int
    label: str

# --- De Test ---

def test_context_recorder_adds_and_serializes_data():
    """
    Tests if the ContextRecorder correctly adds data, serializes the Pydantic
    model, and structures the log correctly.
    """
    # Arrange (De Voorbereiding)
    recorder = ContextRecorder()
    
    test_timestamp = pd.to_datetime("2023-01-01 10:00:00", utc=True)
    test_specialist = "my_test_plugin"
    test_context_obj = MockContextObject(value=123, label="test")
    test_corr_id = uuid.uuid4()

    # Act (De Actie)
    # Voeg de data toe aan de recorder. Dit is de methode die we testen.
    recorder.add_data(
        correlation_id=test_corr_id,
        timestamp=test_timestamp,
        specialist_name=test_specialist,
        context_object=test_context_obj
    )

    # Assert (De Controle)
    # 1. Haal alle data op uit de recorder.
    all_data = recorder.get_all_data()

    # 2. Controleer de structuur.
    assert test_timestamp in all_data, "Timestamp should be the primary key."
    assert test_specialist in all_data[test_timestamp], "Specialist name should be the secondary key."

    # 3. Controleer de inhoud.
    logged_context = all_data[test_timestamp][test_specialist]
    
    # Is het Pydantic-object correct omgezet naar een dictionary?
    assert isinstance(logged_context, dict)
    assert logged_context['value'] == 123
    assert logged_context['label'] == "test"
    
    # Is de correlation_id correct toegevoegd?
    assert 'correlation_id' in logged_context
    assert logged_context['correlation_id'] == str(test_corr_id)

--- END FILE: tests/backend/core/test_context_recoreder.py ---

--- START FILE: tests/backend/core/test_directive_flattener.py ---
# tests/backend/core/test_directive_flattener.py
"""
Unit tests for the DirectiveFlattener utility.
"""

import uuid
import pandas as pd
from backend.dtos.signal import Signal
from backend.dtos.entry_signal import EntrySignal
from backend.dtos.risk_defined_signal import RiskDefinedSignal
from backend.dtos.trade_plan import TradePlan
from backend.dtos.routed_trade_plan import RoutedTradePlan
from backend.dtos.execution_directive import ExecutionDirective
from backend.core.directive_flattener import DirectiveFlattener

def test_flatten_routed_trade_plan_to_directive():
    """
    Tests if a deeply nested RoutedTradePlan is correctly flattened into a
    simple ExecutionDirective.
    """
    # --- Arrange (De Voorbereiding) ---
    test_corr_id = uuid.uuid4()
    test_timestamp = pd.Timestamp("2023-01-01 10:00:00", tz='UTC')

    signal = Signal(correlation_id=test_corr_id, timestamp=test_timestamp, asset="BTC/EUR", direction="long", signal_type="fvg_entry_signal")
    entry_signal = EntrySignal(correlation_id=test_corr_id, signal=signal, entry_price=25000.0)
    risk_defined_signal = RiskDefinedSignal(correlation_id=test_corr_id, entry_signal=entry_signal, sl_price=24800.0, tp_price=25500.0)
    trade_plan = TradePlan(correlation_id=test_corr_id, risk_defined_signal=risk_defined_signal, position_value_quote=1000.0, position_size_asset=0.04)
    routed_trade_plan = RoutedTradePlan(correlation_id=test_corr_id, trade_plan=trade_plan, order_type='limit', limit_price=25000.0, time_in_force='GTC', post_only=True)
    
    expected_directive = ExecutionDirective(
        correlation_id=test_corr_id,
        signal_type="fvg_entry_signal",
        asset="BTC/EUR",
        direction="long",
        entry_price=25000.0,
        sl_price=24800.0,
        tp_price=25500.0,
        position_value_quote=1000.0,
        position_size_asset=0.04,
        order_type='limit',
        limit_price=25000.0,
        time_in_force='GTC',
        post_only=True,
        entry_time=test_timestamp
    )

    # --- Act (De Actie) ---
    flattener = DirectiveFlattener()
    actual_directive = flattener.flatten(routed_trade_plan)

    # --- Assert (De Controle) ---
    assert actual_directive == expected_directive

--- END FILE: tests/backend/core/test_directive_flattener.py ---

--- START FILE: tests/backend/core/test_portfolio.py ---
# tests/backend/core/test_portfolio.py
import pytest
import uuid
import pandas as pd

from backend.core.interfaces.portfolio import Tradable
from backend.core.portfolio import Portfolio
from backend.dtos.execution_directive import ExecutionDirective

class MockLogger:
    def info(self, *args, **kwargs): pass
    def trade(self, *args, **kwargs): pass
    def error(self, *args, **kwargs): pass

class MockContextRecorder:
    def add_data(self, *args, **kwargs): pass

@pytest.fixture
def empty_portfolio() -> Portfolio:
    return Portfolio(
        initial_capital=10000.0, fees_pct=0.001,
        logger=MockLogger(), context_recorder=MockContextRecorder()
    )

@pytest.fixture
def sample_directive() -> ExecutionDirective:
    """A fixture for a sample long execution directive."""
    return ExecutionDirective(
        correlation_id=uuid.uuid4(),
        signal_type='TEST_SIGNAL_LONG',
        entry_time=pd.to_datetime("2023-01-01 10:00:00"),
        asset="BTC/EUR",
        direction='long',
        entry_price=20000.0,
        sl_price=19800.0,
        tp_price=20400.0,
        position_value_quote=1000.0,
        position_size_asset=0.05,
        order_type='limit'  # Toegevoegd verplicht veld
    )

@pytest.fixture
def sample_short_directive() -> ExecutionDirective:
    """A fixture for a sample short execution directive."""
    return ExecutionDirective(
        correlation_id=uuid.uuid4(),
        signal_type='TEST_SIGNAL_SHORT',
        entry_time=pd.to_datetime("2023-01-02 10:00:00"),
        asset="ETH/EUR",
        direction='short',
        entry_price=1500.0,
        sl_price=1520.0,
        tp_price=1460.0,
        position_value_quote=1500.0,
        position_size_asset=1.0,
        order_type='market' # Toegevoegd verplicht veld
    )

# --- Test Cases ---

def test_portfolio_fulfills_tradable_contract(empty_portfolio: Portfolio):
    assert isinstance(empty_portfolio, Tradable)

def test_portfolio_initialization(empty_portfolio: Portfolio):
    assert empty_portfolio.balance == 10000.0
    assert empty_portfolio.initial_capital == 10000.0
    assert not empty_portfolio.active_trades
    assert len(empty_portfolio.closed_trades) == 0

def test_open_trade_success(empty_portfolio: Portfolio, sample_directive: ExecutionDirective):
    empty_portfolio.open_trade(sample_directive)
    assert len(empty_portfolio.active_trades) == 1
    active_trade_data = empty_portfolio.active_trades[sample_directive.correlation_id]
    assert active_trade_data['direction'] == 'long'
    assert active_trade_data['entry_price'] == 20000.0
    assert active_trade_data['position_size_asset'] == 0.05

def test_open_multiple_trades_on_different_assets(empty_portfolio: Portfolio, sample_directive: ExecutionDirective):
    eth_directive = sample_directive.model_copy(update={
        'correlation_id': uuid.uuid4(),
        'signal_type': 'TEST_SIGNAL_ETH_LONG',
        'entry_time': pd.to_datetime("2023-01-01 11:00:00"),
        'asset': "ETH/EUR",
        'entry_price': 1500.0,
        'sl_price': 1480.0,
        'tp_price': 1550.0,
        'position_value_quote': 500.0,
        'position_size_asset': 0.33
    })
    empty_portfolio.open_trade(sample_directive)
    empty_portfolio.open_trade(eth_directive)
    
    active_trades_dict = empty_portfolio.active_trades
    assert len(active_trades_dict) == 2
    assert sample_directive.correlation_id in active_trades_dict
    assert eth_directive.correlation_id in active_trades_dict

def test_process_candle_closes_long_trade_on_stop_loss(empty_portfolio: Portfolio, sample_directive: ExecutionDirective):
    empty_portfolio.open_trade(sample_directive)
    assert empty_portfolio.active_trade_count == 1

    candle_timestamp = pd.to_datetime("2023-01-01 11:00:00")
    killer_candle = pd.Series({
        "open": 19900.0, "high": 19950.0,
        "low": 19800.0, "close": 19850.0
    }, name=candle_timestamp)

    empty_portfolio.process_candle(killer_candle)

    assert empty_portfolio.active_trade_count == 0
    assert len(empty_portfolio.closed_trades) == 1

    closed_trade = empty_portfolio.closed_trades[0]
    assert closed_trade.exit_price == 19800.0
    assert closed_trade.pnl_quote < 0

def test_process_candle_closes_long_trade_on_take_profit(empty_portfolio: Portfolio, sample_directive: ExecutionDirective):
    empty_portfolio.open_trade(sample_directive)
    assert empty_portfolio.active_trade_count == 1

    profit_candle_timestamp = pd.to_datetime("2023-01-01 12:00:00")
    profit_candle = pd.Series({
        "open": 20200.0, "high": 20400.0,
        "low": 20150.0, "close": 20350.0
    }, name=profit_candle_timestamp)

    empty_portfolio.process_candle(profit_candle)

    assert empty_portfolio.active_trade_count == 0
    assert len(empty_portfolio.closed_trades) == 1

    closed_trade = empty_portfolio.closed_trades[0]
    assert closed_trade.exit_price == 20400.0
    assert closed_trade.pnl_quote > 0

def test_process_candle_closes_short_trade_on_stop_loss(empty_portfolio: Portfolio, sample_short_directive: ExecutionDirective):
    empty_portfolio.open_trade(sample_short_directive)
    assert empty_portfolio.active_trade_count == 1

    killer_candle_timestamp = pd.to_datetime("2023-01-02 11:00:00")
    killer_candle = pd.Series({
        "open": 1510.0, "high": 1520.0,
        "low": 1505.0, "close": 1515.0
    }, name=killer_candle_timestamp)

    empty_portfolio.process_candle(killer_candle)

    assert empty_portfolio.active_trade_count == 0
    assert len(empty_portfolio.closed_trades) == 1

    closed_trade = empty_portfolio.closed_trades[0]
    assert closed_trade.exit_price == 1520.0
    assert closed_trade.pnl_quote < 0

def test_process_candle_closes_short_trade_on_take_profit(empty_portfolio: Portfolio, sample_short_directive: ExecutionDirective):
    empty_portfolio.open_trade(sample_short_directive)
    assert empty_portfolio.active_trade_count == 1

    profit_candle_timestamp = pd.to_datetime("2023-01-02 12:00:00")
    profit_candle = pd.Series({
        "open": 1470.0, "high": 1475.0,
        "low": 1460.0, "close": 1465.0
    }, name=profit_candle_timestamp)

    empty_portfolio.process_candle(profit_candle)

    assert empty_portfolio.active_trade_count == 0
    assert len(empty_portfolio.closed_trades) == 1

    closed_trade = empty_portfolio.closed_trades[0]
    assert closed_trade.exit_price == 1460.0
    assert closed_trade.pnl_quote > 0

--- END FILE: tests/backend/core/test_portfolio.py ---

--- START FILE: tests/backend/core/test_strategy_engine.py ---
# tests/backend/core/test_strategy_engine.py
"""Unit tests for the StrategyEngine."""

import uuid
import pandas as pd
from pytest_mock import MockerFixture

from backend.dtos import (
    Signal, EntrySignal, RiskDefinedSignal, TradePlan, RoutedTradePlan,
    TradingContext, EngineCycleResult, ExecutionDirective, CriticalEvent
)
from backend.core.interfaces import Clock
from backend.core.strategy_engine import StrategyEngine
from backend.core.directive_flattener import DirectiveFlattener

def test_strategy_engine_yields_correct_result(mocker: MockerFixture):
    """
    Tests that the StrategyEngine correctly processes the full 9-phase pipeline
    and yields a final, complete EngineCycleResult.
    """
    # --- Arrange (De Voorbereiding) ---
    mock_clock = mocker.MagicMock(spec=Clock)
    test_time = pd.Timestamp("2023-01-01 10:00:00", tz='UTC')
    mock_clock.tick.return_value = [(test_time, pd.Series())]
    corr_id = uuid.uuid4()

    # 1. Bouw de volledige, geneste DTO-keten op
    signal_dto = Signal(correlation_id=corr_id, timestamp=test_time, asset="BTC/EUR", direction="long", signal_type="test_signal")
    entry_signal_dto = EntrySignal(correlation_id=corr_id, signal=signal_dto, entry_price=100.0)
    risk_defined_dto = RiskDefinedSignal(correlation_id=corr_id, entry_signal=entry_signal_dto, sl_price=95.0, tp_price=110.0)
    trade_plan_dto = TradePlan(correlation_id=corr_id, risk_defined_signal=risk_defined_dto, position_value_quote=10000.0, position_size_asset=1.0)
    routed_plan_dto = RoutedTradePlan(correlation_id=corr_id, trade_plan=trade_plan_dto, order_type='limit', limit_price=100.0)
    
    expected_directive = DirectiveFlattener().flatten(routed_plan_dto)

    # 2. Mocks voor elke worker, nu geconfigureerd voor de .process() methode
    # --- DE FIX: Gebruik overal .process in de mocks ---
    mock_signal_generator = mocker.MagicMock(process=mocker.Mock(return_value=[signal_dto]))
    mock_signal_refiner = mocker.MagicMock(process=mocker.Mock(return_value=signal_dto))
    mock_entry_planner = mocker.MagicMock(process=mocker.Mock(return_value=entry_signal_dto))
    mock_exit_planner = mocker.MagicMock(process=mocker.Mock(return_value=risk_defined_dto))
    mock_size_planner = mocker.MagicMock(process=mocker.Mock(return_value=trade_plan_dto))
    mock_order_router = mocker.MagicMock(process=mocker.Mock(return_value=routed_plan_dto))
    mock_event_detector = mocker.MagicMock(process=mocker.Mock(return_value=[]))

    active_workers = {
        'signal_generator': [mock_signal_generator],
        'signal_refiner': [mock_signal_refiner],
        'entry_planner': mock_entry_planner,
        'exit_planner': mock_exit_planner,
        'size_planner': mock_size_planner,
        'order_router': [mock_order_router],
        'critical_event_detector': [mock_event_detector]
    }

    # --- Act (De Actie) ---
    engine = StrategyEngine(active_workers=active_workers)
    cycle_results = list(engine.run(
        trading_context=mocker.MagicMock(spec=TradingContext),
        clock=mock_clock
    ))

    # --- Assert (De Controle) ---
    assert len(cycle_results) == 1
    result = cycle_results[0]
    
    assert isinstance(result, EngineCycleResult)
    assert len(result.execution_directives) == 1
    assert len(result.critical_events) == 0
    assert result.execution_directives[0] == expected_directive

    # Valideer dat de correcte methodes zijn aangeroepen
    mock_signal_generator.process.assert_called_once()
    mock_signal_refiner.process.assert_called_once_with(signal_dto, mocker.ANY)
    mock_entry_planner.process.assert_called_once_with(signal_dto, mocker.ANY)
    mock_exit_planner.process.assert_called_once_with(entry_signal_dto, mocker.ANY)
    mock_size_planner.process.assert_called_once_with(risk_defined_dto, mocker.ANY)
    mock_order_router.process.assert_called_once_with(trade_plan_dto, mocker.ANY)
    mock_event_detector.process.assert_called_once()

--- END FILE: tests/backend/core/test_strategy_engine.py ---

--- START FILE: tests/backend/data/test_data_loader.py ---
# tests/backend/data/test_data_loader.py
"""Unit tests for the DataLoader."""

import os
import re # <-- Importeer de 're' module
from pathlib import Path
import pandas as pd
import pytest
from pytest_mock import MockerFixture

from backend.data.loader import DataLoader

def test_data_loader_successfully_loads_csv(tmp_path: Path, mocker: MockerFixture):
    """
    Tests if the DataLoader correctly reads a valid CSV file, sets the index,
    and returns a DataFrame.
    """
    # Arrange
    csv_content = "timestamp,open,high,low,close,volume\n" \
                  "2023-01-01 10:00:00,100,105,99,102,1000\n" \
                  "2023-01-01 10:01:00,102,103,101,102,500"
    
    data_file = tmp_path / "test_data.csv"
    data_file.write_text(csv_content)
    
    mock_logger = mocker.MagicMock()

    # Act
    loader = DataLoader(file_path=str(data_file), logger=mock_logger)
    df = loader.load()

    # Assert
    assert isinstance(df, pd.DataFrame)
    assert len(df) == 2
    assert isinstance(df.index, pd.DatetimeIndex)
    assert df.index.name == "timestamp"
    mock_logger.info.assert_called_with('loader.load_success')

def test_data_loader_raises_error_for_nonexistent_file(mocker: MockerFixture):
    """
    Tests if the DataLoader raises a FileNotFoundError when the file does not exist.
    This test is now platform-independent.
    """
    # Arrange
    # Bouw het pad op een platform-onafhankelijke manier.
    non_existent_path = os.path.join("path", "that", "does", "not", "exist.csv")
    mock_logger = mocker.MagicMock()

    # CORRECTIE: "Escape" de pad-string zodat de regex-engine backslashes
    # als letterlijke tekens behandelt en niet als speciale codes.
    expected_error_pattern = re.escape(non_existent_path)

    # Act & Assert
    with pytest.raises(FileNotFoundError, match=expected_error_pattern):
        DataLoader(file_path=non_existent_path, logger=mock_logger)

--- END FILE: tests/backend/data/test_data_loader.py ---

--- START FILE: tests/backend/data/__init__.py ---

--- END FILE: tests/backend/data/__init__.py ---

--- START FILE: tests/backend/environments/test_backtest_environment.py ---
# tests/backend/environments/test_backtest_environment.py
"""Unit tests for the BacktestEnvironment."""

import pandas as pd
from pytest_mock import MockerFixture

from backend.environments.backtest_environment import (
    BacktestEnvironment, CSVDataSource, SimulatedClock, BacktestExecutionHandler
)
from backend.core.interfaces import Tradable
from backend.config.schemas.app_schema import AppConfig
from backend.config.schemas.platform_schema import PlatformConfig, PlatformDataConfig
from backend.config.schemas.run_schema import RunBlueprint, RunDataConfig

def test_backtest_environment_initialization(mocker: MockerFixture):
    """
    Tests if the BacktestEnvironment correctly initializes its
    specialized sub-components.
    """
    # Arrange
    mock_df = pd.DataFrame({'close': [1, 2, 3]})
    mocker.patch("backend.data.loader.DataLoader.load", return_value=mock_df)
    mocker.patch("backend.data.loader.DataLoader.__init__", return_value=None)

    # --- DE FIX: Bouw een correcte, geneste mock AppConfig ---
    mock_app_config = mocker.MagicMock(spec=AppConfig)
    mock_app_config.platform = mocker.MagicMock(spec=PlatformConfig)
    mock_app_config.run = mocker.MagicMock(spec=RunBlueprint)

    # Definieer de geneste data-objecten
    mock_app_config.platform.data = mocker.MagicMock(spec=PlatformDataConfig)
    mock_app_config.run.data = mocker.MagicMock(spec=RunDataConfig)

    # Wijs de waarden toe op de juiste, geneste locatie
    mock_app_config.platform.data.source_dir = "mock_data_dir"
    mock_app_config.run.data.trading_pair = "BTC/EUR"
    mock_app_config.run.data.timeframe = "1h"

    mock_portfolio = mocker.MagicMock(spec=Tradable)

    # Act
    environment = BacktestEnvironment(
        app_config=mock_app_config,
        tradable=mock_portfolio
    )

    # Assert
    assert isinstance(environment.source, CSVDataSource)
    assert isinstance(environment.clock, SimulatedClock)
    assert isinstance(environment.handler, BacktestExecutionHandler)

--- END FILE: tests/backend/environments/test_backtest_environment.py ---

--- START FILE: tests/backend/environments/__init__.py ---

--- END FILE: tests/backend/environments/__init__.py ---

--- START FILE: tests/services/test_strategy_operator.py ---
# tests/services/test_strategy_operator.py
"""
Unit tests for the refactored StrategyOperator service.
"""
from pytest_mock import MockerFixture

# De klasse die we gaan testen
from services.strategy_operator import StrategyOperator

# De klassen die we gaan mocken
from backend.config.schemas.app_schema import AppConfig

def test_strategy_operator_run_orchestration(mocker: MockerFixture):
    """
    Tests if the main `run` method correctly calls the private helper
    methods in the correct order, confirming its role as a pure orchestrator.
    """
    # --- Arrange ---
    # Mock the private helper methods that contain the actual logic
    mock_prepare_components = mocker.patch(
        'services.strategy_operator.StrategyOperator._prepare_components'
    )
    mock_prepare_data = mocker.patch('services.strategy_operator.StrategyOperator._prepare_data')
    mock_run_cycle = mocker.patch(
        'services.strategy_operator.StrategyOperator._run_operational_cycle'
    )

    # Create dummy config and logger for instantiation
    mock_app_config = mocker.MagicMock(spec=AppConfig)
    mock_logger = mocker.MagicMock()

    # --- Act ---
    operator = StrategyOperator(app_config=mock_app_config, logger=mock_logger)
    operator.run()

    # --- Assert ---
    # Verify that the main orchestration methods were called exactly once
    mock_prepare_components.assert_called_once()
    mock_prepare_data.assert_called_once()
    mock_run_cycle.assert_called_once()

--- END FILE: tests/services/test_strategy_operator.py ---

--- START FILE: tools/bootstrap_v2.py ---
# bootstrap_v2.py
"""
This script generates the complete directory and file skeleton for the S1mpleTrader V2 architecture.

It creates all necessary directories and populates them with initial files,
including boilerplate content like file headers and class definitions, adhering
to the project's coding standards.

@layer: Tool
@dependencies:
    - os
    - pathlib
@responsibilities:
    - Creates the main directory structure for the V2 application.
    - Generates placeholder files with correct headers and initial content.
    - Establishes a clean, consistent starting point for V2 development.
@inputs: None
@outputs: A complete directory structure on the filesystem.
"""

import os
from pathlib import Path

# --- Configuration ---
ROOT_DIR = Path("S1mpleTrader_V2")

# The full directory and file structure based on our architecture design
STRUCTURE = {
    "config": {
        "runs": {"mss_fvg_strategy.yaml": "# V2 Strategy Blueprint: MSS with FVG Entry"},
        "optimizations": {"optimize_atr_params.yaml": "# V2 Optimization Blueprint: Tune ATR Exit Parameters"},
        "variants": {"robustness_test.yaml": "# V2 Variant Test: Test strategy across multiple markets"},
        "overrides": {"use_eth_pair.yaml": "# V2 Override: Switch trading pair to ETH/EUR"},
        "index.yaml": "# Central index mapping short names to blueprint file paths",
        "platform.yaml": "# Global platform settings: portfolio, logging, etc.",
    },
    "plugins": {
        "regime_filters": {
            "adx_trend_filter": {
                "__init__.py": None,
                "plugin_manifest.yaml": "name: adx_trend_filter\ntype: regime_filter\n...",
                "worker.py": "AdxTrendFilter",
                "schema.py": "AdxTrendFilterParams",
                "tests": {"test_worker.py": "TestAdxTrendFilter"},
            }
        },
        "structural_context": {
            "market_structure_detector": {
                "__init__.py": None,
                "plugin_manifest.yaml": "name: market_structure_detector\ntype: structural_context\n...",
                "worker.py": "MarketStructureDetector",
                "schema.py": "MarketStructureDetectorParams",
                "tests": {"test_worker.py": "TestMarketStructureDetector"},
            }
        },
        "signal_generators": {
            "fvg_entry_detector": {
                "__init__.py": None,
                "plugin_manifest.yaml": "name: fvg_entry_detector\ntype: signal_generator\n...",
                "worker.py": "FvgEntryDetector",
                "schema.py": "FvgEntryDetectorParams",
                "tests": {"test_worker.py": "TestFvgEntryDetector"},
            }
        },
        "signal_refiners": {
            "volume_spike_refiner": {
                "__init__.py": None,
                "plugin_manifest.yaml": "name: volume_spike_refiner\ntype: signal_refiner\n...",
                "worker.py": "VolumeSpikeRefiner",
                "schema.py": "VolumeSpikeRefinerParams",
                "tests": {"test_worker.py": "TestVolumeSpikeRefiner"},
            }
        },
        "trade_constructors": {
            "liquidity_target_exit": {
                "__init__.py": None,
                "plugin_manifest.yaml": "name: liquidity_target_exit\ntype: trade_constructor\n...", # This would likely be an exit planner
                "worker.py": "LiquidityTargetExitPlanner",
                "schema.py": "LiquidityTargetExitParams",
                "tests": {"test_worker.py": "TestLiquidityTargetExitPlanner"},
            }
        },
        "portfolio_overlays": {
            "max_drawdown_overlay": {
                "__init__.py": None,
                "plugin_manifest.yaml": "name: max_drawdown_overlay\ntype: portfolio_overlay\n...",
                "worker.py": "MaxDrawdownOverlay",
                "schema.py": "MaxDrawdownOverlayParams",
                "tests": {"test_worker.py": "TestMaxDrawdownOverlay"},
            }
        },
    },
    "backend": {
        "config": {"schemas": {"__init__.py": None, "app_schema.py": "AppConfig", "blueprint_schema.py": "BlueprintConfig"}},
        "assembly": {
            "__init__.py": None,
            "plugin_registry.py": "PluginRegistry",
            "worker_builder.py": "WorkerBuilder",
            "context_pipeline_runner.py": "ContextPipelineRunner",
        },
        "core": {
            "__init__.py": None,
            "portfolio.py": "Portfolio",
            "execution.py": "ExecutionHandler",
            "performance_analyzer.py": "PerformanceAnalyzer",
            "interfaces.py": "# Contains abstract base classes like Tradable",
            "constants.py": "# Application-wide constants",
        },
        "environments": {
            "__init__.py": None,
            "base_environment.py": "ExecutionEnvironment",
            "backtest_environment.py": "BacktestEnvironment",
            "paper_environment.py": "PaperTradeEnvironment",
            "live_environment.py": "LiveTradeEnvironment",
        },
        "dtos": {
            "__init__.py": None,
            "signal.py": "# Signal DTO dataclass",
            "trade.py": "# Trade DTO dataclass",
            "closed_trade.py": "# ClosedTrade DTO dataclass",
            "backtest_result.py": "# BacktestResult DTO dataclass",
        },
        "utils": {
            "__init__.py": None,
            "app_logger.py": "LogEnricher",
            "translator.py": "Translator",
            "data_utils.py": "# Utility functions for data manipulation",
        },
    },
    "services": {
        "__init__.py": None,
        "strategy_operator.py": "StrategyOperator",
        "optimization_service.py": "OptimizationService",
        "variant_test_service.py": "VariantTestService",
        "parallel_run_service.py": "ParallelRunService",
        "api_services": {
            "__init__.py": None,
            "plugin_query_service.py": "PluginQueryService",
            "visualization_service.py": "VisualizationService",
        },
    },
    "frontends": {
        "web": {
            "api": {
                "__init__.py": None,
                "main.py": "# FastAPI application entry point",
                "routers": {
                    "__init__.py": None,
                    "plugins_router.py": "# API endpoints for plugins",
                    "backtest_router.py": "# API endpoints for running backtests",
                },
            },
            "ui": {
                "src": {"components": {}, "services": {}, "App.tsx": "// Main React/TypeScript component"},
                "package.json": '{\n  "name": "s1mpletrader-ui",\n  "version": "0.1.0"\n}',
            },
        },
        "cli": {
            "presenters": {"optimization_presenter.py": "OptimizationPresenter"},
            "reporters": {"cli_reporter.py": "CliReporter"},
        },
    },
    "locales": {"en.yaml": "app:\n  start: \"S1mpleTrader is starting...\"", "nl.yaml": "app:\n  start: \"S1mpleTrader wordt gestart...\""},
    "tools": {"generate_structure.py": None, "plugin_creator.py": "# A helper script to bootstrap a new plugin directory"},
    "source_data": {"BTC_EUR_15m.csv": "timestamp,open,high,low,close,volume\n..."},
    "results": {
        "20250924_213000_mss_fvg_strategy": {
            "run_config.yaml": None,
            "result_trades.csv": None,
            "result_metrics.yaml": None,
            "run.log.json": None,
        }
    },
    "run_web.py": "# Entrypoint: Starts the Web UI and API",
    "run_supervisor.py": "# Entrypoint: Starts the live trading supervisor",
    "run_backtest_cli.py": "# Entrypoint: For automated (headless) runs",
    "pyproject.toml": '[tool.poetry]\nname = "s1mpletrader-v2"\nversion = "2.0.0"\ndescription = ""\nauthors = ["Your Name <you@example.com>"]',
}

def create_py_content(file_path_str: str, class_name: str) -> str:
    """Generates standard Python file content based on coding standards."""
    return f'# {file_path_str}\n"""\nDocstring for {os.path.basename(file_path_str)}.\n\n@layer: TODO\n@dependencies: TODO\n@responsibilities: TODO\n"""\n\nclass {class_name}:\n    """Docstring for {class_name}."""\n    pass\n'

def create_file_content(file_path: Path, content_instruction: str) -> str:
    """Creates the appropriate boilerplate content for a given file type."""
    file_path_str = str(file_path.relative_to(ROOT_DIR)).replace("\\", "/")
    
    if content_instruction and file_path.suffix == ".py":
        if content_instruction.startswith("#"): # It's a comment, not a class name
             return f'# {file_path_str}\n"""\n{content_instruction[2:]}\n"""\n'
        return create_py_content(file_path_str, content_instruction)
    
    if file_path.name == "__init__.py":
        return "" # Empty init file
        
    if content_instruction and not content_instruction.startswith("#"):
        return content_instruction
    
    return f"# Placeholder for {file_path_str}\n"


def build_structure(current_path: Path, structure_dict: dict):
    """Recursively builds the directory and file structure."""
    current_path.mkdir(exist_ok=True)
    for name, content in structure_dict.items():
        new_path = current_path / name
        if isinstance(content, dict):
            build_structure(new_path, content)
        else:
            print(f"Creating file: {new_path}")
            file_content = create_file_content(new_path, content)
            with open(new_path, "w", encoding="utf-8") as f:
                f.write(file_content)

if __name__ == "__main__":
    if ROOT_DIR.exists():
        print(f"Directory '{ROOT_DIR}' already exists. Please remove it first to start fresh.")
    else:
        print(f"--- 🚀 Bootstrapping S1mpleTrader V2 Structure in '{ROOT_DIR}' ---")
        build_structure(ROOT_DIR, STRUCTURE)
        print("\n--- ✅ Structure created successfully! ---")

--- END FILE: tools/bootstrap_v2.py ---

--- START FILE: tools/combine_docs.py ---
import os

def merge_markdown_files(input_folder, output_file):
    with open(output_file, 'w', encoding='utf-8') as outfile:
        for filename in os.listdir(input_folder):
            if filename.endswith(".md"):
                file_path = os.path.join(input_folder, filename)
                with open(file_path, 'r', encoding='utf-8') as infile:
                    outfile.write(f"# {filename}\n\n")  # optioneel: titel toevoegen
                    outfile.write(infile.read())
                    outfile.write("\n\n---\n\n")  # scheiding tussen bestanden

if __name__ == "__main__":
    input_folder = "./docs/system"       # vervang door jouw map
    output_file = "docs.md" # naam van het resultaat
    merge_markdown_files(input_folder, output_file)
    print(f"Alle bestanden samengevoegd in: {output_file}")
--- END FILE: tools/combine_docs.py ---

--- START FILE: tools/generate_structure.py ---
# tools/generate_structure.py
"""
Generates a text file representing the project's directory and file structure.

This script scans the project from the root directory, respects the rules
in the .gitignore file, and outputs a clean, indented tree structure to a
specified text file.

@layer: Tool
"""

# 1. Standard Library Imports
import os
import fnmatch
from pathlib import Path

# --- CONFIGURATION ---
PROJECT_ROOT = Path(__file__).resolve().parent.parent
OUTPUT_PATH = PROJECT_ROOT / "docs" / "folder_file_structure.txt"
GITIGNORE_PATH = PROJECT_ROOT / ".gitignore"

# Default patterns to always ignore, even if not in .gitignore
DEFAULT_IGNORE = {".git", ".vscode", "*.pyc", "*__pycache__*", ".DS_Store"}


def read_gitignore() -> set:
    """
    Reads and parses the .gitignore file.

    Returns:
        set: A set of patterns to be ignored.
    """
    if not GITIGNORE_PATH.is_file():
        print("Warning: .gitignore file not found at project root.")
        return set()

    with open(GITIGNORE_PATH, 'r', encoding='utf-8') as f:
        patterns = {
            line.strip() for line in f
            if line.strip() and not line.startswith('#')
        }
    return patterns


def should_ignore(path: Path, ignore_patterns: set) -> bool:
    """
    Checks if a path should be ignored using a simplified interpretation
    of .gitignore-style patterns.

    @inputs:
        path (Path): The path object relative to the project root.
        ignore_patterns (set): A set of patterns from .gitignore and defaults.

    @outputs:
        bool: True if the path should be ignored, False otherwise.
    """
    for pattern in ignore_patterns:
        # Handle directory-only patterns (e.g., 'build/', '__pycache__/')
        if pattern.endswith('/'):
            if pattern.rstrip('/') in path.parts:
                return True
        # Handle file/general patterns (e.g., '*.pyc', '.DS_Store')
        elif fnmatch.fnmatch(path.name, pattern):
            return True
    return False


def generate_structure(directory: Path, ignore_patterns: set, prefix: str = "") -> str:
    """
    Recursively generates the directory structure string.
    """
    structure = ""
    # Create a sorted list of items to process
    try:
        items = sorted(list(directory.iterdir()), key=lambda p: (p.is_file(), p.name.lower()))
    except FileNotFoundError:
        return "" # Directory might have been deleted mid-run

    # Create a list of non-ignored items to correctly determine the last element
    valid_items = [p for p in items if not should_ignore(p.relative_to(PROJECT_ROOT), ignore_patterns)]

    for i, path in enumerate(valid_items):
        is_last = (i == len(valid_items) - 1)
        connector = "└── " if is_last else "├── "
        structure += f"{prefix}{connector}{path.name}\n"

        if path.is_dir():
            extension = "    " if is_last else "│   "
            structure += generate_structure(
                path, ignore_patterns, prefix + extension
            )

    return structure


def main():
    """Main function to generate and save the structure file."""
    print("Generating project structure...")

    ignore_patterns = DEFAULT_IGNORE.union(read_gitignore())
    project_name = PROJECT_ROOT.name
    tree_string = generate_structure(PROJECT_ROOT, ignore_patterns)
    full_output = f"{project_name}/\n{tree_string}"

    OUTPUT_PATH.parent.mkdir(parents=True, exist_ok=True)

    with open(OUTPUT_PATH, "w", encoding="utf-8") as f:
        f.write(full_output)

    print(f"✅ Project structure saved to: {OUTPUT_PATH}")


if __name__ == "__main__":
    main()
--- END FILE: tools/generate_structure.py ---

--- START FILE: tools/__init__.py ---

--- END FILE: tools/__init__.py ---

