--- START FILE: GEMINI.md ---
# S1mpleTrader V2 - AI Assistent Instructies

Hallo! Ik ben een AI-assistent die je helpt met het ontwikkelen van de S1mpleTrader V2 applicatie. Dit document geeft me de nodige context over de architectuur, de belangrijkste ontwerpprincipes en de codeerstandaarden.

## 1. Visie & Kernprincipes

Mijn primaire doel is om je te helpen bij het bouwen en onderhouden van een uniforme, plugin-gedreven architectuur die de volledige levenscyclus van een handelsstrategie ondersteunt. Ik houd me aan de volgende vier kernprincipes:

* **Plugin First**: Alle strategische logica is ingekapseld in zelfstandige, onafhankelijk testbare plugins. Dit is de kern van het systeem.
* **Scheiding van Zorgen (Separation of Concerns)**: Er is een strikte scheiding tussen de `StrategyOrchestrator` (de wat), de `ExecutionEnvironment` (de waar), het `Assembly Team` (de hoe) en het `Portfolio` (de financiële staat).
* **Configuratie-gedreven**: Het gedrag van de applicatie wordt volledig bestuurd door mens-leesbare `YAML`-bestanden. De code is de motor, de configuratie is de bestuurder.
* **Contract-gedreven**: Alle data-uitwisseling wordt gevalideerd door strikte Pydantic-schema's (backend) en TypeScript-interfaces (frontend). Dit zorgt voor voorspelbaarheid en type-veiligheid.

## 2. Architectuur Overzicht

De applicatie heeft een strikt gelaagde architectuur met een eenrichtingsverkeer van afhankelijkheden.

+-------------------------------------------------------------+
|  Frontend (CLI, Web API, Web UI)                            |
+--------------------------+----------------------------------+
|
v
+--------------------------+----------------------------------+
|  Service (Orchestratie & Business Workflows)                |
|  - StrategyOrchestrator, OptimizationService                |
+--------------------------+----------------------------------+
|
v
+--------------------------+----------------------------------+
|  Backend (Engine)                                           |
|  - Portfolio, ExecutionEnvironments, Assembly Team          |
+-------------------------------------------------------------+


* **Backend (`/backend`)**: De "engine". Bevat alle kernlogica en is ontworpen als een onafhankelijke library.
* **Service (`/services`)**: De "lijm". Orkestreert backend-componenten tot complete business workflows. Hier leeft de `StrategyOrchestrator`.
* **Frontend (`/frontends`)**: De gebruikersinterface (Web UI, API, CLI).

## 3. De 6-Fasen Quant Workflow

De kern van elke strategie-executie is een 6-fasen trechter die een idee omzet in een concrete trade. Ik moet deze flow begrijpen en respecteren bij het schrijven van code.

1.  **Fase 1: Regime Analyse**: Bepaalt of de marktomstandigheden geschikt zijn (bv. trending).
2.  **Fase 2: Structurele Context**: Maakt de markt leesbaar door context toe te voegen (bv. marktstructuur, trends).
3.  **Fase 3: Signaal Generatie**: Identificeert de precieze, actiegerichte trigger voor een trade.
4.  **Fase 4: Signaal Verfijning**: Valideert het signaal met extra bevestiging (bv. volume).
5.  **Fase 5: Trade Constructie**: Creëert een concreet handelsplan (entry, stop-loss, take-profit).
6.  **Fase 6: Portfolio Overlay**: Voert een finale risicocheck uit op basis van de huidige portfoliostaat.

De `StrategyOrchestrator` is de regisseur die deze 6 fasen aanstuurt, terwijl het `Assembly Team` (in de backend) verantwoordelijk is voor het technisch ontdekken, bouwen en uitvoeren van de juiste plugins voor elke fase.

## 4. Anatomie van een Plugin

Plugins zijn de fundamentele bouwstenen. Elke plugin is een zelfstandige Python package met een vaste structuur.

* `plugins/[plugin_naam]/`:
    * `plugin_manifest.yaml`: De "ID-kaart" die de plugin vindbaar maakt. Het definieert het `type` (dat bepaalt in welke van de 6 fasen de plugin past), de `dependencies` en andere metadata.
    * `worker.py`: Bevat de Python-klasse met de daadwerkelijke businesslogica.
    * `schema.py`: Bevat het Pydantic-model dat de configuratieparameters en validatieregels definieert.
    * `state.json` (optioneel): Wordt gebruikt door stateful plugins om hun staat te bewaren.

## 5. Codeerstandaarden & Best Practices

Ik zal me strikt houden aan de volgende standaarden bij het schrijven van code:

1.  **Code Stijl**:
    * Alle Python-code moet **PEP 8 compliant** zijn.
    * **Volledige Type Hinting** is verplicht.
    * Commentaar en docstrings zijn in het **Engels**.
    * Gebruik **Google Style Python Docstrings** voor alle functies en klassen.

2.  **Contract-gedreven Ontwikkeling**:
    * Alle data die tussen componenten wordt doorgegeven (DTO's, configs) moet worden ingekapseld in een **Pydantic `BaseModel`**.

3.  **Logging**:
    * De primaire output voor analyse is een gestructureerd **`run.log.json`**-bestand.
    * Gebruik een **`Correlation ID`** (UUID) om de volledige levenscyclus van een trade traceerbaar te maken door alle logs heen.

4.  **Testen**:
    * Code zonder tests is incompleet. Elke plugin is **verplicht** om een `tests/test_worker.py` te hebben.

5.  **Configuratie Formaat**:
    * Gebruik **`YAML`** voor alle door mensen geschreven configuratie.
    * Gebruik **`JSON`** voor machine-naar-machine data-uitwisseling (bv. API's, state-bestanden).

## 6. Snelle Referentie: Kernterminologie

* **Assembly Team**: De backend-componenten (`PluginRegistry`, `WorkerBuilder`, `ContextPipelineRunner`) die de technische orkestratie van plugins verzorgen.
* **Run**: Een `YAML`-bestand (`run_schema.yaml`) dat een complete strategie-configuratie beschrijft.
* **DTO (Data Transfer Object)**: Een Pydantic-model (`Signal`, `Trade`) dat als strikt contract dient voor data-uitwisseling.
* **ExecutionEnvironment**: De backend-laag die de "wereld" definieert waarin een strategie draait (`Backtest`, `Paper`, `Live`).
* **StrategyOrchestrator**: De "regisseur" in de Service-laag die de 6-fasen trechter uitvoert voor één enkele run.

Door deze principes en structuren te volgen, help ik je om een consistente, robuuste en onderhoudbare codebase te bouwen. Laten we beginnen!

--- END FILE: GEMINI.md ---

--- START FILE: requirements.txt ---
# requirements.txt

# --- Data & Analysis ---
# Kernbibliotheken voor dataverwerking en analyse.
pandas
pandas-stubs
numpy
scipy
pyarrow
pyarrow-stubs
openpyxl

# --- Configuration ---
# Voor het inlezen van YAML-configuratiebestanden.
PyYAML

# --- Web & API Server ---
# Benodigdheden voor de web-interface en de API.
fastapi
uvicorn[standard]
python-multipart
Jinja2

# --- User Interface (CLI) ---
# Tools voor de command-line interface.
tqdm
rich
prompt-toolkit

# --- Code Quality & Contracts ---
# Voor het afdwingen van code-stijl en datavalidatie.
pylint
pydantic

# --- Testing ---
# Framework en tools voor het schrijven en uitvoeren van tests.
pytest
pytest-mock

# --- Visualization ---
# Voor het genereren van grafieken en visuele rapportages.
plotly

# --- Documentation Generation ---
# Tools voor het genereren van de projectdocumentatie.
sphinx
sphinx-rtd-theme
--- END FILE: requirements.txt ---

--- START FILE: run_backtest_cli.py ---
# run_backtest_cli.py
"""
Entrypoint: For automated (headless) runs
"""

--- END FILE: run_backtest_cli.py ---

--- START FILE: run_supervisor.py ---
# run_supervisor.py
"""
Entrypoint: Starts the live trading supervisor
"""

--- END FILE: run_supervisor.py ---

--- START FILE: run_web.py ---
# run_web.py
"""
Entrypoint: Starts the Web UI and API
"""

--- END FILE: run_web.py ---

--- START FILE: __init__.py ---

--- END FILE: __init__.py ---

--- START FILE: .pytest_cache/README.md ---
# pytest cache directory #

This directory contains data from the pytest's cache plugin,
which provides the `--lf` and `--ff` options, as well as the `cache` fixture.

**Do not** commit this to version control.

See [the docs](https://docs.pytest.org/en/stable/how-to/cache.html) for more information.

--- END FILE: .pytest_cache/README.md ---

--- START FILE: backend/__init__.py ---
# backend/__init__.py
"""
Exposes the public API of the Backend package.
"""
__all__ = [
    # from .core
    "StrategyEngine",
    "Portfolio",
    "BaseStrategyWorker",
    "ContextRecorder",
    # from .dtos
    "Signal",
    "EntrySignal",
    "RiskDefinedSignal",
    "TradePlan",
    "RoutedTradePlan",
    "CriticalEvent",
    "ExecutionDirective",
    "EngineCycleResult",
    "ClosedTrade",
    "TradingContext",
    "BacktestResult",
    "DataCoverage",
    "TradeTick",
    "SynchronizationCommand",
    "ExtendHistoryCommand",
    "FillGapsCommand",
    "FetchPeriodCommand",
    "PairsQuery",
    "CoverageQuery",
    "RangeQuery",
    # from .core
    # from .environments
    "BacktestEnvironment",
#    "LiveEnvironment",
#    "PaperEnvironment",
    # from .assembly
    "ContextBuilder",
    "DependencyValidator",
    "PluginRegistry",
    "WorkerBuilder",
]

from .core import (
    StrategyEngine,
    Portfolio,
    BaseStrategyWorker,
    ContextRecorder,
)
from .dtos import (
    Signal,
    EntrySignal,
    RiskDefinedSignal,
    TradePlan,
    RoutedTradePlan,
    CriticalEvent,
    ExecutionDirective,
    EngineCycleResult,
    ClosedTrade,
    TradingContext,
    BacktestResult,
    DataCoverage,
    TradeTick,
    SynchronizationCommand,
    ExtendHistoryCommand,
    FillGapsCommand,
    FetchPeriodCommand,
    PairsQuery,
    CoverageQuery,
    RangeQuery,
)
from .environments import (
    BacktestEnvironment,
#    LiveEnvironment,
#    PaperEnvironment,
)
from .assembly import (
    ContextBuilder,
    DependencyValidator,
    PluginRegistry,
    WorkerBuilder,
)

--- END FILE: backend/__init__.py ---

--- START FILE: backend/assembly/context_builder.py ---
# backend/assembly/context_builder.py
"""
Contains the ContextBuilder, responsible for executing a sequence of
context-providing plugins to enrich a DataFrame.

@layer: Backend (Assembly)
@dependencies: [pandas]
@responsibilities:
    - Sequentially applies a list of instantiated context workers to a DataFrame.
    - Ensures the original DataFrame is not modified (works on a copy).
    - Returns the final, enriched DataFrame.
"""
from typing import List

import pandas as pd

from backend.core.interfaces import ContextWorker

class ContextBuilder:
    """Executes a pipeline of context workers to enrich a DataFrame."""

    def build(self,
              initial_df: pd.DataFrame,
              context_pipeline: List[ContextWorker]
        ) -> pd.DataFrame:
        """
        Applies a list of context workers sequentially to a DataFrame.

        This method takes a starting DataFrame and a list of worker objects
        (which are expected to have a 'process' method). It creates a copy of
        the DataFrame and then passes it through each worker in order, with the
        output of one worker becoming the input for the next.

        Args:
            initial_df (pd.DataFrame): The raw OHLCV DataFrame.
            context_pipeline (List[object]): An ordered list of instantiated
                                             context worker objects.

        Returns:
            pd.DataFrame: The final, enriched DataFrame after all workers
                          have been executed.
        """
        # Werk altijd op een kopie om onverwachte bijeffecten te voorkomen.
        enriched_df = initial_df.copy()

        for worker in context_pipeline:
            # We gaan ervan uit dat elke 'worker' een .process(df) methode heeft.
            # De test die we hebben geschreven, valideert deze aanname.
            enriched_df = worker.process(enriched_df)

        return enriched_df

--- END FILE: backend/assembly/context_builder.py ---

--- START FILE: backend/assembly/dependency_validator.py ---
# backend/assembly/dependency_validator.py
"""
Contains the DependencyValidator, responsible for ensuring the integrity
of a context pipeline before execution.

@layer: Backend (Assembly)
@dependencies: [.plugin_registry]
@responsibilities:
    - Validates that the data dependencies of each plugin in a sequence are met
      by the outputs of the preceding plugins.
"""
from typing import List
from backend.assembly.plugin_registry import PluginRegistry

class DependencyValidator:
    """Validates the dataflow integrity of a context worker pipeline."""

    def __init__(self, plugin_registry: PluginRegistry):
        """Initializes the DependencyValidator.

        Args:
            plugin_registry (PluginRegistry): The registry to fetch manifests from.
        """
        self._registry = plugin_registry

    def validate(self, context_pipeline: List[str]) -> bool:
        """
        Validates a sequential pipeline of context workers.

        It checks if the `dependencies` of each worker are satisfied by the
        initial `DataFrame` columns or the `provides` of the workers that
        run before it.

        Args:
            context_pipeline (List[str]): An ordered list of context worker names.

        Returns:
            True if the pipeline is valid.

        Raises:
            ValueError: If a dependency is not met, with a descriptive error.
        """
        # Start met de basiskolommen die altijd aanwezig zijn in de ruwe DataFrame.
        available_columns = {"open", "high", "low", "close", "volume"}

        for plugin_name in context_pipeline:
            plugin_data = self._registry.get_plugin_data(plugin_name)
            if not plugin_data:
                raise ValueError(f"Plugin '{plugin_name}' not found in registry.")

            manifest, _ = plugin_data

            # <<< CORRECTIE DEEL 1: Itereer over de 'requires' lijst >>>
            # Controleer de dependencies van de huidige plugin.
            if manifest.dependencies and manifest.dependencies.requires:
                for dep in manifest.dependencies.requires:
                    if dep not in available_columns:
                        raise ValueError(
                            f"Dependency '{dep}' for plugin '{plugin_name}' not met. "
                            f"Available columns: {sorted(list(available_columns))}"
                        )

            # <<< CORRECTIE DEEL 2: Update met de 'provides' lijst >>>
            # Voeg de output van deze plugin toe aan de set van beschikbare kolommen.
            if manifest.dependencies and manifest.dependencies.provides:
                available_columns.update(manifest.dependencies.provides)

        return True

--- END FILE: backend/assembly/dependency_validator.py ---

--- START FILE: backend/assembly/engine_builder.py ---
# backend/assembly/engine_builder.py
"""
Contains the EngineBuilder, a specialist for assembling a StrategyEngine.
"""
from typing import Any, Dict, List

from backend.assembly.worker_builder import WorkerBuilder
from backend.config.schemas.run_schema import RunBlueprint, WorkerDefinition
from backend.core.enums import PipelinePhase
from backend.core.strategy_engine import StrategyEngine
from backend.core.interfaces.worker import ContextWorker

class EngineBuilder:
    """Assembles a StrategyEngine with all its required workers."""

    def __init__(self, worker_builder: WorkerBuilder):
        self._worker_builder = worker_builder

    def build_context_pipeline(
        self, run_conf: RunBlueprint
    ) -> List[ContextWorker]:
        """Builds the initial pipeline of context workers."""
        context_plugin_names: List[str] = run_conf.taskboard.root.get(
            PipelinePhase.STRUCTURAL_CONTEXT, []
        )

        built_workers = [
            self._worker_builder.build(
                name=name,
                user_params=run_conf.workforce.get(name, WorkerDefinition()).params
            ) for name in context_plugin_names if name
        ]
        # Filter out any workers that failed to build
        return [worker for worker in built_workers if worker is not None]

    def build_engine(self, run_conf: RunBlueprint) -> StrategyEngine:
        """Builds and returns a configured StrategyEngine."""
        active_workers: Dict[str, Any] = {}
        for phase, plugin_names in run_conf.taskboard.root.items():
            if phase != PipelinePhase.STRUCTURAL_CONTEXT:
                worker_list = [
                    self._worker_builder.build(
                        name=name,
                        user_params=run_conf.workforce.get(name, WorkerDefinition()).params
                    ) for name in plugin_names if name
                ]
                active_workers[phase.value] = [w for w in worker_list if w is not None]

        return StrategyEngine(active_workers=active_workers)

--- END FILE: backend/assembly/engine_builder.py ---

--- START FILE: backend/assembly/plugin_creator.py ---
# backend/assembly/plugin_creator.py
"""
Contains the PluginCreator, a service for generating plugin boilerplate code.

@layer: Backend (Assembly)
@dependencies: [pathlib, backend.utils.app_logger]
@responsibilities:
    - Creates the directory structure for a new plugin.
    - Generates all required files from templates (manifest, worker, etc.).
    - Provides a simple interface for creating new plugins.
"""

from pathlib import Path
import shutil
from backend.utils.app_logger import LogEnricher

class PluginCreator:
    """
    A service class responsible for creating the boilerplate structure for a new plugin.
    It uses template files to generate the necessary Python and YAML files.
    """

    def __init__(self, plugins_root: Path, logger: LogEnricher):
        """Initializes the PluginCreator."""
        self._logger = logger
        self.plugins_root = plugins_root
        # Het pad naar de templates is relatief aan DIT bestand
        self.template_root = Path(__file__).parent / "templates"

        if not self.plugins_root.is_dir():
            self._logger.error(f"Plugins root directory does not exist: {self.plugins_root}")
            raise FileNotFoundError(f"Plugins root directory does not exist: {self.plugins_root}")

        if not self.template_root.is_dir():
            self._logger.error(f"Template directory not found at: {self.template_root}")
            raise FileNotFoundError(f"Template directory not found at: {self.template_root}")

    def create(self, name: str, plugin_type: str) -> bool:
        """Creates a new plugin skeleton from templates."""
        plugin_path = self.plugins_root / plugin_type / name
        tests_path = plugin_path / "tests"

        try:
            self._logger.info(f"Creating plugin '{name}' at: {plugin_path}")
            tests_path.mkdir(parents=True, exist_ok=True)

            # Gecorrigeerde paden voor de template bestanden
            template_files = {
                "manifest.yaml.tpl": plugin_path / "manifest.yaml",
                "schema.py.tpl": plugin_path / "schema.py",
                "worker.py.tpl": plugin_path / "worker.py",
                "context_schema.py.tpl": plugin_path / "context_schema.py",
                "test/test_worker.py.tpl": tests_path / "test_worker.py"
            }

            for template_name, target_path in template_files.items():
                source_path = self.template_root / template_name
                
                if not source_path.is_file():
                    self._logger.error(f"Template file not found: {source_path}")
                    raise FileNotFoundError(f"Template file not found: {source_path}")

                shutil.copy(source_path, target_path)

            self._logger.info(f"Successfully created plugin '{name}'.")
            return True

        except Exception as e:
            self._logger.error(f"Failed to create plugin '{name}': {e}", exc_info=True)
            if plugin_path.exists():
                shutil.rmtree(plugin_path) # Cleanup partial creation
            return False

--- END FILE: backend/assembly/plugin_creator.py ---

--- START FILE: backend/assembly/plugin_registry.py ---
# backend/assembly/plugin_registry.py
"""
Contains the PluginRegistry, responsible for discovering, validating, and indexing
all available plugins within the ecosystem.

@layer: Backend (Assembly)
@dependencies: [Pydantic, PyYAML, backend.config.schemas]
@responsibilities:
    - Scans plugin directories for manifests.
    - Validates manifest schemas against the PluginManifest contract.
    - Builds and maintains the central in-memory plugin registry.
"""

from pathlib import Path
from typing import Dict, Optional, Tuple

import yaml

from pydantic import ValidationError
from backend.config.schemas.platform_schema import PlatformConfig
from backend.config.schemas.plugin_manifest_schema import PluginManifest
from backend.utils.app_logger import LogEnricher

class PluginRegistry:
    """
    Discovers all valid plugins and holds their manifest data in an
    in-memory dictionary for fast retrieval by other components.
    """

    def __init__(self, platform_config: PlatformConfig, logger: LogEnricher):
        """
        Initializes the registry by scanning and validating all plugins.

        Args:
            platform_config (PlatformConfig): The validated platform configuration object.
            logger (LogEnricher): The logger instance.
        """
        self._logger = logger
        self._plugins_root_path = Path(platform_config.plugins_root_path)
        self._registry: Dict[str, Tuple[PluginManifest, Path]] = {}

        self._scan_and_register_plugins()

    def _scan_and_register_plugins(self):
        """
        Scans the plugin directory, validates each manifest, and populates the registry.
        """
        self._logger.info(f"Scanning for plugins in '{self._plugins_root_path}'...")

        if not self._plugins_root_path.is_dir():
            self._logger.error(f"Plugin root path '{self._plugins_root_path}' not found.")
            return

        for manifest_path in self._plugins_root_path.rglob("plugin_manifest.yaml"):
            try:
                with open(manifest_path, 'r', encoding='utf-8') as f:
                    manifest_data = yaml.safe_load(f)

                # Valideer de data tegen ons Pydantic-contract
                manifest = PluginManifest(**manifest_data)

                # Controleer op dubbele namen
                plugin_name = manifest.identification.name

                # Controleer op dubbele namen
                if plugin_name in self._registry:
                    self._logger.warning(
                        f"Duplicate plugin name '{plugin_name}' found at '{manifest_path}'. "
                        "Skipping."
                    )
                    continue

                # Voeg de gevalideerde manifest toe aan de registry
                plugin_directory = manifest_path.parent
                self._registry[plugin_name] = (manifest, plugin_directory)

            except yaml.YAMLError as e:
                self._logger.warning(f"Could not parse manifest at '{manifest_path}': {e}")
            except ValidationError as e:
                self._logger.warning(f"Invalid manifest at '{manifest_path}':\n{e}")

        self._logger.info(
            f"Scan complete. Found and registered {len(self._registry)} valid plugins."
        )

    def get_plugin_data(self, plugin_name: str) -> Optional[Tuple[PluginManifest, Path]]:
        """
        Retrieves the validated manifest for a single plugin by its unique name.

        Args:
            plugin_name (str): The unique name of the plugin.

        Returns:
            Optional[PluginManifest]: The Pydantic model of the manifest, or None if not found.
        """
        return self._registry.get(plugin_name)

    def get_all_manifests(self) -> Dict[str, PluginManifest]:
        """
        Returns the entire registry of validated plugin manifests.

        Returns:
            Dict[str, PluginManifest]: A dictionary of all registered plugins.
        """
        return {name: data[0] for name, data in self._registry.items()}

--- END FILE: backend/assembly/plugin_registry.py ---

--- START FILE: backend/assembly/worker_builder.py ---
# backend/assembly/worker_builder.py
"""
Contains the WorkerBuilder, responsible for instantiating a single plugin worker
based on its manifest and user-provided configuration.

@layer: Backend (Assembly)
@dependencies:
    - .plugin_registry: To get the manifest (the "blueprint") for a worker.
    - backend.utils.dynamic_loader: To dynamically import plugin code.
    - backend.utils.app_logger: To create and inject a specific logger for the worker.
@responsibilities:
    - Dynamically loads a worker's code and its Pydantic schema.
    - Validates user-provided parameters against the plugin's schema.
    - Injects dependencies (like a logger) into the worker instance.
    - Returns a fully instantiated and validated worker object.
"""
from typing import Any, Dict, Optional, cast

from pydantic import ValidationError

from backend.assembly.plugin_registry import PluginRegistry
from backend.utils.dynamic_loader import load_class_from_module
from backend.utils.app_logger import LogEnricher


class WorkerBuilder:
    """Constructs a single, validated plugin worker instance."""

    def __init__(self, plugin_registry: PluginRegistry, logger: LogEnricher):
        """Initializes the WorkerBuilder.

        Args:
            plugin_registry (PluginRegistry): The registry containing all discovered plugins.
            logger (LogEnricher): The main logger, used to create child loggers.
        """
        self._registry = plugin_registry
        self._logger = logger

    def build(self, name: str, user_params: Dict[str, Any]) -> Optional[Any]:
        """Builds, validates, and instantiates a single worker.

        This method orchestrates the entire lifecycle of creating a worker, from
        finding its definition to validating user input and injecting dependencies.

        Args:
            name (str): The unique name of the worker to build.
            user_params (Dict[str, Any]): The parameter dictionary from the
                                           run_blueprint's 'workforce' section.

        Returns:
            An instantiated and validated worker object if successful, otherwise None.
        """
        # 1. Vraag Manifest en Pad op
        plugin_data = self._registry.get_plugin_data(name)
        if not plugin_data:
            self._logger.error(f"Cannot build worker: plugin '{name}' not found in registry.")
            return None

        manifest, plugin_path = plugin_data

        try:
            # Converteer het file path (bv. "plugins\\signal_generators\\fvg")
            # naar een Python module path (bv. "plugins.signal_generators.fvg")
            plugin_module_path = ".".join(plugin_path.parts)

            # 2. Dynamisch Laden met expliciete paden
            schema_module_path = f"{plugin_module_path}.{manifest.schema_path.replace('.py', '')}"
            worker_module_path = f"{plugin_module_path}.worker"

            schema_class = load_class_from_module(schema_module_path, manifest.params_class)
            worker_class = load_class_from_module(worker_module_path, manifest.entry_class)

            # 3. Valideer Parameters
            validated_params = schema_class(**user_params)

            # 4. Creëer & Injecteer Logger
            # Haal de onderliggende standaard logger op om een child te maken.
            indent_val = self._logger.extra.get('indent', 0) if self._logger.extra else 0
            current_indent = cast(int, indent_val)
            child_logger = self._logger.logger.getChild(name)
            worker_logger = LogEnricher(
                child_logger,
                indent=current_indent + 1
            )

            # 5. Instantieer de Worker
            worker_instance = worker_class(
                name=name,
                params=validated_params,
                logger=worker_logger
            )

            self._logger.info(f"Successfully built worker '{name}'.")
            return worker_instance

        except (ImportError, AttributeError) as e:
            self._logger.error(
                f"Failed to load code for worker '{name}': {e}"
            )
        except ValidationError as e:
            self._logger.error(
                f"Invalid parameters for worker '{name}':\n{e}"
            )

        return None

--- END FILE: backend/assembly/worker_builder.py ---

--- START FILE: backend/assembly/__init__.py ---
# backend/assembly/__init__.py
"""
Exposes the public API of the Assembly sub-package.
"""
__all__ = [
    "ContextBuilder",
    "DependencyValidator",
    "PluginRegistry",
    "WorkerBuilder",
]

from .context_builder import ContextBuilder
from .dependency_validator import DependencyValidator
from .plugin_registry import PluginRegistry
from .worker_builder import WorkerBuilder

--- END FILE: backend/assembly/__init__.py ---

--- START FILE: backend/assembly/templates/__init__.py ---

--- END FILE: backend/assembly/templates/__init__.py ---

--- START FILE: backend/assembly/templates/test/__init__.py ---

--- END FILE: backend/assembly/templates/test/__init__.py ---

--- START FILE: backend/config/__init__.py ---

--- END FILE: backend/config/__init__.py ---

--- START FILE: backend/config/schemas/app_schema.py ---
# backend/config/schemas/app_schema.py
"""
Contains the final, composed Pydantic model for a complete application run.

@layer: Backend (Config)
@dependencies: [Pydantic, .platform_schema, .run_schema]
@responsibilities:
    - Composes platform-level and run-level configurations into a single,
      unified, and immutable AppConfig object.
"""
from pydantic import BaseModel, Field
from .platform_schema import PlatformConfig
from .run_schema import RunBlueprint

class AppConfig(BaseModel):
    """
    The final, composed configuration object for a run. It explicitly
    combines platform-wide settings (PlatformConfig) with the blueprint for a
    specific run (RunBlueprint), creating a single source of truth.
    """
    platform: PlatformConfig = Field(
        ...,
        description="app_config.platform.desc"
    )
    run: RunBlueprint = Field(
        ...,
        description="app_config.run.desc"
    )

--- END FILE: backend/config/schemas/app_schema.py ---

--- START FILE: backend/config/schemas/connectors_schema.py ---
# In bestand: backend/config/schemas/connectors_schema.py
"""
Contains Pydantic models that define the structure of the connectors.yaml file.

This schema is the single source of truth for defining and validating all
API connector configurations used by the platform.

@layer: Backend (Config/Schemas)
@dependencies: [pydantic, .connectors.kraken_schema]
"""
from typing import Dict, Union, Annotated
from pydantic import BaseModel, Field, RootModel

from backend.config.schemas.connectors.kraken_schema import (
    KrakenPublicConfig,
    KrakenPrivateConfig,
)

# Use a Tagged Union to allow Pydantic to automatically select the correct
# model based on the 'type' field in the YAML.
AnyConnectorConfig = Annotated[
    Union[KrakenPublicConfig, KrakenPrivateConfig],
    Field(discriminator="type"),
]

class ConnectorDefinition(BaseModel):
    """
    Defines a single, named connector instance. The 'type' field determines
    which specific configuration schema will be used for validation.
    """
    type: str = Field(
        ...,
        description="connectors.definition.type.desc"
    )
    config: AnyConnectorConfig = Field(
        ...,
        description="connectors.definition.config.desc"
    )


class ConnectorsConfig(RootModel[Dict[str, ConnectorDefinition]]):
    """
    The root model for the entire connectors.yaml file.
    It represents a dictionary where each key is a unique, user-defined name
    for a connector instance, and the value is its validated definition.
    """
    root: Dict[str, ConnectorDefinition] = Field(
        ...,
        description="connectors.root.desc"
    )

--- END FILE: backend/config/schemas/connectors_schema.py ---

--- START FILE: backend/config/schemas/platform_schema.py ---
# In bestand: backend/config/schemas/platform_schema.py
"""
Contains Pydantic models that define the structure of the platform.yaml file.

This schema defines the global, platform-wide settings and acts as a single
source of truth for the application's core configuration, reflecting the
main architectural layers.

@layer: Backend (Config/Schemas)
@dependencies: [pydantic, backend.core.enums]
"""
from typing import Dict, List, Literal
from pydantic import BaseModel, Field
from backend.core.enums import LogLevel

# --- Sub-models for Logical Grouping ---

class LoggingConfig(BaseModel):
    """Defines the structure for the 'logging' section."""
    profile: Literal['developer', 'analysis'] = Field(
        default='analysis',
        description="platform_config.core.logging.profile.desc"
    )
    profiles: Dict[str, List[LogLevel]] = Field(
        default_factory=dict,
        description="platform_config.core.logging.profiles.desc"
    )

class CoreConfig(BaseModel):
    """Defines core platform settings."""
    language: Literal['en', 'nl'] = Field(
        default='en',
        description="platform_config.core.language.desc"
    )
    plugins_root_path: str = Field(
        default="plugins",
        description="platform_config.core.plugins_root_path.desc"
    )
    logging: LoggingConfig = Field(default_factory=LoggingConfig)

class DataCollectionLimits(BaseModel):
    """Defines safety limits for historical data collection."""
    max_history_days: int = Field(
        default=365 * 5,
        gt=0,
        description="platform_config.services.data_collection.max_history_days.desc"
    )
    warn_history_days: int = Field(
        default=365,
        gt=0,
        description="platform_config.services.data_collection.warn_history_days.desc"
    )

class DataCollectionConfig(BaseModel):
    """Groups all settings related to the DataCollectionService."""
    limits: DataCollectionLimits = Field(default_factory=DataCollectionLimits)

class ServicesConfig(BaseModel):
    """Groups all service-layer configurations."""
    data_collection: DataCollectionConfig = Field(default_factory=DataCollectionConfig)

class DataConfig(BaseModel):
    """Defines settings related to data sources and storage."""
    source_dir: str = Field(
        default="source_data",
        description="platform_config.data.source_dir.desc"
    )

class PortfolioDefaults(BaseModel):
    """Defines the default financial parameters for portfolios."""
    initial_capital: float = Field(
        default=10000.0,
        gt=0,
        description="platform_config.portfolio.defaults.initial_capital.desc"
    )
    fees_pct: float = Field(
        default=0.001,
        ge=0,
        description="platform_config.portfolio.defaults.fees_pct.desc"
    )

class PortfolioConfig(BaseModel):
    """Groups all portfolio-related configurations."""
    defaults: PortfolioDefaults = Field(default_factory=PortfolioDefaults)

# --- Main Platform Configuration Model ---

class PlatformConfig(BaseModel):
    """
    The main Pydantic model that validates the entire platform.yaml file.
    It composes all other configuration models into a logical, hierarchical structure.
    """
    core: CoreConfig = Field(default_factory=CoreConfig)
    services: ServicesConfig = Field(default_factory=ServicesConfig)
    data: DataConfig = Field(default_factory=DataConfig)
    portfolio: PortfolioConfig = Field(default_factory=PortfolioConfig)

--- END FILE: backend/config/schemas/platform_schema.py ---

--- START FILE: backend/config/schemas/plugin_manifest_schema.py ---
# backend/config/schemas/plugin_manifest_schema.py
"""
Pydantic schema for validating the plugin_manifest.yaml file.

This schema acts as the single source of truth for the structure and data
types required for a plugin to be considered valid by the platform.

@layer: Backend (Config)
@dependencies: [Pydantic, backend.core.enums]
@responsibilities:
    - Defines the data contract for a plugin's manifest.
    - Enables automatic validation of manifests during plugin registration.
"""
from typing import Annotated, List, Literal

from pydantic import BaseModel, Field, StringConstraints

from backend.core.enums import PipelinePhase

# --- Nested Models for Grouping and Clarity ---

class CoreIdentity(BaseModel):
    """System-level fields for versioning and schema identification."""
    apiVersion: Literal["s1mpletrader.io/v1"] = Field(
        description="manifest.core_identity.apiVersion.desc"
    )
    kind: Literal["PluginManifest"] = Field(
        description="manifest.core_identity.kind.desc"
    )

class PluginIdentification(BaseModel):
    """Descriptive metadata for identifying the plugin."""
    name: Annotated[
        str,
        StringConstraints(strip_whitespace=True, to_lower=True, pattern=r"^[a-z0-9_]+$")
    ] = Field(description="manifest.identification.name.desc")

    display_name: str = Field(
        description="manifest.identification.display_name.desc"
    )
    type: PipelinePhase = Field(
        description="manifest.identification.type.desc"
    )
    version: Annotated[
        str,
        StringConstraints(pattern=r"^\d+\.\d+\.\d+$")
    ] = Field(description="manifest.identification.version.desc")

    description: str = Field(
        description="manifest.identification.description.desc"
    )
    author: str = Field(
        description="manifest.identification.author.desc"
    )

class Dependencies(BaseModel):
    """Defines the data contract for the plugin's interaction with context."""
    requires: List[str] = Field(
        description="manifest.dependencies.requires.desc",
        default_factory=list
    )
    provides: List[str] = Field(
        description="manifest.dependencies.provides.desc",
        default_factory=list
    )

class Permissions(BaseModel):
    """Defines the security permissions required by the plugin."""
    network_access: List[str] = Field(
        description="manifest.permissions.network_access.desc",
        default_factory=list
    )
    filesystem_access: List[str] = Field(
        description="manifest.permissions.filesystem_access.desc",
        default_factory=list
    )

# --- The Main Manifest Model ---

class PluginManifest(BaseModel):
    """The complete, validated Pydantic model for a plugin's manifest.yaml."""
    core_identity: CoreIdentity = Field(..., description="manifest.core_identity.desc")
    identification: PluginIdentification = Field(..., description="manifest.identification.desc")
    dependencies: Dependencies = Field(..., description="manifest.dependencies.desc")
    permissions: Permissions = Field(..., description="manifest.permissions.desc")

--- END FILE: backend/config/schemas/plugin_manifest_schema.py ---

--- START FILE: backend/config/schemas/run_schema.py ---
# backend/config/schemas/run_schema.py
"""
Contains Pydantic models that define the structure of a run_schema.yaml file.
This schema defines how a user composes a strategy from available plugins.

@layer: Backend (Config)
@dependencies: [Pydantic]
@responsibilities:
    - Defines the schema for a single strategy configuration.
    - Validates the assignment of plugins to taskboard phases.
    - Validates the parameter definitions for the used plugins.
"""

from typing import List, Dict, Any
from pydantic import BaseModel, Field, RootModel
from backend.core.enums import PipelinePhase

class RunDataConfig(BaseModel):
    """Defines the data settings specific to this run."""
    trading_pair: str = Field(..., description="run_blueprint.data.trading_pair.desc")
    timeframe: str = Field(..., description="run_blueprint.data.timeframe.desc")

class TaskboardConfig(RootModel[Dict[PipelinePhase, List[str]]]):
    """
    Defines which plugins are assigned to each phase.
    This is a flexible dictionary where keys must be valid PipelinePhase members.
    By inheriting from RootModel, this class instance acts directly as a dictionary.
    """
    root: Dict[PipelinePhase, List[str]] = Field(
        ...,
        description="run_blueprint.taskboard.desc"
    )

class WorkerDefinition(BaseModel):
    """
    Defines the user-provided parameters for a single plugin.
    The system will validate this 'params' dict against the plugin's own schema.py.
    """
    params: Dict[str, Any] = Field(
        default_factory=dict,
        description="run_blueprint.workforce.worker.params.desc"
    )

class RunBlueprint(BaseModel):
    """
    The main Pydantic model that validates a complete run_blueprint.yaml file.
    """
    data: RunDataConfig = Field(..., description="run_blueprint.data.desc")
    taskboard: TaskboardConfig = Field(..., description="run_blueprint.taskboard.desc")
    workforce: Dict[str, WorkerDefinition] = Field(
        default_factory=dict,
        description="run_blueprint.workforce.desc"
    )

--- END FILE: backend/config/schemas/run_schema.py ---

--- START FILE: backend/config/schemas/__init__.py ---

--- END FILE: backend/config/schemas/__init__.py ---

--- START FILE: backend/config/schemas/connectors/kraken_schema.py ---
# In bestand: backend/config/schemas/connectors/kraken_schema.py
"""
Contains Pydantic models that define the configuration structure for the
KrakenAPIConnector.

@layer: Backend (Config/Schemas)
@dependencies: [pydantic]
"""
from pydantic import BaseModel, Field, SecretStr

class KrakenAPIRetryConfig(BaseModel):
    """Defines the retry strategy for API requests."""
    max_attempts: int = Field(default=3, gt=0, description="kraken.retries.max_attempts.desc")
    delay_seconds: int = Field(default=2, gt=0, description="kraken.retries.delay_seconds.desc")

class KrakenPublicConfig(BaseModel):
    """
    Validation schema for the PUBLIC Kraken API Connector.
    It contains settings for accessing public, unauthenticated endpoints.
    """
    base_url: str = Field(
        default="https://api.kraken.com/0/public",
        description="kraken.public.base_url.desc"
    )
    retries: KrakenAPIRetryConfig = Field(
        default_factory=KrakenAPIRetryConfig,
        description="kraken.retries.desc"
    )

class KrakenPrivateConfig(KrakenPublicConfig):
    """
    Validation schema for the PRIVATE Kraken API Connector.
    Inherits from the public config and adds fields for API credentials.
    These credentials should be loaded from environment variables.
    """
    api_key: SecretStr = Field(..., description="kraken.private.api_key.desc")
    api_secret: SecretStr = Field(..., description="kraken.private.api_secret.desc")

    # Override de base_url voor private endpoints indien nodig
    base_url: str = Field(
        default="https://api.kraken.com/0/private",
        description="kraken.private.base_url.desc"
    )

--- END FILE: backend/config/schemas/connectors/kraken_schema.py ---

--- START FILE: backend/config/schemas/connectors/__init__.py ---

--- END FILE: backend/config/schemas/connectors/__init__.py ---

--- START FILE: backend/core/base_worker.py ---
# backend/core/base_worker.py
"""
Contains optional, concrete base classes for Strategy Workers to simplify
plugin development by automating DTO nesting and providing direct access
to key identifiers like the correlation_id.

@layer: Backend (Core)
@dependencies: [abc, typing, uuid]
@responsibilities:
    - Provide a generic BaseStrategyWorker that handles DTO nesting.
    - Provide specific, inheritable base classes for each worker category
      to minimize boilerplate code in plugins.
    - Automate the propagation of the correlation_id.
"""
from __future__ import annotations

from abc import ABC, abstractmethod
from typing import TYPE_CHECKING, Any, List, Dict, Generic, Optional, TypeVar
from uuid import UUID

if TYPE_CHECKING:
    from backend.dtos.pipeline.entry_signal import EntrySignal
    from backend.dtos.pipeline.risk_defined_signal import RiskDefinedSignal
    from backend.dtos.pipeline.routed_trade_plan import RoutedTradePlan
    from backend.dtos.pipeline.signal import Signal
    from backend.dtos.pipeline.trade_plan import TradePlan
    from backend.dtos.state.trading_context import TradingContext
    from backend.dtos.execution.critical_event import CriticalEvent


# --- Generieke Type Variabelen (volgens PEP 484 conventie) ---
InputDTO_T = TypeVar("InputDTO_T")  # pylint: disable=invalid-name
OutputDTO_T = TypeVar("OutputDTO_T")  # pylint: disable=invalid-name


class BaseStrategyWorker(ABC, Generic[InputDTO_T, OutputDTO_T]):
    """
    A generic base class that automates DTO creation and `correlation_id` handling.

    This class should typically not be inherited from directly. Use the
    specific, category-based classes below (e.g., BaseEntryPlanner) instead,
    as they pre-configure the DTO types and field names, resulting in
    minimal boilerplate for the plugin developer.
    """

    def __init__(self, params: Any):
        self.params = params

    def proces(
        self, input_dto: InputDTO_T, context: "TradingContext"
    ) -> Optional[OutputDTO_T]:
        """
        Public method called by the StrategyEngine. It extracts the correlation_id,
        calls the plugin's specific logic, and wraps the result in the
        correct output DTO.
        """
        # Haal de gepromote correlation_id direct van de input DTO
        correlation_id = getattr(input_dto, "correlation_id", None)
        if not isinstance(correlation_id, UUID):
            return None  # Veiligheid: stop als er geen ID is in de keten

        # Roep de kernlogica van de plugin aan
        new_data = self._process_internal(input_dto, correlation_id, context)

        if new_data is None:
            return None

        output_dto_class = self._get_output_dto_class()
        source_field_name = self._get_source_field_name()

        # Bouw de argumenten voor de constructor van de nieuwe DTO
        constructor_args: Dict[str, Any] = {
            "correlation_id": correlation_id,
            source_field_name: input_dto,
            **new_data,
        }

        return output_dto_class(**constructor_args)

    @abstractmethod
    def _process_internal(
        self,
        input_dto: InputDTO_T,
        correlation_id: UUID,
        context: "TradingContext",
    ) -> Optional[Dict[str, Any]]:
        """
        Plugin-specific logic must be implemented here by the developer.

        Args:
            input_dto: The DTO from the previous pipeline stage.
            correlation_id: The unique ID of the signal chain, provided for convenience.
            context: The full trading context.

        Returns:
            A dictionary with the new fields for the output DTO,
            or None if no output should be generated.
        """
        raise NotImplementedError

    @abstractmethod
    def _get_output_dto_class(self) -> type[OutputDTO_T]:
        """Must specify the output DTO type."""
        raise NotImplementedError

    @abstractmethod
    def _get_source_field_name(self) -> str:
        """Must specify the field name for the nested source DTO."""
        raise NotImplementedError


# --- Categorie-Specifieke Basisklassen ---

class BaseSignalGenerator(ABC):
    """Base class for SignalGenerator plugins (Fase 3)."""
    def __init__(self, params: Any):
        self.params = params

    @abstractmethod
    def process(self, context: "TradingContext") -> List["Signal"]:
        """
        Generates a list of raw Signal DTOs based on the market context.

        Args:
            context: The full trading context, including the enriched DataFrame.

        Returns:
            A list of Signal DTOs, or an empty list if no opportunities are found.
        """
        raise NotImplementedError

class BaseSignalRefiner(BaseStrategyWorker["Signal", "Signal"]):
    """Base class for SignalRefiner plugins (Fase 4)."""

    def execute(
        self, input_dto: "Signal", context: "TradingContext"
    ) -> Optional["Signal"]:
        """
        Overrides the base execute method for the specific case of a refiner,
        which acts as a filter (1-to-1 or 1-to-0).
        """
        is_valid = self._process(input_dto, input_dto.correlation_id, context)
        return input_dto if is_valid else None

    @abstractmethod
    def _process(  # type: ignore
        self,
        input_dto: "Signal",
        correlation_id: UUID,
        context: "TradingContext",
    ) -> bool:
        """Return True to keep the signal, False to discard it."""
        raise NotImplementedError


class BaseEntryPlanner(BaseStrategyWorker["Signal", "EntrySignal"]):
    """Base class for EntryPlanner plugins (Fase 5)."""

    def _get_output_dto_class(self) -> type["EntrySignal"]:
        # pylint: disable=import-outside-toplevel
        from backend.dtos import EntrySignal

        return EntrySignal

    def _get_source_field_name(self) -> str:
        return "signal"


class BaseExitPlanner(BaseStrategyWorker["EntrySignal", "RiskDefinedSignal"]):
    """Base class for ExitPlanner plugins (Fase 6)."""

    def _get_output_dto_class(self) -> type["RiskDefinedSignal"]:
        # pylint: disable=import-outside-toplevel
        from backend.dtos import RiskDefinedSignal

        return RiskDefinedSignal

    def _get_source_field_name(self) -> str:
        return "entry_signal"


class BaseSizePlanner(BaseStrategyWorker["RiskDefinedSignal", "TradePlan"]):
    """Base class for SizePlanner plugins (Fase 7)."""

    def _get_output_dto_class(self) -> type["TradePlan"]:
        # pylint: disable=import-outside-toplevel
        from backend.dtos import TradePlan

        return TradePlan

    def _get_source_field_name(self) -> str:
        return "risk_defined_signal"


class BaseOrderRouter(BaseStrategyWorker["TradePlan", "RoutedTradePlan"]):
    """Base class for OrderRouter plugins (Fase 8)."""

    def _get_output_dto_class(self) -> type["RoutedTradePlan"]:
        # pylint: disable=import-outside-toplevel
        from backend.dtos.pipeline.routed_trade_plan import RoutedTradePlan

        return RoutedTradePlan

    def _get_source_field_name(self) -> str:
        return "trade_plan"

class BaseCriticalEventDetector(ABC):
    """Base class for CriticalEventDetector plugins (Fase 9)."""
    def __init__(self, params: Any):
        self.params = params

    @abstractmethod
    def process(
        self, routed_trade_plans: List["RoutedTradePlan"], context: "TradingContext"
    ) -> List["CriticalEvent"]:
        """
        Detects and returns a list of critical events based on the final context.

        Args:
            routed_trade_plans: The list of proposed trades for the current cycle.
            context: The full trading context.

        Returns:
            A list of CriticalEvent DTOs, or an empty list if no events are detected.
        """
        raise NotImplementedError

--- END FILE: backend/core/base_worker.py ---

--- START FILE: backend/core/constants.py ---
# backend/core/constants.py
"""
Application-wide constants
"""

--- END FILE: backend/core/constants.py ---

--- START FILE: backend/core/context_recorder.py ---
# backend/core/context_recorder.py
"""
Contains the ContextRecorder, a class that acts as a central in-memory database
for storing contextual data produced by various strategy components during a run.

@layer: Backend (Core)
@dependencies: [pydantic, pandas, uuid]
@responsibilities:
    - Provides a single, unified interface for plugins to log contextual data.
    - Stores data in a structured way, indexed by timestamp and plugin name.
    - Serializes Pydantic models into JSON-compatible dictionaries for storage.
"""
import uuid
from typing import Any, Dict
import pandas as pd
from pydantic import BaseModel

class ContextRecorder:
    """A central, in-memory database for recording contextual data from specialists."""

    def __init__(self):
        """Initializes the ContextRecorder with an empty data log."""
        self._data_log: Dict[pd.Timestamp, Dict[str, Any]] = {}

    def add_data(
        self,
        correlation_id: uuid.UUID,
        timestamp: pd.Timestamp,
        specialist_name: str,
        context_object: BaseModel
    ):
        """
        Records a Pydantic context object from a specialist at a specific timestamp.

        The object is immediately serialized to a JSON-compatible dictionary to ensure
        immutability and prevent downstream side effects.

        Args:
            correlation_id (uuid.UUID): The unique ID of the trade lifecycle.
            timestamp (pd.Timestamp): The timestamp of the event to log.
            specialist_name (str): The name of the component logging the data.
            context_object (BaseModel): The Pydantic model with the context data.
        """
        if timestamp not in self._data_log:
            self._data_log[timestamp] = {}

        # Gebruik model_dump() om direct een dictionary te krijgen
        serializable_context = context_object.model_dump()

        # We voegen de correlation_id toe aan de gelogde data voor traceability
        serializable_context['correlation_id'] = str(correlation_id)

        self._data_log[timestamp][specialist_name] = serializable_context

    def get_all_data(self) -> Dict[pd.Timestamp, Dict[str, Any]]:
        """
        Returns the complete, raw data log.

        Returns:
            The nested dictionary containing all recorded context data.
        """
        return self._data_log

--- END FILE: backend/core/context_recorder.py ---

--- START FILE: backend/core/directive_flattener.py ---
# backend/core/directive_flattener.py
"""
Contains a utility class to flatten a deeply nested RoutedTradePlan DTO
into a simple, flat ExecutionDirective.

@layer: Backend (Core)
@dependencies: [backend.dtos, pydantic]
@responsibilities:
    - Decouples the StrategyEngine's complex data structure from the simple
      contract required by the ExecutionHandler.
    - Dynamically unnests DTOs to create a flat data structure.
"""
from typing import Any, Dict, cast
from backend.dtos.pipeline.routed_trade_plan import RoutedTradePlan
from backend.dtos.execution.execution_directive import ExecutionDirective

class DirectiveFlattener:
    """
    A utility responsible for dynamically flattening the nested trade plan
    structure into a final, flat execution directive.
    """

    def _flatten_recursively(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Recursively unnests Pydantic models within a dictionary.
        """
        flat_dict: Dict[str, Any] = {}
        for key, value in data.items():
            # --- DE FIX: Controleer op 'dict' in plaats van 'BaseModel' ---
            if isinstance(value, dict):
                # We willen de geneste container-keys (zoals 'signal', 'trade_plan') niet meenemen.
                # We pakken alleen de inhoud ervan uit.
                flat_dict.update(self._flatten_recursively(cast(Dict[str, Any], value)))
            else:
                flat_dict[key] = value
        return flat_dict

    def flatten(self, routed_trade_plan: RoutedTradePlan) -> ExecutionDirective:
        """
        Transforms a deeply nested RoutedTradePlan into a flat ExecutionDirective
        using a dynamic, recursive approach.

        Args:
            routed_trade_plan (RoutedTradePlan): The complete, nested output
                                                 from the OrderRouter (Fase 8).

        Returns:
            ExecutionDirective: A flat DTO containing all necessary data for execution.
        """
        # 1. Converteer het toplevel DTO naar een dictionary.
        nested_dict = routed_trade_plan.model_dump()

        # 2. Roep de recursieve functie aan om de dictionary plat te slaan.
        flat_data = self._flatten_recursively(nested_dict)

        # 3. Rename 'timestamp' from Signal to 'entry_time' for the directive
        if 'timestamp' in flat_data:
            flat_data['entry_time'] = flat_data.pop('timestamp')

        # 4. Creëer de uiteindelijke, platte DTO. Pydantic negeert
        #    automatisch alle overbodige velden (zoals de geneste objecten zelf).
        return ExecutionDirective(**flat_data)

--- END FILE: backend/core/directive_flattener.py ---

--- START FILE: backend/core/enums.py ---
# backend/core/enums.py
"""
Contains application-wide enumerations to provide type-safety and a single
source of truth for specific sets of values.

@layer: Core
"""
from enum import Enum

class LogLevel(str, Enum):
    """Defines all valid logging levels, including custom ones."""
    DEBUG = "DEBUG"
    INFO = "INFO"
    WARNING = "WARNING"
    ERROR = "ERROR"
    CRITICAL = "CRITICAL"
    SETUP = "SETUP"
    MATCH = "MATCH"
    FILTER = "FILTER"
    POLICY = "POLICY"
    RESULT = "RESULT"
    TRADE = "TRADE"

class PipelinePhase(str, Enum):
    """Defines the valid phases of the 6-phase strategy funnel."""
    REGIME_FILTER = "regime_filter"
    STRUCTURAL_CONTEXT = "structural_context"
    SIGNAL_GENERATOR = "signal_generator"
    SIGNAL_REFINER = "signal_refiner"
    ENTRY_PLANNER = "entry_planner"
    EXIT_PLANNER = "exit_planner"
    SIZE_PLANNER = "size_planner"
    ORDER_ROUTER = "order_router"
    CRITICAL_EVENT_DETECTOR = "critical_event_detector"

--- END FILE: backend/core/enums.py ---

--- START FILE: backend/core/execution.py ---
"""Contains the concrete implementation of an execution handler for backtests.

This module provides the `BacktestExecutionHandler`, which is a concrete
implementation of the `ExecutionHandler` interface. It is responsible for
simulating the execution of trading directives within a backtesting environment.

@layer: Backend (Core)
@dependencies: [backend.core.interfaces, backend.dtos, backend.utils]
@responsibilities:
    - Implement the `ExecutionHandler` interface for simulated backtests.
    - Receive `ExecutionDirective` objects and translate them into trade actions.
    - Interact with a `Tradable` component (e.g., Portfolio) to open trades.
    - Log all execution activities.
"""
from typing import List

# --- CORRECTIE: Importeer de GECENTRALISEERDE interface ---
from backend.core.interfaces.execution import ExecutionHandler
from backend.core.interfaces.portfolio import Tradable
from backend.dtos.execution.execution_directive import ExecutionDirective
from backend.utils.app_logger import LogEnricher

class BacktestExecutionHandler(ExecutionHandler):
    """
    Handles the execution of directives within a simulated backtest environment.
    """
    def __init__(self, tradable: Tradable, logger: LogEnricher):
        self._tradable = tradable
        self._logger = logger

    def execute_plan(self, directives: List[ExecutionDirective]):
        """
        Processes a list of execution directives by calling the appropriate
        methods on the tradable entity (Portfolio).
        """
        for directive in directives:
            # Hier kun je in de toekomst logica toevoegen voor verschillende directive types
            # De huidige implementatie geeft de directive direct door.
            self._tradable.open_trade(directive)

--- END FILE: backend/core/execution.py ---

--- START FILE: backend/core/performance_analyzer.py ---
# backend/core/performance_analyzer.py
"""
Docstring for performance_analyzer.py.

@layer: TODO
@dependencies: TODO
@responsibilities: TODO
"""

class PerformanceAnalyzer:
    """Docstring for PerformanceAnalyzer."""
    pass

--- END FILE: backend/core/performance_analyzer.py ---

--- START FILE: backend/core/portfolio.py ---
# backend/core/portfolio.py
"""
Contains the Portfolio class, which manages the financial state of a backtest.

@layer: Backend
@dependencies:
    - backend.core.interfaces.portfolio: Implements the Tradable protocol.
    - backend.dtos: Uses ExecutionDirective and ClosedTrade DTOs.
@responsibilities:
    - Manages the account balance based on a starting capital.
    - Executes fully-formed ExecutionDirective objects without strategic validation.
    - Holds the state for multiple active trades.
    - Maintains a list of all closed trades as ClosedTrade DTOs.
@inputs:
    - `initial_capital` (float) and `fees_pct` (float) on initialization.
    - `ExecutionDirective` DTOs to be executed.
@outputs:
    - A list of `ClosedTrade` DTOs.
"""
from typing import Any, Dict, List
from uuid import UUID
import pandas as pd

from backend.core.interfaces.portfolio import Tradable
from backend.dtos.execution.execution_directive import ExecutionDirective
from backend.dtos.results.closed_trade import ClosedTrade
from backend.utils.app_logger import LogEnricher
from backend.core.context_recorder import ContextRecorder


class Portfolio(Tradable):
    """
    Manages account capital, active trades, and a list of closed trades.

    This class acts as a stateful ledger. Its primary responsibility is to
    maintain the financial state of the simulation by executing pre-calculated
    ExecutionDirective objects and updating the balance. It is a concrete implementation
    of the Tradable protocol, capable of managing multiple concurrent trades.
    """

    def __init__(self,
                 initial_capital: float,
                 fees_pct: float,
                 logger: LogEnricher,
                 context_recorder: ContextRecorder):
        """
        Initializes the Portfolio.
        """
        self._initial_capital: float = initial_capital
        self._balance: float = initial_capital
        self._fees_pct: float = fees_pct
        self.logger = logger
        self.context_recorder = context_recorder

        self._closed_trades: List[ClosedTrade] = []
        self._active_trades: Dict[UUID, Dict[str, Any]] = {}

    @property
    def initial_capital(self) -> float:
        """Returns the starting capital of the portfolio."""
        return self._initial_capital

    @property
    def balance(self) -> float:
        """Returns the current account balance."""
        return self._balance

    @property
    def closed_trades(self) -> List[ClosedTrade]:
        """Returns the list of all closed trades."""
        return self._closed_trades

    @property
    def active_trades(self) -> Dict[UUID, Dict[str, Any]]:
        """A dictionary of all currently open trades, keyed by correlation_id."""
        return self._active_trades

    @property
    def active_trade_count(self) -> int:
        """Returns the number of active trades."""
        return len(self._active_trades)

    def get_active_trades(self) -> Dict[UUID, Dict[str, Any]]:
        """Returns the dictionary of active trades."""
        return self.active_trades

    def open_trade(self, execution_directive: ExecutionDirective):
        """
        Opens a new trade based on a pre-calculated ExecutionDirective object.
        """
        for trade in self._active_trades.values():
            if trade['asset'] == execution_directive.asset:
                self.logger.error(
                    "Attempted to open a trade on an asset with an existing position.",
                    values={'asset': execution_directive.asset}
                )
                return

        if execution_directive.position_value_quote > self._balance:
            self.logger.error(
                "Insufficient capital to open trade.",
                values={'required': execution_directive.position_value_quote,
                        'available': self._balance}
            )
            return

        # Sla ALLE benodigde data op, inclusief correlation_id en signal_type
        self._active_trades[execution_directive.correlation_id] = {
            "correlation_id": execution_directive.correlation_id,
            "signal_type": execution_directive.signal_type,
            "entry_time": execution_directive.entry_time,
            "asset": execution_directive.asset,
            "direction": execution_directive.direction,
            "entry_price": execution_directive.entry_price,
            "sl_price": execution_directive.sl_price,
            "tp_price": execution_directive.tp_price,
            "position_size_asset": execution_directive.position_size_asset,
            "position_value_eur": execution_directive.position_value_quote,
        }

        self.logger.trade(
            'portfolio.open_trade',
            values={
                'direction': execution_directive.direction.upper(),
                'price': f"{execution_directive.entry_price:,.2f}",
                'sl': f"{execution_directive.sl_price:,.2f}",
                'tp': f"{execution_directive.tp_price:,.2f}"if execution_directive.tp_price else "N/A"
            }
        )

    def process_candle(self, candle: pd.Series):
        """
        Processes the latest market data candle to check for SL/TP hits
        for all active trades.
        """
        if not self._active_trades:
            return

        # FIX: Controleer of de index van de candle (candle.name) een geldig Timestamp-object is
        if not isinstance(candle.name, pd.Timestamp):
            # Log een waarschuwing of negeer de candle
            return

        exit_timestamp = candle.name # Nu weten we zeker dat het een Timestamp is

        trade_ids_to_check = list(self._active_trades.keys())

        for correlation_id in trade_ids_to_check:
            trade = self._active_trades.get(correlation_id)
            if not trade:
                continue

            exit_price = None

            # TODO: In a multi-asset scenario, the candle should contain the asset
            # it belongs to, to match against the trade's asset.

            if trade['direction'] == 'long':
                if candle['low'] <= trade['sl_price']:
                    exit_price = trade['sl_price']
                elif trade['tp_price'] and candle['high'] >= trade['tp_price']:
                    exit_price = trade['tp_price']

            elif trade['direction'] == 'short':
                if candle['high'] >= trade['sl_price']:
                    exit_price = trade['sl_price']
                elif trade['tp_price'] and candle['low'] <= trade['tp_price']:
                    exit_price = trade['tp_price']

            if exit_price:
                self._close_trade(correlation_id, exit_timestamp, exit_price)

    def _close_trade(self, correlation_id: UUID, exit_timestamp: pd.Timestamp, exit_price: float):
        """
        Closes an active trade, calculates PnL, updates the balance, and archives
        the transaction.
        """
        trade_to_close = self._active_trades.pop(correlation_id, None)
        if not trade_to_close:
            return

        price_delta = exit_price - trade_to_close['entry_price']
        if trade_to_close['direction'] == 'short':
            price_delta *= -1

        gross_pnl = price_delta * trade_to_close['position_size_asset']
        fees = trade_to_close['position_value_eur'] * self._fees_pct * 2
        net_pnl = gross_pnl - fees

        self._balance += net_pnl

        # FIX: Voeg de ontbrekende correlation_id en signal_type velden toe
        closed_trade = ClosedTrade(
            correlation_id=trade_to_close['correlation_id'],
            signal_type=trade_to_close['signal_type'],
            entry_time=trade_to_close['entry_time'],
            exit_time=exit_timestamp,
            asset=trade_to_close['asset'],
            direction=trade_to_close['direction'],
            entry_price=trade_to_close['entry_price'],
            exit_price=exit_price,
            sl_price=trade_to_close['sl_price'],
            tp_price=trade_to_close['tp_price'],
            position_value_quote=trade_to_close['position_value_eur'],
            position_size_asset=trade_to_close['position_size_asset'],
            pnl_quote=net_pnl,
        )
        self._closed_trades.append(closed_trade)

        self.logger.trade(
            'portfolio.close_trade',
            values={
                'direction': closed_trade.direction.upper(),
                'price': f"{closed_trade.exit_price:,.2f}",
                'pnl': f"{closed_trade.pnl_quote:,.2f}",
                'result': "WIN" if closed_trade.pnl_quote > 0 else "LOSS"
            }
        )

--- END FILE: backend/core/portfolio.py ---

--- START FILE: backend/core/strategy_engine.py ---
# backend/core/strategy_engine.py
"""
Contains the StrategyEngine, the core component for executing the
signal-driven phases (3-9) of the trading strategy pipeline.

@layer: Backend (Core)
@dependencies:
    - backend.core.interfaces: For adhering to the Environment contract.
    - backend.dtos: For processing the DTO chain.
    - .directive_flattener: To flatten the final trade plan.
@responsibilities:
    - Orchestrates the event-driven loop, timed by the Environment's Clock.
    - Manages the DTO dataflow from Signal generation to final RoutedTradePlan.
    - Flattens approved plans into ExecutionDirectives.
    - Detects critical system-wide events.
    - Bundles all results into a final EngineCycleResult for each tick.
"""
from typing import Dict, List, Any, Generator

from backend.core.interfaces import (
    Clock, BaseStrategyEngine, SignalGenerator, SignalRefiner, EntryPlanner,
    ExitPlanner, SizePlanner, OrderRouter, CriticalEventDetector
)
from backend.dtos.state.trading_context import TradingContext
from backend.dtos.pipeline.signal import Signal
from backend.dtos.pipeline.routed_trade_plan import RoutedTradePlan
from backend.dtos.results.engine_cycle_result import EngineCycleResult
from backend.dtos.execution.critical_event import CriticalEvent
from .directive_flattener import DirectiveFlattener


class StrategyEngine(BaseStrategyEngine):
    """
    The engine that orchestrates the signal-driven workflow (Fase 3-9).
    """

    def __init__(self, active_workers: Dict[str, Any]):
        """Initializes the StrategyEngine with a pre-built set of workers."""
        super().__init__(active_workers=active_workers)

        self._signal_generators: List[SignalGenerator] = active_workers.get('signal_generator', [])
        self._signal_refiners: List[SignalRefiner] = active_workers.get('signal_refiner', [])
        self._entry_planner: EntryPlanner | None = active_workers.get('entry_planner')
        self._exit_planner: ExitPlanner | None = active_workers.get('exit_planner')
        self._size_planner: SizePlanner | None = active_workers.get('size_planner')
        self._order_routers: List[OrderRouter] = active_workers.get('order_router', [])
        self._critical_event_detectors: List[CriticalEventDetector] = active_workers.get(
            'critical_event_detector', []
        )
        self._flattener = DirectiveFlattener()

    def run(self,
            trading_context: TradingContext,
            clock: Clock) -> Generator[EngineCycleResult, None, None]:
        """
        Starts the main event loop and yields a complete result for each cycle.
        """
        for _timestamp, _row in clock.tick():
            final_routed_plans: List[RoutedTradePlan] = []

            raw_signals: List[Signal] = []
            for generator in self._signal_generators:
                raw_signals.extend(generator.process(context=trading_context))

            for signal in raw_signals:
                routed_plan = self._process_single_signal(signal, trading_context)
                if routed_plan:
                    final_routed_plans.append(routed_plan)

            directives = [self._flattener.flatten(plan) for plan in final_routed_plans]

            events: List[CriticalEvent] = []
            for detector in self._critical_event_detectors:
                events.extend(detector.process(final_routed_plans, trading_context))

            yield EngineCycleResult(
                execution_directives=directives,
                critical_events=events
            )

    def _process_single_signal(
        self, signal: Signal, context: TradingContext
    ) -> RoutedTradePlan | None:
        """Leidt één enkel signaal door de trechter van Fase 4 tot 8."""

        approved_signal: Signal | None = signal
        for refiner in self._signal_refiners:
            if not (approved_signal := refiner.process(approved_signal, context)):
                return None

        if not self._entry_planner:
            return None
        if not (entry_signal := self._entry_planner.process(approved_signal, context)):
            return None

        if not self._exit_planner:
            return None
        if not (risk_defined_signal := self._exit_planner.process(entry_signal, context)):
            return None

        if not self._size_planner:
            return None
        if not (trade_plan := self._size_planner.process(risk_defined_signal, context)):
            return None

        final_routed_plan: RoutedTradePlan | None = None
        for router in self._order_routers:
            if final_routed_plan := router.process(trade_plan, context):
                break

        return final_routed_plan

--- END FILE: backend/core/strategy_engine.py ---

--- START FILE: backend/core/__init__.py ---
# backend/core/__init__.py
"""
Exposes the public API of the Core sub-package, making key components
available for other layers of the application, such as the Service layer
and test suites.

This centralization allows for cleaner imports, as consumers can import
directly from `backend.core` without needing to know the internal
file structure.

@layer: Backend (Core)
"""
__all__ = [
    "StrategyEngine",
    "Portfolio",
    "BaseStrategyWorker",
    "ContextRecorder",
    "BacktestExecutionHandler"
]

from .strategy_engine import StrategyEngine
from .portfolio import Portfolio
from .base_worker import BaseStrategyWorker
from .context_recorder import ContextRecorder
from .execution import BacktestExecutionHandler

--- END FILE: backend/core/__init__.py ---

--- START FILE: backend/core/interfaces/connectors.py ---
# backend/core/interfaces/connectors.py
"""
Contains the abstract contracts (Protocols) for all external data connectors.

@layer: Backend (Core Interfaces)
@dependencies: [typing, pandas, backend.dtos]
@responsibilities:
    - Defines the universal `IAPIConnector` contract that every external data
      source (CEX, DEX, wallet) must adhere to.
"""
from typing import Protocol, List, Optional, Any, Dict
import pandas as pd
from backend.dtos.market.trade_tick import TradeTick
from backend.dtos.state.portfolio_state import PortfolioState
from backend.dtos.execution.execution_directive import ExecutionDirective

class IAPIConnector(Protocol):
    """
    An interface for any component that can communicate with an external data
    source.

    This protocol is the cornerstone of the system's agnosticism. It ensures
    that high-level services like the `DataCollectionService` or the
    `LiveEnvironment` can interact with any data source (a CEX via REST, a
    DEX via RPC, or a wallet via CSV) in a consistent, standardized way.
    The interface defines the required methods for both historical data
    retrieval and live trading operations.
    """

    def get_available_pairs(self) -> List[str]:
        """Fetches a list of all available trading pairs from the data source.

        This method is used to discover which assets are supported by the
        connector, allowing for validation and dynamic UI generation.

        Returns:
            List[str]: A list of strings, where each string is a trading pair
                in the format expected by the API (e.g., ['BTC/EUR', 'XRP/EUR']).
        """
        ...

    # --- Historical Data Acquisition ---

    def get_historical_trades(self,
                              pair: str,
                              since: int,
                              until: Optional[int] = None,
                              limit: Optional[int] = None) -> List[TradeTick]:
        """
        Fetches a list of historical transactions (ticks) from the data source.

        Args:
            pair (str): The trading pair to fetch data for.
            since (int): The UNIX timestamp (nanoseconds) to start fetching from.
            until (Optional[int]): An optional UNIX timestamp (nanoseconds) to
                                   stop fetching at.
            limit (Optional[int]): An optional limit on the number of trades
                                   to return.

        Returns:
            A list of validated TradeTick DTOs.
        """
        ...

    def get_historical_ohlcv(self,
                             pair: str,
                             timeframe: str,
                             since: int,
                             until: Optional[int] = None,
                             limit: Optional[int] = None) -> pd.DataFrame:
        """
        Fetches historical OHLCV (candlestick) data from the data source.

        Args:
            pair (str): The trading pair.
            timeframe (str): The timeframe identifier (e.g., '15m', '1h').
            since (int): The UNIX timestamp to start fetching from.
            limit (Optional[int]): An optional limit on the number of candles
                                   to return.

        Returns:
            A pandas DataFrame containing OHLCV data, indexed by timestamp.
        """
        ...

    # --- Real-time Portfolio Management ---

    def get_portfolio_state(self) -> PortfolioState:
        """
        Retrieves the complete, current state of the portfolio from the source.
        
        This is crucial for state reconciliation after a restart or disconnect.
        """
        ...

    def get_open_orders(self) -> List[Dict[str, Any]]:
        """Retrieves a list of all currently open (unfilled) orders."""
        ...

    # --- Real-time Order Management ---

    def place_order(self, directive: ExecutionDirective) -> Dict[str, Any]:
        """
        Places a new order on the exchange based on an ExecutionDirective.
        
        Returns:
            A dictionary containing the response from the exchange, typically
            including the new order's ID.
        """
        ...

    def cancel_order(self, order_id: str) -> Dict[str, Any]:
        """
        Cancels an open order based on its unique identifier.
        
        Returns:
            A dictionary containing the response from the exchange.
        """
        ...

    # --- Real-time Data Streams ---

    def start_market_data_stream(self, pair: str, callback: Any) -> None:
        """
        Starts the PUBLIC data stream for market-wide transactions and events.
        
        Args:
            pair (str): The trading pair to subscribe to.
            callback (Any): A callable function that will be invoked with each
                          new piece of market data.
        """
        ...

    def stop_market_data_stream(self) -> None:
        """Stops the active PUBLIC market data stream."""
        ...

    def start_user_data_stream(self, callback: Any) -> None:
        """
        Starts the PRIVATE, authenticated data stream for account-specific
        updates (e.g., order fills, balance changes).
        
        Args:
            callback (Any): A callable function that will be invoked with each
                          new user data event.
        """
        ...

    def stop_user_data_stream(self) -> None:
        """Stops the active PRIVATE user data stream."""
        ...

--- END FILE: backend/core/interfaces/connectors.py ---

--- START FILE: backend/core/interfaces/engine.py ---
# backend/core/interfaces/engine.py
"""
Contains the behavioral contracts (ABCs) for the core strategy execution engine.

@layer: Backend (Core Interfaces)
@dependencies: [abc, typing, backend.dtos]
@responsibilities:
    - Defines the abstract contract for any component that can execute the
      signal-driven portion of a strategy.
"""
from __future__ import annotations

from abc import ABC, abstractmethod
from typing import Any, Dict, Generator, TYPE_CHECKING

if TYPE_CHECKING:
    from backend.core.interfaces import Clock
    from backend.dtos.state.trading_context import TradingContext
    from backend.dtos.results.engine_cycle_result import EngineCycleResult

class BaseStrategyEngine(ABC):
    """
    Abstract contract for a Strategy Engine.

    This interface defines the "motor" that drives the core trading logic.
    It is designed to be a pure, high-performance generator of TradePlans,
    completely decoupled from the environment in which it operates.
    """
    def __init__(self, active_workers: Dict[str, Any]):
        """Initializes the engine with its pre-built "toolbox" of workers."""
        ...

    @abstractmethod
    def run(self,
            trading_context: 'TradingContext',
            clock: 'Clock') -> Generator['EngineCycleResult', None, None]:
        """
        Starts the main event loop and yields approved TradePlans.

        Args:
            trading_context (TradingContext): The shared context object.
            clock (Clock): The clock that controls the flow of time.

        Yields:
            TradePlan: A fully validated and approved trade plan, ready for execution.
        """
        ...

--- END FILE: backend/core/interfaces/engine.py ---

--- START FILE: backend/core/interfaces/environment.py ---
# backend/core/interfaces/environment.py
"""
Contains the behavioral contracts (ABCs) for the Execution Environment
and its sub-components.

@layer: Backend (Core Interfaces)
@dependencies: [abc, typing, pandas, backend.dtos]
@responsibilities:
    - Defines the abstract contracts for the operational "world".
"""
from __future__ import annotations
from abc import ABC, abstractmethod
from typing import Generator, Tuple, TYPE_CHECKING
import pandas as pd

# --- CORRECTIE: Importeer de ExecutionHandler interface HIER ---
if TYPE_CHECKING:
    from backend.core.interfaces.execution import ExecutionHandler

class DataSource(ABC):
    """Abstract contract for any component that provides market data."""
    @abstractmethod
    def get_data(self) -> pd.DataFrame:
        """Returns the complete historical dataset for the environment."""
        ...

class Clock(ABC):
    """Abstract contract for any component that controls the flow of time."""
    @abstractmethod
    def tick(self) -> Generator[Tuple[pd.Timestamp, pd.Series], None, None]:
        """Yields the next moment in time (timestamp and data row)."""
        ...

class BaseEnvironment(ABC):
    """
    Abstract contract for an Execution Environment.
    This interface defines the "world" in which a strategy operates.
    """
    @property
    @abstractmethod
    def source(self) -> DataSource:
        """The data source for this environment."""
        ...

    @property
    @abstractmethod
    def clock(self) -> Clock:
        """The clock that controls time in this environment."""
        ...

    @property
    @abstractmethod
    def handler(self) -> "ExecutionHandler":
        """The execution handler for this environment."""
        ...

--- END FILE: backend/core/interfaces/environment.py ---

--- START FILE: backend/core/interfaces/execution.py ---
"""Defines the abstract contract for all execution handlers.

This module contains the `ExecutionHandler` Abstract Base Class (ABC), which
enforces a standard interface for any component that executes trading
directives.

@layer: Backend (Core Interfaces)
@dependencies: [abc, backend.dtos]
@responsibilities:
    - Define the `ExecutionHandler` abstract base class.
    - Specify the `execute_plan` method as the required contract for all execution environments.
"""
from __future__ import annotations
from abc import ABC, abstractmethod
from typing import List, TYPE_CHECKING

# CORRECTIE: Importeer DTOs alleen binnen een TYPE_CHECKING block
if TYPE_CHECKING:
    from backend.dtos.execution.execution_directive import ExecutionDirective

class ExecutionHandler(ABC):
    """
    Abstract Base Class that defines the contract for any component capable
    of executing trade directives.
    """

    @abstractmethod
    def execute_plan(self, directives: List["ExecutionDirective"]) -> None:
        """
        Processes a list of execution directives.

        Args:
            directives (List[ExecutionDirective]): The directives to be executed.
        """
        raise NotImplementedError

--- END FILE: backend/core/interfaces/execution.py ---

--- START FILE: backend/core/interfaces/persistors.py ---
# In bestand: backend/core/interfaces/persistors.py
"""
Contains the abstract contracts (Protocols) for all data persistence components.

@layer: Backend (Core Interfaces)
@dependencies: [typing, pandas, backend.dtos]
@responsibilities:
    - Defines the universal `IDataPersistor` contract that every data storage
      mechanism (e.g., Parquet, Database) must adhere to.
"""
from typing import Protocol, List
from backend.dtos.market.trade_tick import TradeTick
from backend.dtos.market.data_coverage import DataCoverage

class IDataPersistor(Protocol):
    """
    An interface for any component that can read from and write to a durable
    data storage.
    """
    def get_last_timestamp(self, pair: str) -> int:
        """
        Retrieves the timestamp (in nanoseconds) of the most recently stored
        trade for a given pair. Returns 0 if no data exists.
        """
        ... # pylint: disable=unnecessary-ellipsis
    def save_trades(self, pair: str, trades: List[TradeTick]) -> None:
        """
        Saves a list of TradeTick DTOs to the persistent storage.
        """
        ... # pylint: disable=unnecessary-ellipsis

    def get_data_coverage(self, pair: str) -> List[DataCoverage]:
        """
        Returns a list of contiguous data blocks available for a given pair.
        """
        ... # pylint: disable=unnecessary-ellipsis

--- END FILE: backend/core/interfaces/persistors.py ---

--- START FILE: backend/core/interfaces/portfolio.py ---
# backend/core/interfaces/portfolio.py
# pylint: disable=unnecessary-ellipsis
"""
Contains the abstract contract (Protocol) for any component that can manage
and execute trades within the S1mpleTrader ecosystem.

@layer: Backend (Core Interfaces)
"""
from __future__ import annotations
from typing import Protocol, List, Dict, Any, runtime_checkable, TYPE_CHECKING
from uuid import UUID
import pandas as pd

# CORRECTIE: Importeer DTOs alleen binnen een TYPE_CHECKING block
if TYPE_CHECKING:
    from backend.dtos.execution.execution_directive import ExecutionDirective
    from backend.dtos.results.closed_trade import ClosedTrade

@runtime_checkable
class Tradable(Protocol):
    """
    An interface for any object that can manage a financial state, open
    positions, and a history of closed trades. This contract ensures that
    high-level components like an ExecutionHandler can interact with any
    portfolio implementation in a consistent way.
    """

    @property
    def initial_capital(self) -> float:
        """The starting capital of the portfolio."""
        ...

    @property
    def balance(self) -> float:
        """The current, real-time balance of the portfolio."""
        ...

    @property
    def active_trades(self) -> Dict[UUID, Dict[str, Any]]:
        """A dictionary of all currently open trades."""
        ...

    @property
    def closed_trades(self) -> List["ClosedTrade"]:
        """A list of all closed trades."""
        ...

    def open_trade(self, execution_directive: "ExecutionDirective") -> None:
        """
        Receives a complete trade plan and processes it to open a new
        position, updating the internal state.
        """
        ...

    def process_candle(self, candle: pd.Series) -> None:
        """
        Processes the latest market data candle to check if any active
        trades should be closed based on their SL/TP levels.
        """
        ...

--- END FILE: backend/core/interfaces/portfolio.py ---

--- START FILE: backend/core/interfaces/worker.py ---
# backend/core/interfaces/worker.py
# pylint: disable=unnecessary-ellipsis
"""
Contains the behavioral contracts (Protocols) for all plugin worker types.
These interfaces are the "constitution" for the S1mpleTrader plugin
ecosystem, ensuring that any component created by a developer will correctly
integrate with the StrategyEngine.
@layer: Backend (Core Interfaces)
@dependencies: [typing, pandas]
@responsibilities:
    - Defines the structural contracts for all types of plugin workers.
    - Enforces the logical data flow of the 9-fase strategy pipeline.
"""
from __future__ import annotations
from typing import (Any, List, Optional, Protocol, TYPE_CHECKING, runtime_checkable)
import pandas as pd

# Use TYPE_CHECKING to prevent circular imports at runtime
if TYPE_CHECKING:
    from backend.dtos.pipeline.signal import Signal
    from backend.dtos.pipeline.entry_signal import EntrySignal
    from backend.dtos.pipeline.risk_defined_signal import RiskDefinedSignal
    from backend.dtos.pipeline.trade_plan import TradePlan
    from backend.dtos.pipeline.routed_trade_plan import RoutedTradePlan
    from backend.dtos.execution.critical_event import CriticalEvent
    from backend.dtos.state.trading_context import TradingContext

# --- Specific Worker Contracts ---

@runtime_checkable
class ContextWorker(Protocol):
    """A contract for a data enrichment worker (Fase 1 & 2)."""
    def __init__(self, name: str, params: Any, logger: Any):
        """Initializes the worker with its name, params, and logger."""
        ...

    def process(self, df: pd.DataFrame, context: "TradingContext") -> pd.DataFrame:
        """Processes the DataFrame to add context."""
        ...

@runtime_checkable
class StrategyWorker(Protocol):
    """A base contract for any worker operating within the main DTO pipeline."""
    def __init__(self, name: str, params: Any, logger: Any):
        """Initializes the worker with its name, params, and logger."""
        ...

# --- Specific Strategy Worker Contracts (The Pipeline) ---

@runtime_checkable
class SignalGenerator(StrategyWorker, Protocol):
    """Fase 3: A contract for a worker that generates trading opportunities."""
    def process(self, context: "TradingContext") -> List["Signal"]:
        """Generates raw Signal DTOs based on the market context."""
        ...

@runtime_checkable
class SignalRefiner(StrategyWorker, Protocol):
    """Fase 4: A contract for a worker that filters raw signals."""
    def process(
        self, signal: "Signal", context: "TradingContext"
    ) -> Optional["Signal"]:
        """Processes a single signal and returns it if valid, otherwise None."""
        ...

@runtime_checkable
class EntryPlanner(StrategyWorker, Protocol):
    """Fase 5: A contract for a worker that defines the entry tactic."""
    def process(
        self, signal: "Signal", context: "TradingContext"
    ) -> Optional["EntrySignal"]:
        """Enriches a Signal with a concrete entry price."""
        ...

@runtime_checkable
class ExitPlanner(StrategyWorker, Protocol):
    """Fase 6: A contract for a worker that defines risk parameters (SL/TP)."""
    def process(
        self, entry_signal: "EntrySignal", context: "TradingContext"
    ) -> Optional["RiskDefinedSignal"]:
        """Enriches an EntrySignal with stop-loss and take-profit prices."""
        ...

@runtime_checkable
class SizePlanner(StrategyWorker, Protocol):
    """Fase 7: A contract for a worker that calculates position size."""
    def process(
        self, risk_defined_signal: "RiskDefinedSignal", context: "TradingContext"
    ) -> Optional["TradePlan"]:
        """Enriches a RiskDefinedSignal with the final position size."""
        ...

@runtime_checkable
class OrderRouter(StrategyWorker, Protocol):
    """Fase 8: A contract for a worker that translates a TradePlan."""
    def process(
        self, trade_plan: "TradePlan", context: "TradingContext"
    ) -> Optional["RoutedTradePlan"]:
        """Enriches a TradePlan with specific order execution instructions."""
        ...

@runtime_checkable
class CriticalEventDetector(StrategyWorker, Protocol):
    """Fase 9: A contract for a worker that scans for systemic events."""
    def process(
        self, routed_trade_plans: List["RoutedTradePlan"], context: "TradingContext"
    ) -> List["CriticalEvent"]:
        """Detects and returns a list of critical events."""
        ...

--- END FILE: backend/core/interfaces/worker.py ---

--- START FILE: backend/core/interfaces/__init__.py ---
# backend/core/interfaces/__init__.py
"""Exposes all interface contracts from a single, clean namespace."""

from .worker import *
from .environment import *
from .engine import *
from .portfolio import *

--- END FILE: backend/core/interfaces/__init__.py ---

--- START FILE: backend/data/loader.py ---
# backend/data/loader.py
"""
Handles loading raw data from CSV files and performing initial cleaning.

@layer: Backend (Data)
@dependencies: [pathlib, pandas, backend.utils.app_logger]
@responsibilities:
    - Loads raw OHLCV data from a specific CSV file.
    - Performs initial data cleaning, sets the timestamp index, and handles NA values.
"""
from pathlib import Path
import pandas as pd
from backend.utils.app_logger import LogEnricher

class DataLoader:
    """Loads and performs initial preparation of OHLCV data from a CSV file."""

    def __init__(self, file_path: str, logger: LogEnricher):
        """Initializes the DataLoader."""
        self.file_path = Path(file_path)
        self.logger = logger
        if not self.file_path.is_file():
            raise FileNotFoundError(f"Data file not found at path: {self.file_path}")

    def load(self) -> pd.DataFrame:
        """Loads data from the CSV, sets the index, and cleans the data."""
        self.logger.info('loader.loading_from', values={'filename': self.file_path.name})

        df: pd.DataFrame = pd.read_csv(self.file_path) # pyright: ignore[reportUnknownMemberType]

        # Converteer timestamp naar datetime objecten, stel in als index en sorteer.
        df['timestamp'] = pd.to_datetime(df['timestamp'], utc=True, errors='coerce')
        df = df.set_index('timestamp').sort_index()

        # Verwijder rijen met ontbrekende waarden om de datakwaliteit te garanderen.
        df = df.dropna() # pyright: ignore[reportUnknownMemberType]

        self.logger.info('loader.load_success')
        return df

--- END FILE: backend/data/loader.py ---

--- START FILE: backend/data/__init__.py ---

--- END FILE: backend/data/__init__.py ---

--- START FILE: backend/data/persistors/parquet_persistor.py ---
# In bestand: backend/data/persistors/parquet_persistor.py
"""
Contains the concrete implementation of the IDataPersistor interface using
a partitioned Parquet dataset.
"""
from pathlib import Path
from typing import Any, List, Optional

import pandas as pd
import pyarrow as pa
import pyarrow.parquet as pq

from backend.core.interfaces.persistors import IDataPersistor
from backend.dtos.market.data_coverage import DataCoverage
from backend.dtos.market.trade_tick import TradeTick


class ParquetPersistor(IDataPersistor):
    """
    An implementation of IDataPersistor that uses a partitioned Parquet dataset,
    optimizing for performance and scalability with large time-series data.
    """

    def __init__(self, data_dir: Path):
        self._data_dir = data_dir
        self._data_dir.mkdir(parents=True, exist_ok=True)

    def _get_dataset_path(self, pair: str) -> Path:
        """Constructs the root directory path for a given trading pair's dataset."""
        return self._data_dir / pair

    def _read_data(self, pair: str) -> Optional[pd.DataFrame]:
        """Reads a partitioned Parquet dataset for a pair into a single DataFrame."""
        dataset_path = self._get_dataset_path(pair)
        if not dataset_path.is_dir():
            return None
        try:
            dataset = pq.ParquetDataset(dataset_path)
            df = dataset.read().to_pandas() # type: ignore
            return df if not df.empty else None
        except (IOError, ValueError, pa.ArrowInvalid):
            return None

    def get_last_timestamp(self, pair: str) -> int:
        """Efficiently retrieves the last timestamp from a partitioned dataset."""
        dataset_path = self._get_dataset_path(pair)
        if not dataset_path.is_dir():
            return 0
        try:
            dataset = pq.ParquetDataset(dataset_path)
            if not dataset.fragments:
                return 0

            # --- DE FIX: Gebruik de correcte API voor Parquet metadata ---

            # 1. Bepaal de index van de 'timestamp' kolom in het schema.
            try:
                timestamp_col_index = dataset.schema.names.index('timestamp')
            except ValueError:
                # Kolom 'timestamp' bestaat niet in het schema.
                return 0

            timestamps: List[Any] = []
            # 2. Itereer door de fragmenten en hun row groups.
            for frag in dataset.fragments:
                for i in range(frag.num_row_groups):
                    row_group_meta = frag.metadata.row_group(i)
                    # 3. Vraag de metadata voor de specifieke kolom op.
                    column_meta = row_group_meta.column(timestamp_col_index)

                    # 4. Controleer of statistieken bestaan en haal .max op.
                    if column_meta.statistics and column_meta.statistics.has_min_max: # type: ignore
                        timestamps.append(column_meta.statistics.max)

            if not timestamps:
                return 0

            max_ts = max(timestamps)
            # Converteer de waarde (die een Arrow-native type kan zijn)
            # naar een nanoseconde integer.
            return int(pd.Timestamp(max_ts).value)

        except (IOError, ValueError, pa.ArrowInvalid, KeyError, IndexError):
            # Vang ook een mogelijke IndexError op als de kolomindex niet gevonden wordt.
            return 0


    def save_trades(self, pair: str, trades: List[TradeTick]) -> None:
        """Saves trades to a partitioned Parquet dataset by year and month."""
        if not trades:
            return

        dataset_path = self._get_dataset_path(pair)

        new_df = pd.DataFrame([trade.model_dump() for trade in trades])
        if new_df.empty:
            return

        new_df['timestamp'] = pd.to_datetime(new_df['timestamp'], utc=True)
        new_df['year'] = new_df['timestamp'].dt.year
        new_df['month'] = new_df['timestamp'].dt.month

        table = pa.Table.from_pandas(new_df, preserve_index=False)

        pq.write_to_dataset( # type: ignore
            table,
            root_path=dataset_path,
            partition_cols=['year', 'month'],
            existing_data_behavior='overwrite_or_ignore'
        )

    def get_data_coverage(self, pair: str) -> List[DataCoverage]:
        """Analyzes the data file and returns a list of contiguous data blocks."""
        df = self._read_data(pair)
        if df is None:
            return []

        df.sort_values(by='timestamp', inplace=True)

        gap_threshold = pd.Timedelta(seconds=60)
        time_diffs = df['timestamp'].diff()
        gap_indices = df.index[time_diffs > gap_threshold].tolist()

        block_starts = df.index[[0] + gap_indices].tolist()
        block_ends = df.index[[i - 1 for i in gap_indices] + [len(df) - 1]].tolist()

        coverage_blocks: List[DataCoverage] = []
        for start_idx, end_idx in zip(block_starts, block_ends):
            block_df = df.loc[start_idx:end_idx]

            coverage_blocks.append(
                DataCoverage(
                    start_time=block_df['timestamp'].iloc[0],
                    end_time=block_df['timestamp'].iloc[-1],
                    trade_count=len(block_df)
                )
            )
        return coverage_blocks

--- END FILE: backend/data/persistors/parquet_persistor.py ---

--- START FILE: backend/dtos/__init__.py ---
# backend/dtos/__init__.py
"""
Exposes the public API of the DTOs sub-package.

This file centralizes all DTO imports, allowing other parts of the
application to import any DTO directly from `backend.dtos` without
needing to know the specific internal file structure.

@layer: Backend (DTO)
"""
__all__ = [
    # commands
    "FetchPeriodCommand",
    "FillGapsCommand",
    "ExtendHistoryCommand",
    "SynchronizationCommand",
    # queries
    "PairsQuery",
    "CoverageQuery",
    "RangeQuery",
    # pipeline
    "Signal",
    "EntrySignal",
    "RiskDefinedSignal",
    "TradePlan",
    "RoutedTradePlan",
    # execution
    "CriticalEvent",
    "ExecutionDirective",
    # market
    "TradeTick",
    "DataCoverage",
    # state
    "PortfolioState",
    "TradingContext",
    # results
    "EngineCycleResult",
    "ClosedTrade",
    "BacktestResult",
]

from .commands import *
from .queries import *
from .pipeline import *
from .execution import *
from .market import *
from .state import *
from .results import *

--- END FILE: backend/dtos/__init__.py ---

--- START FILE: backend/dtos/commands/extend_history_command.py ---
# In bestand: backend/dtos/commands/extend_history_command.py
"""
Contains the Command DTO for extending the history of an existing data archive.

@layer: Backend (DTO/Commands)
@dependencies: [pydantic]
@responsibilities:
    - Defines the data contract for extending an existing data archive further
      into the past.
"""
from pydantic import BaseModel, Field

class ExtendHistoryCommand(BaseModel):
    """
    Data contract for a command to extend an existing data archive further
    into the past from its oldest known data point.
    """
    pair: str = Field(..., description="extend_history_command.pair.desc")
    period_days: int = Field(..., gt=0, description="extend_history_command.period_days.desc")

--- END FILE: backend/dtos/commands/extend_history_command.py ---

--- START FILE: backend/dtos/commands/fetch_period_command.py ---
# In bestand: backend/dtos/commands/fetch_period_command.py
"""
Contains the Command DTO for fetching a specific historical data period.

@layer: Backend (DTO/Commands)
@dependencies: [pydantic, pandas]
@responsibilities:
    - Defines the data contract for fetching a new, specific period of historical data.
"""
from typing import Optional
from pydantic import BaseModel, Field, ConfigDict
import pandas as pd

class FetchPeriodCommand(BaseModel):
    """
    Data contract for a command to fetch a specific, bounded period of
    historical data. This is typically used to build a new archive or
    manually add a large, specific historical block.
    """
    model_config = ConfigDict(arbitrary_types_allowed=True)

    pair: str = Field(..., description="fetch_period_command.pair.desc")
    start_date: pd.Timestamp = Field(..., description="fetch_period_command.start_date.desc")
    end_date: Optional[pd.Timestamp] = Field(None, description="fetch_period_command.end_date.desc")

--- END FILE: backend/dtos/commands/fetch_period_command.py ---

--- START FILE: backend/dtos/commands/fill_gaps_command.py ---
# In bestand: backend/dtos/commands/fill_gaps_command.py
"""
Contains the Command DTO for a request to fill all gaps in a data archive.

@layer: Backend (DTO/Commands)
@dependencies: [pydantic]
@responsibilities:
    - Defines the data contract for a request to find and fill all gaps in a
      data archive.
"""
from pydantic import BaseModel, Field

class FillGapsCommand(BaseModel):
    """
    Data contract for a command to find and fill all identified gaps in an
    existing historical data archive for a specific pair.
    """
    pair: str = Field(..., description="fill_gaps_command.pair.desc")

--- END FILE: backend/dtos/commands/fill_gaps_command.py ---

--- START FILE: backend/dtos/commands/synchronization_command.py ---
# In bestand: backend/dtos/commands/synchronization_command.py
"""
Contains the Command DTO for a data synchronization action.

@layer: Backend (DTO/Commands)
@dependencies: [pydantic]
@responsibilities:
    - Defines the data contract for initiating an incremental data synchronization.
"""
from pydantic import BaseModel, Field

class SynchronizationCommand(BaseModel):
    """
    Data contract for a command to synchronize a data archive with the
    latest trades since the last known data point.
    """
    pair: str = Field(..., description="synchronization_command.pair.desc")

--- END FILE: backend/dtos/commands/synchronization_command.py ---

--- START FILE: backend/dtos/commands/__init__.py ---
# backend/dtos/__init__.py
"""
Exposes the public API of the DTOs sub-package.

This file centralizes all DTO imports, allowing other parts of the
application to import any DTO directly from `backend.dtos` without
needing to know the specific internal file structure.

@layer: Backend (DTO)
"""
__all__ = [
"SynchronizationCommand",
"ExtendHistoryCommand",
"FillGapsCommand",
"FetchPeriodCommand",
]

from .synchronization_command import SynchronizationCommand
from .extend_history_command import ExtendHistoryCommand
from .fill_gaps_command import FillGapsCommand
from .fetch_period_command import FetchPeriodCommand

--- END FILE: backend/dtos/commands/__init__.py ---

--- START FILE: backend/dtos/execution/critical_event.py ---
# backend/dtos/critical_event.py
"""
Contains the DTO for a critical event notification.

@layer: Backend (DTO)
@dependencies: [pydantic, pandas, uuid]
@responsibilities:
    - Defines the standardized data structure for a critical event detected
      by the StrategyEngine.
"""
import uuid
import pandas as pd
from pydantic import BaseModel, ConfigDict, Field

class CriticalEvent(BaseModel):
    """
    Represents a minimal, agnostic notification of a critical event.

    This DTO acts as an "alarm bell" generated by a CriticalEventDetector
    plugin (Fase 9). It signals that a significant systemic event has
    occurred. The detailed context that led to this event is stored
    separately in the ContextRecorder. The RunService interprets this
    event and decides on the appropriate action (e.g., halting trading).

    Attributes:
        correlation_id (uuid.UUID): A unique ID for this specific event.
        event_type (str): A string identifier for the event type (e.g., "MAX_DRAWDOWN_BREACHED").
        timestamp (pd.Timestamp): The timestamp of the candle on which the event was detected.
    """
    correlation_id: uuid.UUID = Field(default_factory=uuid.uuid4)
    event_type: str
    timestamp: pd.Timestamp

    model_config = ConfigDict(
        arbitrary_types_allowed=True
    )

--- END FILE: backend/dtos/execution/critical_event.py ---

--- START FILE: backend/dtos/execution/execution_directive.py ---
# backend/dtos/execution_directive.py
"""
Contains the DTO for a final, flattened execution instruction.

@layer: Backend (DTO)
@dependencies: [pydantic, pandas, uuid]
@responsibilities:
    - Defines the flat, universal contract for an instruction sent to the
      ExecutionHandler.
"""
import uuid
from typing import Literal, Optional, Dict, Any
from pydantic import BaseModel, ConfigDict
import pandas as pd

class ExecutionDirective(BaseModel):
    """
    A flat, final, and universal instruction for the ExecutionHandler.

    This DTO is a flattened representation of a RoutedTradePlan and contains
    all necessary information to execute, manage, and track a trade. It serves
    as the definitive, simple contract between the StrategyEngine's output and
    the execution layer.

    Attributes:
        correlation_id (uuid.UUID): The unique ID from the source Signal.
        signal_type (str): The name of the logic that generated the original signal.
        asset (str): The asset to be traded.
        direction (Literal['long', 'short']): The direction of the trade.
        entry_price (float): The calculated entry price for the trade.
        sl_price (float): The absolute stop-loss price.
        tp_price (Optional[float]): The absolute take-profit price, if any.
        position_value_quote (float): The total value of the position in the quote currency.
        position_size_asset (float): The size of the position in the base asset.
        order_type (Literal['market', 'limit']): The fundamental order type.
        limit_price (Optional[float]): The price for a limit order.
        time_in_force (Literal['GTC', 'IOC', 'FOK']): How long the order remains valid.
        post_only (bool): Flag to ensure the order is a "maker" order.
        execution_strategy (Optional[Literal['twap']]): Label for an algorithmic strategy.
        strategy_params (Optional[Dict[str, Any]]): Parameters for the algorithmic strategy.
        preferred_exchange (Optional[str]): A hint for the ExecutionHandler.
        entry_time (pd.Timestamp): From the original signal.
    """
    # Traceability & Identity
    correlation_id: uuid.UUID
    signal_type: str

    # Core Trade Parameters
    asset: str
    direction: Literal['long', 'short']
    entry_price: float
    sl_price: float
    tp_price: Optional[float]

    # Sizing
    position_value_quote: float
    position_size_asset: float

    # Tactical Execution Instructions
    order_type: Literal['market', 'limit']
    limit_price: Optional[float] = None
    time_in_force: Literal['GTC', 'IOC', 'FOK'] = 'GTC'
    post_only: bool = False
    execution_strategy: Optional[Literal['twap']] = None
    strategy_params: Optional[Dict[str, Any]] = None
    preferred_exchange: Optional[str] = None

    # Timestamps
    entry_time: pd.Timestamp

    model_config = ConfigDict(
        arbitrary_types_allowed=True
    )

--- END FILE: backend/dtos/execution/execution_directive.py ---

--- START FILE: backend/dtos/execution/__init__.py ---
__all__ = [
    "ExecutionDirective",
    "CriticalEvent",
]

from .execution_directive import ExecutionDirective
from .critical_event import CriticalEvent

--- END FILE: backend/dtos/execution/__init__.py ---

--- START FILE: backend/dtos/market/data_coverage.py ---
# In bestand: backend/dtos/market/data_coverage.py
"""
Contains the DTO for representing a contiguous block of historical data.

@layer: Backend (DTO/Market)
@dependencies: [pydantic, pandas]
@responsibilities:
    - Defines the standardized data structure for reporting on the availability
      of historical market data in the persistence layer.
"""
from pydantic import BaseModel, ConfigDict, Field
import pandas as pd

class DataCoverage(BaseModel):
    """
    A data contract representing a single, contiguous block of historical data.

    This DTO is returned by the IDataPersistor's get_data_coverage method to
    provide a clear summary of the available data, which can be used to
    validate backtest ranges or identify gaps in the data history.
    """
    start_time: pd.Timestamp = Field(
        ...,
        description="data_coverage.start_time.desc"
    )
    end_time: pd.Timestamp = Field(
        ...,
        description="data_coverage.end_time.desc"
    )
    trade_count: int = Field(
        ...,
        description="data_coverage.trade_count.desc"
    )

    model_config = ConfigDict(arbitrary_types_allowed=True)

--- END FILE: backend/dtos/market/data_coverage.py ---

--- START FILE: backend/dtos/market/trade_tick.py ---
# backend/dtos/common.py
"""
Bevat de meest basale, herbruikbare DTO's die op meerdere plekken worden gebruikt.

@layer: Backend (DTO)
@dependencies: [pydantic, pandas]
@responsibilities:
    - Definieert de gestandaardiseerde datastructuur voor een enkele transactie (tick).
"""
from __future__ import annotations
from typing import Any, Literal

import pandas as pd
from pydantic import BaseModel, Field, field_validator


class TradeTick(BaseModel):
    """
    Een enkele, daadwerkelijk uitgevoerde transactie (Time & Sales),
    gemodelleerd naar de Kraken REST API 'Trades' response.
    """
    model_config = {"arbitrary_types_allowed": True}

    price: float = Field(..., description="De prijs van de transactie.")
    volume: float = Field(
        ..., description="De omvang (volume) van de transactie."
    )
    timestamp: pd.Timestamp = Field(
        ..., description="De exacte UTC timestamp van de transactie."
    )
    side: Literal['buy', 'sell'] = Field(
        ..., description="De kant van de agressor ('buy' of 'sell')."
    )
    order_type: Literal['market', 'limit'] = Field(
        ..., description="Het type order van de agressor ('market' of 'limit')."
    )
    misc: str = Field(
        "", description="Diverse aanvullende informatie van de API."
    )

    @field_validator('timestamp', mode='before')
    @classmethod
    def convert_unix_to_timestamp(cls, v: Any) -> pd.Timestamp:
        """Converteert een UNIX timestamp (in seconden) naar een Pandas Timestamp."""
        if isinstance(v, (int, float)):
            return pd.to_datetime(v, unit='s', utc=True)
        return v

--- END FILE: backend/dtos/market/trade_tick.py ---

--- START FILE: backend/dtos/market/__init__.py ---
__all__ = [
    "TradeTick",
    "DataCoverage",
]

from .trade_tick import TradeTick
from .data_coverage import DataCoverage

--- END FILE: backend/dtos/market/__init__.py ---

--- START FILE: backend/dtos/pipeline/entry_signal.py ---
# backend/dtos/entry_signal.py
"""
Contains the data class for a signal enriched with an entry tactic.

@layer: Backend (DTO)
@dependencies: [pydantic, pandas, uuid]
@responsibilities:
    - Defines the data structure for a signal that has been enriched with
      a concrete entry tactic by an EntryPlanner.
"""
import uuid
from pydantic import BaseModel, ConfigDict
from .signal import Signal

class EntrySignal(BaseModel):
    """Represents a signal with a concrete entry tactic.

    This DTO is created by an EntryPlanner (Fase 5a). It enriches a raw
    Signal DTO with a calculated entry price and a descriptive entry method.
    It serves as the input for the ExitPlanner (Fase 5b).

    Attributes:
        correlation_id (uuid.UUID): The unique ID inherited from the source Signal.
        signal: the original Signal DTO.
        entry_price (float): The calculated entry price for the trade.
    """
    correlation_id: uuid.UUID
    signal: Signal
    entry_price: float

    model_config = ConfigDict(
        arbitrary_types_allowed=True
    )

--- END FILE: backend/dtos/pipeline/entry_signal.py ---

--- START FILE: backend/dtos/pipeline/risk_defined_signal.py ---
# backend/dtos/risk_defined_signal.py
"""
Contains the data class for a signal enriched with exit prices (risk definition).

@layer: Backend (DTO)
@dependencies: [pydantic, uuid]
@responsibilities:
    - Defines the data structure for a signal that has been enriched with
      absolute stop-loss and take-profit prices by an ExitPlanner.
"""
import uuid
from typing import Optional
from pydantic import BaseModel, ConfigDict
from .entry_signal import EntrySignal

class RiskDefinedSignal(BaseModel):
    """Represents a signal with its risk parameters fully defined.

    This DTO is created by an ExitPlanner (Fase 5b). It enriches an
    EntrySignal with the absolute stop-loss and (optional) take-profit prices.
    It serves as the direct input for the SizePlanner (Fase 5c), providing all
    necessary information to calculate position size based on risk.

    Attributes:
        correlation_id (uuid.UUID): The unique ID from the source Signal.
        entry_signal: the original EntrySignal DTO.
        sl_price (float): The absolute stop-loss price, defining the risk boundary.
        tp_price (Optional[float]): The absolute take-profit price, if any.
    """
    correlation_id: uuid.UUID
    entry_signal: EntrySignal
    sl_price: float
    tp_price: Optional[float] = None

    model_config = ConfigDict(
        arbitrary_types_allowed=True
    )

--- END FILE: backend/dtos/pipeline/risk_defined_signal.py ---

--- START FILE: backend/dtos/pipeline/routed_trade_plan.py ---
# backend/dtos/routed_trade_plan.py
"""
Contains the DTO that represents a TradePlan decorated with execution tactics.

@layer: Backend (DTO)
@dependencies: [pydantic, uuid, backend.dtos.trade_plan]
@responsibilities:
    - Defines the universal blueprint for how an order should be executed.
"""
import uuid
from typing import Literal, Optional, Dict, Any
from pydantic import BaseModel, ConfigDict

from .trade_plan import TradePlan

class RoutedTradePlan(BaseModel):
    """
    The universal blueprint for how an order should be executed.

    This DTO is the output of an OrderRouter plugin (Fase 8). It takes the
    complete strategic intent (the TradePlan) and enriches it with concrete,
    technical execution instructions for the ExecutionHandler. It serves as the
    definitive contract between the strategy layer and the execution layer.

    Attributes:
        correlation_id (uuid.UUID): The unique ID from the source Signal.
        trade_plan (TradePlan): The nested strategic plan to be executed.
        order_type (Literal['market', 'limit']): The fundamental order type.
        limit_price (Optional[float]): The price for a limit order.
        time_in_force (Literal['GTC', 'IOC', 'FOK']): How long the order remains valid.
        post_only (bool): Flag to ensure the order is a "maker" order.
        execution_strategy (Optional[Literal['twap']]): Label for an algorithmic strategy.
        strategy_params (Optional[Dict[str, Any]]): Parameters for the algorithmic strategy.
        preferred_exchange (Optional[str]): A hint for the ExecutionHandler.
    """
    correlation_id: uuid.UUID
    trade_plan: TradePlan

    # --- Tactical Execution Instructions ---
    order_type: Literal['market', 'limit']
    limit_price: Optional[float] = None
    time_in_force: Literal['GTC', 'IOC', 'FOK'] = 'GTC'
    post_only: bool = False
    execution_strategy: Optional[Literal['twap']] = None
    strategy_params: Optional[Dict[str, Any]] = None
    preferred_exchange: Optional[str] = None

    model_config = ConfigDict(
        arbitrary_types_allowed=True
    )

--- END FILE: backend/dtos/pipeline/routed_trade_plan.py ---

--- START FILE: backend/dtos/pipeline/signal.py ---
# backend/dtos/signal.py
"""
Contains the data class for a raw, unfiltered signal event.

@layer: Backend (DTO)
@dependencies: [pydantic, pandas, uuid]
@responsibilities:
    - Defines the standardized data structure for a raw signal event, generated
      by a SignalGenerator plugin.
"""
import uuid
from typing import Literal
from pydantic import BaseModel, Field, ConfigDict
import pandas as pd

class Signal(BaseModel):
    """Represents a pure, unrefined signal event from a specific strategy logic.

    This DTO signifies that a pattern or condition was met at a specific time.
    It contains only the essential "what, where, and when" information, plus a
    unique identifier for traceability. This is the first DTO in the
    SignalOrchestrator's pipeline.

    Attributes:
        correlation_id (uuid.UUID): The unique ID that links this signal and all
                                    subsequent objects (Trade, ClosedTrade) to its
                                    full context log in the ContextRecorder.
                                    Generated by the SignalGenerator.
        timestamp (pd.Timestamp): The timestamp of the candle where the signal occurred.
        asset (str): The asset for which the signal was generated (e.g., 'BTC/EUR').
        direction (str): The directional bias of the signal ('long' or 'short').
        signal_type (str): The name of the logic that generated the signal (e.g.,
                           'golden_cross', 'fvg_entry_detector'). This defines the
                           identity of the signal.
    """
    correlation_id: uuid.UUID = Field(default_factory=uuid.uuid4)
    timestamp: pd.Timestamp
    asset: str
    direction: Literal['long', 'short']
    signal_type: str

    model_config = ConfigDict(
        arbitrary_types_allowed=True
    )

--- END FILE: backend/dtos/pipeline/signal.py ---

--- START FILE: backend/dtos/pipeline/trade_plan.py ---
# backend/dtos/trade_plan.py
"""
Contains the data class for a complete, executable trade plan.

@layer: Backend (DTO)
@dependencies: [pydantic, uuid, .risk_defined_signal]
@responsibilities:
    - Defines the standardized data structure for a fully planned trade, ready
      for the OrderRouter.
"""
import uuid
from pydantic import BaseModel, ConfigDict
from .risk_defined_signal import RiskDefinedSignal

class TradePlan(BaseModel):
    """
    Represents a complete strategic plan for a single trade.

    This DTO is created by a SizePlanner (Fase 7). It enriches a
    RiskDefinedSignal with the final position size and value. It contains
    all necessary strategic information before being passed to the
    OrderRouter (Fase 8) to be translated into tactical execution instructions.

    Attributes:
        correlation_id (uuid.UUID): The unique ID from the source Signal.
        risk_defined_signal (RiskDefinedSignal): The nested DTO from the previous phase.
        position_value_quote (float): The total value of the position in the quote currency.
        position_size_asset (float): The size of the position in the base asset.
    """
    correlation_id: uuid.UUID
    risk_defined_signal: RiskDefinedSignal
    position_value_quote: float
    position_size_asset: float

    model_config = ConfigDict(
        arbitrary_types_allowed=True
    )

--- END FILE: backend/dtos/pipeline/trade_plan.py ---

--- START FILE: backend/dtos/pipeline/__init__.py ---
__all__ = [
    "Signal",
    "EntrySignal",
    "RiskDefinedSignal",
    "TradePlan",
    "RoutedTradePlan",
]

from .signal import Signal
from .entry_signal import EntrySignal
from .risk_defined_signal import RiskDefinedSignal
from .trade_plan import TradePlan
from .routed_trade_plan import RoutedTradePlan

--- END FILE: backend/dtos/pipeline/__init__.py ---

--- START FILE: backend/dtos/queries/coverage_query.py ---
# In bestand: backend/dtos/queries/coverage_query.py
"""
Contains the Query DTO for a data coverage request.

@layer: Backend (DTO/Queries)
@dependencies: [pydantic]
@responsibilities:
    - Defines the data contract for requesting a data coverage map for a pair.
"""
from pydantic import BaseModel, Field

class CoverageQuery(BaseModel):
    """
    Data contract for a query to retrieve the data coverage map for a
    specific trading pair from the archive.
    """
    pair: str = Field(..., description="coverage_query.pair.desc")

--- END FILE: backend/dtos/queries/coverage_query.py ---

--- START FILE: backend/dtos/queries/pairs_query.py ---
# In bestand: backend/dtos/queries/pairs_query.py
"""
Contains the Query DTO for requesting available pairs from a connector.

@layer: Backend (DTO/Queries)
@dependencies: [pydantic]
@responsibilities:
    - Defines the data contract for requesting a list of available trading pairs.
"""
from pydantic import BaseModel, Field

class PairsQuery(BaseModel):
    """
    Data contract for a query to retrieve the list of all available trading
    pairs from a specific, named connector instance.
    """
    exchange_id: str = Field(..., description="pairs_query.exchange_id.desc")

--- END FILE: backend/dtos/queries/pairs_query.py ---

--- START FILE: backend/dtos/queries/range_query.py ---
# In bestand: backend/dtos/queries/range_query.py
"""
Contains the Query DTO for requesting historical data within a specific range.

@layer: Backend (DTO/Queries)
@dependencies: [pydantic, pandas]
@responsibilities:
    - Defines the data contract for requesting raw trade data for a specific pair
      and time period.
"""
from pydantic import BaseModel, Field, ConfigDict
import pandas as pd

class RangeQuery(BaseModel):
    """
    Data contract for a query to retrieve raw trade ticks for a specific
    pair and time range from the data archive.
    """
    model_config = ConfigDict(arbitrary_types_allowed=True)

    pair: str = Field(..., description="range_query.pair.desc")
    start_date: pd.Timestamp = Field(..., description="range_query.start_date.desc")
    end_date: pd.Timestamp = Field(..., description="range_query.end_date.desc")

--- END FILE: backend/dtos/queries/range_query.py ---

--- START FILE: backend/dtos/queries/__init__.py ---
# backend/dtos/__init__.py
"""
Exposes the public API of the DTOs sub-package.

This file centralizes all DTO imports, allowing other parts of the
application to import any DTO directly from `backend.dtos` without
needing to know the specific internal file structure.

@layer: Backend (DTO)
"""
__all__ = [
"PairsQuery",
"CoverageQuery",
"RangeQuery",
]

from .pairs_query import PairsQuery
from .coverage_query import CoverageQuery
from .range_query import RangeQuery

--- END FILE: backend/dtos/queries/__init__.py ---

--- START FILE: backend/dtos/results/backtest_result.py ---
# backend/dtos/backtest_result.py
"""
Contains the data class for storing the complete result of a single backtest run.

@layer: Backend (DTO)
@dependencies: [pydantic, pandas]
@responsibilities:
    - Defines the standardized data structure for holding all results from a
      single backtest run, ready for analysis and presentation.
"""
from typing import Dict, Any
from pydantic import BaseModel, ConfigDict
import pandas as pd

class BacktestResult(BaseModel):
    """A container for all results of a single backtest run.

    This object acts as a standardized Data Transfer Object (DTO) that holds
    all the essential, aggregated outputs from a backtest analysis, produced
    by the PerformanceAnalyzer.

    Attributes:
        trades_df (pd.DataFrame): A DataFrame containing all ClosedTrade objects.
        equity_curve (pd.Series): A Series representing the portfolio's equity over time.
        drawdown_curve (pd.Series): A Series representing the portfolio's drawdown.
        metrics (Dict[str, Any]): A dictionary of calculated performance metrics.
        initial_capital (float): The starting capital for the run.
    """
    trades_df: pd.DataFrame
    equity_curve: pd.Series
    drawdown_curve: pd.Series
    metrics: Dict[str, Any]
    initial_capital: float

    model_config = ConfigDict(
        arbitrary_types_allowed=True
    )

--- END FILE: backend/dtos/results/backtest_result.py ---

--- START FILE: backend/dtos/results/closed_trade.py ---
# backend/dtos/closed_trade.py
"""
Contains the data class for a closed trade, representing a completed transaction.

@layer: Backend (DTO)
@dependencies: [pydantic, pandas, uuid]
@responsibilities:
    - Defines the standardized data structure for a trade that has been fully
      executed and resulted in a profit or loss.
"""
import uuid
from typing import Optional
from pydantic import BaseModel, ConfigDict
import pandas as pd

class ClosedTrade(BaseModel):
    """Represents the final, recorded result of a single completed trade.

    This DTO is created by the Portfolio after a position is closed. It serves
    as the definitive, historical record for performance analysis and reporting.
    It contains all information from the original TradePlan, enriched with the
    actual exit details and the resulting profit or loss.

    Attributes:
        correlation_id (uuid.UUID): The unique ID linking this record to its
                                    full context log. Inherited from the TradePlan.
        entry_time (pd.Timestamp): The timestamp when the trade was opened.
        exit_time (pd.Timestamp): The timestamp when the trade was closed.
        asset (str): The asset that was traded.
        direction (str): The direction of the trade ('long' or 'short').
        signal_type (str): The name of the logic that generated the original signal.
        entry_price (float): The actual entry price.
        exit_price (float): The actual exit price.
        sl_price (float): The original stop-loss price from the TradePlan.
        tp_price (Optional[float]): The original take-profit price, if any.
        position_value_quote (float): The initial value of the position.
        position_size_asset (float): The size of the position.
        pnl_quote (float): The net profit or loss of the trade, after fees.
    """
    correlation_id: uuid.UUID
    entry_time: pd.Timestamp
    exit_time: pd.Timestamp
    asset: str
    direction: str
    signal_type: str
    entry_price: float
    exit_price: float
    sl_price: float
    tp_price: Optional[float] = None
    position_value_quote: float
    position_size_asset: float
    pnl_quote: float

    model_config = ConfigDict(
        arbitrary_types_allowed=True
    )

--- END FILE: backend/dtos/results/closed_trade.py ---

--- START FILE: backend/dtos/results/engine_cycle_result.py ---
# backend/dtos/engine_cycle_result.py
"""
Contains the DTO that represents the complete output of a single StrategyEngine cycle.

@layer: Backend (DTO)
@dependencies: [pydantic, .execution_directive, .critical_event]
@responsibilities:
    - Bundles all outcomes of a single engine tick into one object.
    - Decouples new trade instructions from systemic event notifications.
"""
from typing import List
from pydantic import BaseModel, ConfigDict
from backend.dtos.execution.execution_directive import ExecutionDirective
from backend.dtos.execution.critical_event import CriticalEvent

class EngineCycleResult(BaseModel):
    """
    Represents the complete output of a single processing cycle (tick)
    of the StrategyEngine. It decouples trade proposals from critical events.

    This object is yielded by the StrategyEngine on every tick and allows the
    consuming service (e.g., BacktestService) to intelligently decide on the
    next course of action.

    Attributes:
        execution_directives (List[ExecutionDirective]): A list of new trades to be executed.
        critical_events (List[CriticalEvent]): A list of detected systemic events.
    """
    execution_directives: List[ExecutionDirective]
    critical_events: List[CriticalEvent]

    model_config = ConfigDict(arbitrary_types_allowed=True)

--- END FILE: backend/dtos/results/engine_cycle_result.py ---

--- START FILE: backend/dtos/results/__init__.py ---
__all__ = [
    "ClosedTrade",
    "EngineCycleResult",
    "BacktestResult",
]

from .closed_trade import ClosedTrade
from .engine_cycle_result import EngineCycleResult
from .backtest_result import BacktestResult

--- END FILE: backend/dtos/results/__init__.py ---

--- START FILE: backend/dtos/state/portfolio_state.py ---
# backend/dtos/portfolio_state.py
"""
Contains the DTO for a snapshot of the portfolio's financial state.

@layer: Backend (DTO)
@dependencies: [pydantic]
@responsibilities:
    - Defines the standardized, read-only data structure that represents the
      financial state of a portfolio at a specific moment in time.
    - Defines the specific data contracts for open positions and open orders.
"""
from __future__ import annotations
from uuid import UUID
from typing import List, Literal, Optional
from pydantic import BaseModel, Field
import pandas as pd

class OpenPosition(BaseModel):
    """
    A data contract for a single, currently open position.
    """
    model_config = {"arbitrary_types_allowed": True}

    correlation_id: UUID = Field(..., description="open_position.correlation_id.desc")
    asset: str = Field(..., description="open_position.asset.desc")
    direction: Literal['long', 'short'] = Field(..., description="open_position.direction.desc")
    entry_price: float = Field(..., description="open_position.entry_price.desc")
    position_size_asset: float = Field(..., description="open_position.position_size_asset.desc")
    entry_timestamp: pd.Timestamp = Field(..., description="open_position.entry_timestamp.desc")


class OpenOrder(BaseModel):
    """
    A data contract for a single, currently open (unfilled) order.
    """
    order_id: str = Field(..., description="open_order.order_id.desc")
    asset: str = Field(..., description="open_order.asset.desc")
    side: Literal['buy', 'sell'] = Field(..., description="open_order.side.desc")
    order_type: Literal['limit',
                        'market',
                        'stop_loss',
                        'take_profit'] = Field(..., description="open_order.order_type.desc")
    amount: float = Field(..., description="open_order.amount.desc")
    price: Optional[float] = Field(None, description="open_order.price.desc")


class PortfolioState(BaseModel):
    """
    A read-only snapshot of the financial state of the portfolio.

    This DTO is used to pass the current financial reality to various
    components, such as the StrategyEngine, without exposing the full
    Portfolio object. This ensures that the state can be read but not
    modified, adhering to a clear data flow.
    """
    equity: float = Field(
        ...,
        description="portfolio_state.equity.desc"
    )
    available_cash: float = Field(
        ...,
        description="portfolio_state.available_cash.desc"
    )
    total_exposure_quote: float = Field(
        ...,
        description="portfolio_state.total_exposure_quote.desc"
    )
    open_positions: List[OpenPosition] = Field( # type: ignore
        default_factory=list,
        description="portfolio_state.open_positions.desc"
    )
    open_orders: List[OpenOrder] = Field( # type: ignore
        default_factory=list,
        description="portfolio_state.open_orders.desc"
    )

# Pylance Suppression Note for 'future Me':
# The Pylance linter may incorrectly flag the 'default_factory=list' lines
# below as a 'reportUnknownVariableType' error. This is a known false
# positive. Using 'default_factory=list' is the idiomatic and correct
# Pydantic approach for providing a mutable default (an empty list).
# The '# type: ignore' comment is intentionally used here to suppress this
# specific, non-critical linter warning while maintaining the functionally
# superior implementatio

--- END FILE: backend/dtos/state/portfolio_state.py ---

--- START FILE: backend/dtos/state/trading_context.py ---
# backend/dtos/trading_context.py
"""
Contains the data class for the TradingContext.

@layer: Backend (DTO)
@dependencies: [pydantic, pandas, backend.core.interfaces]
@responsibilities:
    - Defines a standardized data structure to hold all shared, contextual
      information available during a single run.
"""

from __future__ import annotations

from typing import Any, Dict, TYPE_CHECKING

import pandas as pd
from pydantic import BaseModel, ConfigDict, PrivateAttr

from backend.core.context_recorder import ContextRecorder

# Use TYPE_CHECKING to avoid runtime circular imports.
if TYPE_CHECKING:
    from backend.core.interfaces import Tradable

def _new_struct_registry() -> Dict[str, Any]:
    """Typed factory for the structural context registry."""
    return {}

class TradingContext(BaseModel):
    """Container for all shared data available during a single run.

    This object represents the single source of truth for contextual data made
    available to plugins at the time of execution.

    Attributes:
        enriched_df: Fully enriched DataFrame, containing all indicator and context columns.
        portfolio: Reference to the active portfolio object.
        context_recorder: Recorder used to persist detailed context for inspection.
    """

    enriched_df: pd.DataFrame
    portfolio: "Tradable"  # Forward reference to avoid runtime import cycles.
    context_recorder: ContextRecorder

    # Runtime-only registry for complex, non-tabular context data.
    # Not part of the model schema / validation / serialization.
    _structural_context_registry: Dict[str, Any] = PrivateAttr(
        default_factory=_new_struct_registry
    )

    model_config = ConfigDict(arbitrary_types_allowed=True)

    # --- Public API -------------------------------------------------------

    def register_structural_data(self, source_plugin: str, data: Any) -> None:
        """Register a complex, non-tabular data structure for later retrieval.

        Args:
            source_plugin: Identifier of the producing plugin.
            data: Arbitrary, non-tabular structure to store.
        """
        self._structural_context_registry[source_plugin] = data

    def get_structural_data(self, source_plugin: str, default: Any | None = None) -> Any | None:
        """Retrieve previously registered complex data by plugin identifier.

        Args:
            source_plugin: Identifier of the producing plugin.
            default: Value returned if no entry exists (defaults to None).

        Returns:
            The stored data for the given plugin identifier, or `default` if not present.
        """
        return self._structural_context_registry.get(source_plugin, default)

    def has_structural_data(self, source_plugin: str) -> bool:
        """Check whether complex data was registered for the given plugin.

        Args:
            source_plugin: Identifier of the producing plugin.

        Returns:
            True if data is present, False otherwise.
        """
        return source_plugin in self._structural_context_registry

--- END FILE: backend/dtos/state/trading_context.py ---

--- START FILE: backend/dtos/state/__init__.py ---
__all__ = [
    "PortfolioState",
    "TradingContext",
]

from .portfolio_state import PortfolioState
from .trading_context import TradingContext

--- END FILE: backend/dtos/state/__init__.py ---

--- START FILE: backend/environments/backtest_environment.py ---
# backend/environments/backtest_environment.py
"""
Contains the BacktestEnvironment and its specialized sub-components, providing
a complete, isolated world for running historical strategy tests.

@layer: Backend (Environment)
@dependencies: [pandas, backend.core.interfaces, backend.data.loader]
@responsibilities:
    - Implements the BaseEnvironment interface for backtesting.
    - Orchestrates the creation of CSV-based data sources, simulated clocks,
      and backtest execution handlers.
"""
import logging
from pathlib import Path
from typing import Generator, Tuple

import pandas as pd

from backend.config.schemas.app_schema import AppConfig
# --- CORRECTIE: Importeer de JUISTE, GECENTRALISEERDE interfaces ---
from backend.core.interfaces import BaseEnvironment, Clock, DataSource, Tradable
from backend.core.interfaces.execution import ExecutionHandler
from backend.data.loader import DataLoader
from backend.utils.app_logger import LogEnricher
# --- CORRECTIE: Importeer de CONCRETE handler ---
from backend.core.execution import BacktestExecutionHandler


# --- Sub-component Implementations ---
class CSVDataSource(DataSource):
    """A data source that loads market data from a CSV file."""

    def __init__(self, source_dir: str, trading_pair: str, timeframe: str, logger: LogEnricher):
        self._logger = logger
        base_path = Path(source_dir)
        pair_filename = trading_pair.replace('/', '_')
        filename = f"{pair_filename}_{timeframe}.csv"
        file_path = base_path / filename

        self._data_loader = DataLoader(str(file_path), self._logger)
        self._data: pd.DataFrame = pd.DataFrame()

    def get_data(self) -> pd.DataFrame:
        """Loads data if not already loaded, then returns it."""
        if self._data.empty:
            self._data = self._data_loader.load()
        return self._data

class SimulatedClock(Clock):
    """A clock that simulates the passage of time by iterating over a DataFrame."""

    def __init__(self, df: pd.DataFrame):
        self._df = df

    def tick(self) -> Generator[Tuple[pd.Timestamp, pd.Series], None, None]:
        """Yields each row of the DataFrame as a moment in time."""
        for timestamp, row in self._df.iterrows():
            assert isinstance(timestamp, pd.Timestamp)
            yield timestamp, row

# --- Main Environment Class ---
class BacktestEnvironment(BaseEnvironment):
    """
    The concrete implementation of a BaseEnvironment for running backtests.
    """
    def __init__(self, app_config: AppConfig, tradable: Tradable):
        """Initializes the environment and constructs its sub-components."""
        self._logger = LogEnricher(logging.getLogger(__name__))

        self._source = CSVDataSource(
            source_dir=app_config.platform.data.source_dir,
            trading_pair=app_config.run.data.trading_pair,
            timeframe=app_config.run.data.timeframe,
            logger=self._logger
        )
        self._clock = SimulatedClock(self._source.get_data())
        self._handler = BacktestExecutionHandler(tradable, self._logger)

    @property
    def source(self) -> DataSource:
        return self._source

    @property
    def clock(self) -> Clock:
        return self._clock

    @property
    def handler(self) -> ExecutionHandler:
        return self._handler

--- END FILE: backend/environments/backtest_environment.py ---

--- START FILE: backend/environments/live_environment.py ---
# backend/environments/live_environment.py
"""
Docstring for live_environment.py.

@layer: TODO
@dependencies: TODO
@responsibilities: TODO
"""

class LiveTradeEnvironment:
    """Docstring for LiveTradeEnvironment."""

--- END FILE: backend/environments/live_environment.py ---

--- START FILE: backend/environments/paper_environment.py ---
# backend/environments/paper_environment.py
"""
Docstring for paper_environment.py.

@layer: TODO
@dependencies: TODO
@responsibilities: TODO
"""

class PaperTradeEnvironment:
    """Docstring for PaperTradeEnvironment."""
    pass

--- END FILE: backend/environments/paper_environment.py ---

--- START FILE: backend/environments/__init__.py ---
# backend/environments/__init__.py
"""
Exposes the public API of the Environments sub-package.
"""
__all__ = [
    "BacktestEnvironment",
#    "LiveEnvironment",
#    "PaperEnvironment",
]

from .backtest_environment import BacktestEnvironment
#from .live_environment import LiveEnvironment
#from .paper_environment import PaperEnvironment

--- END FILE: backend/environments/__init__.py ---

--- START FILE: backend/environments/api_connectors/kraken_connector.py ---
# backend/environments/api_connectors/kraken_connector.py
"""
Contains the concrete implementation of an APIConnector for the Kraken exchange.

@layer: Backend (Environment/APIConnectors)
@dependencies: [requests, pydantic, pandas, backend.core.interfaces, backend.dtos]
@responsibilities:
    - Implements the IAPIConnector interface for the Kraken REST API.
    - Fetches raw market data and translates it into application-specific DTOs.
    - Handles API-specific details like pagination, error parsing, and retries.
"""
import time
from typing import cast, List, Dict, Any, Optional
import requests
import pandas as pd
from pydantic import ValidationError

from backend.core.interfaces.connectors import IAPIConnector
from backend.dtos.market.trade_tick import TradeTick
from backend.utils.app_logger import LogEnricher
from backend.config.schemas.connectors.kraken_schema import KrakenPublicConfig

class KrakenAPIConnector(IAPIConnector):
    """
    A concrete implementation of the IAPIConnector interface that communicates
    with the public Kraken REST API.
    """

    def __init__(self, logger: LogEnricher, config: KrakenPublicConfig):
        self.logger = logger
        self.config = config
        self.session = requests.Session()
        self.session.headers.update({'User-Agent': 'S1mpleTraderV2'})

    def _request(self, endpoint: str, params: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """
        Executes a single, robust request to a public Kraken API endpoint,
        including the full retry logic.
        """
        attempt = 1
        while attempt <= self.config.retries.max_attempts:
            try:
                log_msg = (
                    f"Requesting '{endpoint}' (attempt {attempt}/"
                    f"{self.config.retries.max_attempts})"
                )
                self.logger.info(log_msg)
                response = self.session.get(f"{self.config.base_url}{endpoint}", params=params)
                response.raise_for_status()

                data = cast(Dict[str, Any], response.json())
                if data.get('error'):
                    self.logger.error(
                        f"Kraken API error for endpoint '{endpoint}': {data['error']}"
                    )
                    return {}

                return data

            except requests.RequestException as e:
                self.logger.error(f"HTTP Request failed (attempt {attempt}): {e}")
                if attempt == self.config.retries.max_attempts:
                    self.logger.error("Max retries reached. Aborting request.")
                    return {}
                time.sleep(self.config.retries.delay_seconds)
                attempt += 1
        return {}

    def get_historical_trades(
        self, pair: str, since: int, until: Optional[int] = None, limit: Optional[int] = None
    ) -> List[TradeTick]:
        all_trades: List[TradeTick] = []
        current_since_ns = str(since)

        # Hou de timestamp van de allerlaatste trade bij om duplicaten te voorkomen
        last_processed_timestamp_ns = since

        while True:
            data = self._request("/Trades", params={'pair': pair, 'since': current_since_ns})
            result = cast(Dict[str, Any], data.get('result', {}))

            if not result:
                break

            pair_key = next(iter(result.keys()), None)
            trade_data = cast(List[List[Any]], result.get(pair_key, [])) if pair_key else []

            if not trade_data:
                break

            last_api_timestamp_ns = result.get('last')

            for trade in trade_data:
                try:
                    timestamp_obj = pd.to_datetime(float(trade[2]), unit='s', utc=True)
                    trade_timestamp_ns = int(timestamp_obj.value)

                    # **DE ECHTE FIX: Sla trades over die we al hebben verwerkt.**
                    # Dit is de meest robuuste manier om duplicaten te voorkomen.
                    if trade_timestamp_ns <= last_processed_timestamp_ns:
                        continue

                    if until and trade_timestamp_ns > until:
                        return all_trades

                    side = 'sell' if trade[3] == 's' else 'buy'
                    order_type = 'market' if trade[4] == 'm' else 'limit'
                    tick = TradeTick(
                        price=float(trade[0]), volume=float(trade[1]),
                        timestamp=timestamp_obj, side=side,
                        order_type=order_type, misc=str(trade[5])
                    )
                    all_trades.append(tick)

                    # Update de laatst geziene timestamp
                    last_processed_timestamp_ns = trade_timestamp_ns

                except (ValidationError, IndexError, ValueError) as e:
                    self.logger.warning(f"Skipping invalid trade data: {trade}. Error: {e}")

            if last_api_timestamp_ns == current_since_ns:
                break

            current_since_ns = last_api_timestamp_ns
            time.sleep(1)

        return all_trades

    def get_historical_ohlcv(
        self,
        pair: str,
        timeframe: str,
        since: int,
        until: Optional[int] = None,
        limit: Optional[int] = None
    ) -> pd.DataFrame:
        """
        Fetches historical OHLCV (candlestick) data from the data source.
        """
        # Kraken API gebruikt 'interval' voor timeframe. Standaard is 1 minuut.
        # We moeten onze timeframe string (bv. '15m') omzetten naar minuten.
        interval_map = {
            '1m': 1, '5m': 5, '15m': 15, '30m': 30, '1h': 60, '4h': 240, '1d': 1440,
            '1w': 10080
        }
        interval = interval_map.get(timeframe.lower())

        if interval is None:
            raise ValueError(
                f"Unsupported timeframe: {timeframe}. "
                f"Supported values are: {list(interval_map.keys())}"
            )

        params: Dict[str, Any] = {
            'pair': pair,
            'interval': interval,
            'since': since
        }

        data = self._request("/OHLC", params=params)
        result = cast(Dict[str, Any], data.get('result', {}))

        if not result:
            return pd.DataFrame() # Retourneer een lege DataFrame bij geen resultaat

        pair_key = next(iter(result.keys()), None)
        ohlcv_data = cast(List[List[Any]], result.get(pair_key, [])) if pair_key else []

        if not ohlcv_data:
            return pd.DataFrame()

        # Definieer de kolomnamen volgens de Kraken API documentatie
        columns = ["timestamp", "open", "high", "low", "close", "vwap", "volume", "count"]
        df = pd.DataFrame(ohlcv_data, columns=columns)

        # Converteer data naar de juiste types
        df['timestamp'] = pd.to_datetime(df['timestamp'], unit='s', utc=True)
        df.set_index('timestamp', inplace=True)

        numeric_columns = ["open", "high", "low", "close", "vwap", "volume"]
        df[numeric_columns] = df[numeric_columns].astype(float)
        df['count'] = df['count'].astype(int)

        return df

--- END FILE: backend/environments/api_connectors/kraken_connector.py ---

--- START FILE: backend/utils/app_logger.py ---
# utils/app_logger.py
"""
Configures and provides the application's logging system.

This module is the single source of truth for all logging-related setup.
It is designed to be configured once at the application's entry point.

@layer: Utility
@dependencies:
    - Translator: The `LogFormatter` receives a `Translator` instance to translate log message keys.
    - Constants: Uses `core.constants` for log level definitions and default profile names.
@responsibilities:
    - Defines the custom `LogFormatter` to handle translation and indentation of log messages.
    - Defines the `LogEnricher` adapter, which is the standard logger interface for the application.
    - Defines the `LogProfiler` to filter logs based on the configured profile.
    - Provides the central `configure_logging` function to bootstrap the logging system.
@inputs:
    - The application `config` dictionary.
    - A `Translator` instance.
@outputs:
    - A fully configured root logger (side-effect).
"""

# 1. Standard Library Imports
import logging
import sys
from typing import Any, Dict, List, Literal, MutableMapping, Optional, Tuple

# 3. Our Application Imports
from backend.utils.translator import Translator
from backend.core.enums import LogLevel
from backend.config.schemas.platform_schema import LoggingConfig

class LogFormatter(logging.Formatter):
    """A custom log formatter that handles translation, value formatting, and indentation.

    This formatter intercepts the log record, translates the message key if
    applicable, formats the translated string with any provided values, and
    applies an indentation level based on the record's context.
    """

    def __init__(self,
                 fmt: Optional[str] = None,
                 datefmt: Optional[str] = None,
                 style: Literal['%', '{', '$'] = '%',
                 translator: Optional[Translator] = None):
        """Initializes the LogFormatter.

        Args:
            fmt (str, optional): The format string for the log. Defaults to None.
            datefmt (str, optional): The format string for dates. Defaults to None.
            style (str, optional): The formatting style. Defaults to '%'.
            translator (Translator, optional): The translator instance for
                                               translating log keys. Defaults to None.
        """
        super().__init__(fmt, datefmt, style)
        self.translator = translator

    def format(self, record: logging.LogRecord) -> str:
        """Formats the log record by translating, populating, and indenting it.

        Args:
            record (logging.LogRecord): The log record to format.

        Returns:
            The fully formatted log message string.
        """
        key = record.msg
        translated_template = key
        values_dict = getattr(record, 'values', {})

        # Step 1: Translate the message key, if it's a valid key.
        if self.translator and isinstance(key, str) and '.' in key and ' ' not in key:
            translated_template = self.translator.get(key, default=key)

        # Step 2: Format the template with any provided values.
        final_message = translated_template
        if values_dict:
            try:
                final_message = translated_template.format(**values_dict)
            except (KeyError, TypeError):
                final_message = f"{translated_template} [FORMATTING ERROR]"

        record.msg = final_message
        record.args = ()

        # Step 3: Apply our custom indentation directly to the message content.
        indent_level = getattr(record, 'indent', 0)
        indented_message = "  " * indent_level + final_message

        # Place the fully prepared (and indented) message back into the record.
        record.msg = indented_message

        # Step 4: Let the original Formatter handle the primary layout (e.g., [INFO   ]).
        return super().format(record)

class LogEnricher(logging.LoggerAdapter[logging.Logger]):
    """A logger adapter that enriches log records with indentation and context.

    This adapter provides the standard logging interface for the application. Its
    main purpose is to shuttle contextual data (like 'indent' or 'values') into
    the 'extra' payload of a log record, which can then be used by the
    `LogFormatter`. It also provides convenience methods for custom log levels.
    """

    def __init__(self, logger: logging.Logger, indent: int = 0):
        """Initializes the LogEnricher adapter.

        Args:
            logger: The logger instance to wrap.
            indent: The indentation level for messages from this logger.
        """
        super().__init__(logger, {'indent': indent})

    def process(
        self, msg: Any, kwargs: MutableMapping[str, Any]
    ) -> Tuple[Any, MutableMapping[str, Any]]:
        """Merges the adapter's contextual information into the kwargs.

        Args:
            msg: The log message.
            kwargs: Keyword arguments to the logging call.

        Returns:
            A tuple containing the message and the modified keyword arguments.
        """
        # Ensure 'extra' dictionary exists and merge adapter's context into it.
        kwargs["extra"] = kwargs.get("extra", {})
        kwargs["extra"].update(self.extra)

        # Move 'values' from kwargs into the 'extra' dict for the formatter.
        if 'values' in kwargs:
            kwargs['extra']['values'] = kwargs.pop('values')

        return msg, kwargs

    # --- Convenience methods for custom levels ---
    def setup(self, key: str, **values: Any) -> None:
        """Logs a message with the SETUP level."""
        self.log(CUSTOM_LEVELS[LogLevel.SETUP], key, values=values)

    def match(self, key: str, **values: Any) -> None:
        """Logs a message with the MATCH level."""
        self.log(CUSTOM_LEVELS[LogLevel.MATCH], key, values=values)

    def filter(self, key: str, **values: Any) -> None:
        """Logs a message with the FILTER level."""
        self.log(CUSTOM_LEVELS[LogLevel.FILTER], key, values=values)

    def policy(self, key: str, **values: Any) -> None:
        """Logs a message with the POLICY level."""
        self.log(CUSTOM_LEVELS[LogLevel.POLICY], key, values=values)

    def result(self, key: str, **values: Any) -> None:
        """Logs a message with the RESULT level."""
        self.log(CUSTOM_LEVELS[LogLevel.RESULT], key, values=values)

    def trade(self, key: str, **values: Any) -> None:
        """Logs a message with the TRADE level."""
        self.log(CUSTOM_LEVELS[LogLevel.TRADE], key, values=values)

class LogProfiler(logging.Filter):
    """A logging filter that allows messages based on the active profile."""

    def __init__(self, profile: str, profile_definitions: Dict[str, List[LogLevel]]):
        """Initializes the filter.

        Args:
            profile: The name of the active logging profile.
            profile_definitions: A dictionary defining all available profiles
                                 and their allowed log level names.
        """
        super().__init__()
        allowed_levels_for_profile = profile_definitions.get(profile, [])
        self.allowed_levels = {level.value for level in allowed_levels_for_profile}

    def filter(self, record: logging.LogRecord) -> bool:
        """Determines if a log record should be processed.

        Args:
            record: The log record to check.

        Returns:
            True if the record's level name is in the allowed set for the
            active profile, False otherwise.
        """
        return record.levelname in self.allowed_levels

# Define and register custom log levels. This mapping is an implementation
# detail of the logger and is therefore defined here, not in core.constants.
CUSTOM_LEVELS = {
    LogLevel.SETUP: 15,
    LogLevel.MATCH: 22,
    LogLevel.FILTER: 23,
    LogLevel.POLICY: 24,
    LogLevel.RESULT: 25,
    LogLevel.TRADE: 26,
}

def configure_logging(logging_config: LoggingConfig, translator: Translator):
    """Configures the central, root logger for the entire application.

    This function should be called only once from `main.py`. It sets up custom
    log levels, creates a handler, attaches the custom `LogFormatter` and
    `LogProfiler` filter, and adds the handler to the root logger.

    Args:
        logging_config: The Pydantic model for the logging configuration.
        translator: An existing translator instance to be used by the formatter.
    """
    for level_enum, level_value in CUSTOM_LEVELS.items():
        logging.addLevelName(level_value, level_enum.value)

    # Use dotted access on the Pydantic model
    log_profile = logging_config.profile
    profile_definitions = logging_config.profiles
    log_format = '[%(levelname)-8s] %(message)s'

    logger = logging.getLogger()
    logger.setLevel(logging.DEBUG)

    # Clear any existing handlers to prevent duplicate logs.
    if logger.hasHandlers():
        logger.handlers.clear()

    handler = logging.StreamHandler(sys.stdout)
    # The Formatter is the only component that needs the translator.
    handler.setFormatter(LogFormatter(log_format, translator=translator))
    handler.addFilter(LogProfiler(log_profile, profile_definitions))
    logger.addHandler(handler)

--- END FILE: backend/utils/app_logger.py ---

--- START FILE: backend/utils/data_utils.py ---
# backend/utils/data_utils.py
"""
Utility functions for data manipulation
"""

--- END FILE: backend/utils/data_utils.py ---

--- START FILE: backend/utils/dynamic_loader.py ---
# backend/utils/dynamic_loader.py
"""
Handles the dynamic loading of classes from plugin modules.

@layer: Backend (Utility)
@dependencies: [importlib]
@responsibilities:
    - Provides a single function to dynamically import a class from a module
      using a string-based path and class name.
"""

import importlib
from typing import Any

def load_class_from_module(module_path: str, class_name: str) -> Any:
    """
    Dynamically imports a module and returns a specific class from it.

    Args:
        module_path (str): The full, dot-separated path to the Python module
                           (e.g., "plugins.signal_generators.my_plugin.worker").
        class_name (str): The exact name of the class to load from the module.

    Raises:
        ImportError: If the module cannot be found.
        AttributeError: If the class does not exist within the module.

    Returns:
        The loaded class object (not an instance).
    """
    try:
        module = importlib.import_module(module_path)
        loaded_class = getattr(module, class_name)
        return loaded_class
    except ImportError as e:
        raise ImportError(f"Could not import module '{module_path}': {e}") from e
    except AttributeError as e:
        raise AttributeError(
            f"Class '{class_name}' not found in module '{module_path}': {e}"
        ) from e

--- END FILE: backend/utils/dynamic_loader.py ---

--- START FILE: backend/utils/translator.py ---
# utils/translator.py
"""
Handles loading and retrieving translated strings for the application.

@layer: Utility
@dependencies:
    - Constants: Uses `core.constants` to get the default language and locale directory path.
@responsibilities:
    - Loads the appropriate language file based on the application configuration.
    - Provides a `get` method to retrieve translated strings using dot-notation keys.
    - Provides a `get_param_name` method for the special case of parameter display names.
@inputs:
    - The application `config` dictionary on initialization.
    - Dot-notation keys and format values for getter methods.
@outputs:
    - Translated and formatted strings.
"""

# 1. Standard Library Imports
from pathlib import Path
from typing import Any, Dict

# 2. Third-Party Imports
import yaml

# 3. Our Application Imports
from backend.config.schemas.platform_schema import PlatformConfig

class Translator:
    """Loads and manages internationalization (i18n) strings from YAML files.

    This class is instantiated once at startup. It loads the appropriate
    language file based on the application configuration and provides methods
    to retrieve translated strings using a dot-notation key.
    """
    def __init__(self, platform_config: PlatformConfig):
        """Initializes the Translator by loading the appropriate language file.

        Args:
            app_config (AppConfig): The application Pydantic config object.
        """
        lang_path = Path('locales') / f"{platform_config.language}.yaml"
        self.strings: Dict[str, Any] = {}
        try:
            with open(lang_path, 'r', encoding='utf-8') as f:
                self.strings = yaml.safe_load(f)
        except FileNotFoundError:
            print(f"WARNING: Language file not found at {lang_path}")

    def get(self, key: str, default: str | None = None, **kwargs: Any) -> str:
        """Retrieves and formats a nested translated string using dot-notation.

        If the key is not found, it returns the default value, or the key
        itself if no default is provided.

        Args:
            key (str): The dot-notation key (e.g., 'app.start').
            default (str, optional): A fallback value to return if the key is
                                     not found. Defaults to None.
            **kwargs: Values to format into the translated string.

        Returns:
            The translated and formatted string.
        """
        try:
            value: Any = self.strings
            for part in key.split('.'):
                value = value[part]

            # A valid translation must be a string. If we resolved a dict
            # (incomplete key), it's invalid.
            if not isinstance(value, str):
                return default or key

            return value.format(**kwargs)
        except (KeyError, TypeError):
            return default or key

    def get_param_name(self, param_path: str, default: str | None = None) -> str:
        """Retrieves a display name for a full parameter path.

        This performs a direct lookup in the 'params_display_names' dictionary
        within the language file, which is a flat key-value map.

        Args:
            param_path (str): The full parameter path to look up.
            default (str, optional): The value to return if the path is not
                                     found. Defaults to the original param_path.

        Returns:
            The display name or a fallback value.
        """
        param_dict = self.strings.get('params_display_names', {})
        return param_dict.get(param_path, default or param_path)

--- END FILE: backend/utils/translator.py ---

--- START FILE: backend/utils/__init__.py ---

--- END FILE: backend/utils/__init__.py ---

--- START FILE: config/index.yaml ---
# Placeholder for config/index.yaml

--- END FILE: config/index.yaml ---

--- START FILE: config/platform.yaml ---
# config/platform.yaml
# De minimale, fundamentele configuratie voor S1mpleTrader V2.

language: 'en'
plugins_root_path: 'plugins'

data:
  source_dir: 'source_data'

portfolio:
  initial_capital: 10000.0
  fees_pct: 0.001

logging:
  profile: 'analysis'
  profiles:
    developer: ['WARNING', 'ERROR', 'CRITICAL']
    analysis: ['DEBUG', 'INFO', 'MATCH', 'FILTER', 'TRADE', 'RESULT', 'WARNING', 'ERROR', 'CRITICAL']

--- END FILE: config/platform.yaml ---

--- START FILE: config/__init__.py ---

--- END FILE: config/__init__.py ---

--- START FILE: config/optimizations/optimize_atr_params.yaml ---
# Placeholder for config/optimizations/optimize_atr_params.yaml

--- END FILE: config/optimizations/optimize_atr_params.yaml ---

--- START FILE: config/overrides/use_eth_pair.yaml ---
# Placeholder for config/overrides/use_eth_pair.yaml

--- END FILE: config/overrides/use_eth_pair.yaml ---

--- START FILE: config/runs/mss_fvg_strategy.yaml ---
# Placeholder for config/runs/mss_fvg_strategy.yaml

--- END FILE: config/runs/mss_fvg_strategy.yaml ---

--- START FILE: config/variants/robustness_test.yaml ---
# Placeholder for config/variants/robustness_test.yaml

--- END FILE: config/variants/robustness_test.yaml ---

--- START FILE: frontends/__init__.py ---

--- END FILE: frontends/__init__.py ---

--- START FILE: frontends/cli/presenters/optimization_presenter.py ---
# frontends/cli/presenters/optimization_presenter.py
"""
Docstring for optimization_presenter.py.

@layer: TODO
@dependencies: TODO
@responsibilities: TODO
"""

class OptimizationPresenter:
    """Docstring for OptimizationPresenter."""
    pass

--- END FILE: frontends/cli/presenters/optimization_presenter.py ---

--- START FILE: frontends/cli/reporters/cli_reporter.py ---
# frontends/cli/reporters/cli_reporter.py
"""
Docstring for cli_reporter.py.

@layer: TODO
@dependencies: TODO
@responsibilities: TODO
"""

class CliReporter:
    """Docstring for CliReporter."""
    pass

--- END FILE: frontends/cli/reporters/cli_reporter.py ---

--- START FILE: frontends/web/api/main.py ---
# frontends/web/api/main.py
"""
FastAPI application entry point
"""

--- END FILE: frontends/web/api/main.py ---

--- START FILE: frontends/web/api/__init__.py ---

--- END FILE: frontends/web/api/__init__.py ---

--- START FILE: frontends/web/api/routers/backtest_router.py ---
# frontends/web/api/routers/backtest_router.py
"""
API endpoints for running backtests
"""

--- END FILE: frontends/web/api/routers/backtest_router.py ---

--- START FILE: frontends/web/api/routers/plugins_router.py ---
# frontends/web/api/routers/plugins_router.py
"""
API endpoints for plugins
"""

--- END FILE: frontends/web/api/routers/plugins_router.py ---

--- START FILE: frontends/web/api/routers/__init__.py ---

--- END FILE: frontends/web/api/routers/__init__.py ---

--- START FILE: locales/en.yaml ---
# locales/en.yaml

# This file contains user-facing text for the application.
# For the MVP, we use English keys that map to English text.

# --- Labels for Platform Configuration schema ---
platform_config:
  core:
    language:
      desc: "The default language for the application's user interface and logs."
    plugins_root_path:
      desc: "The root directory where the application will scan for plugins."
    logging:
      profile:
        desc: "The active logging profile ('developer' or 'analysis'), which determines which log messages are shown."
      profiles:
        desc: "The definition of all available logging profiles and the log levels they include."
  
  services:
    data_collection:
      max_history_days:
        desc: "The absolute maximum number of days of history a user can request in a single operation. Acts as a safety limit."
      warn_history_days:
        desc: "The number of days after which the UI should display a warning for a large historical data request."

  data:
    source_dir:
      desc: "The root directory where raw, downloaded source data (e.g., from CSV files) is stored."

  portfolio:
    defaults:
      initial_capital:
        desc: "The default starting capital for any new portfolio or backtest."
      fees_pct:
        desc: "The default transaction fee percentage to apply to trades (e.g., 0.001 for 0.1%)."

# --- Labels for App Composition Schema ---
app_config:
  platform:
    desc: "The global, platform-wide configuration settings."
  run:
    desc: "The specific configuration blueprint for this particular run."

# --- Labels for Run Blueprint Schema ---
run_blueprint:
  data:
    desc: "Defines the market data to be used for this run."
    trading_pair:
      desc: "The trading pair to be analyzed (e.g., 'BTC/EUR')."
    timeframe:
      desc: "The timeframe for the OHLCV data (e.g., '15m', '1h')."
  taskboard:
    desc: "Assigns specific plugins (by name) to the different phases of the analytical pipeline."
  workforce:
    desc: "Defines the user-configured parameters for each plugin used in the taskboard."
    worker:
      params:
        desc: "A dictionary of key-value pairs that override the default parameters of this specific plugin."

# --- Labels for Plugin Manifest Schema ---
manifest:
  core_identity:
    desc: "System-level fields that identify the schema version of this manifest."
    apiVersion:
      desc: "The schema version of the manifest file (e.g., 's1mpletrader.io/v1')."
    kind:
      desc: "The type of this configuration document, which is always 'PluginManifest'."
  identification:
    desc: "Descriptive metadata that identifies the plugin to both the system and the user."
    name:
      desc: "The unique, machine-readable name of the plugin (snake_case)."
    display_name:
      desc: "The human-readable name of the plugin as shown in the UI."
    type:
      desc: "The functional category that determines in which phase of the pipeline this plugin operates."
    version:
      desc: "The semantic version of the plugin (e.g., '1.0.0')."
    description:
      desc: "A brief, clear explanation of what the plugin does."
    author:
      desc: "The name of the developer or team that created the plugin."
  dependencies:
    desc: "Defines the data contract for the plugin's interaction with the market data context."
    requires:
      desc: "A list of data columns that this plugin expects as input."
    provides:
      desc: "A list of new data columns that this plugin adds as output."
  permissions:
    desc: "Defines the security permissions required by the plugin to operate."
    network_access:
      desc: "A list of allowed network destinations (URLs) the plugin can access."
    filesystem_access:
      desc: "A list of allowed file or directory paths the plugin can access."

# --- Labels for Connector Configurations ---
connectors:
  definition:
    type:
      desc: "The type of the connector implementation to use (e.g., 'kraken_public', 'kraken_private'). This determines which configuration options are available."
    config:
      desc: "The specific configuration block for the chosen connector type."
  root:
    desc: "The main dictionary defining all available connector instances for the platform."

kraken:
  retries:
    desc: "Configuration for the retry strategy on failed API requests."
    max_attempts:
      desc: "The maximum number of times to retry a failed API request."
    delay_seconds:
      desc: "The number of seconds to wait between retry attempts."
  public:
    base_url:
      desc: "The base URL for Kraken's public, unauthenticated API endpoints."
  private:
    api_key:
      desc: "The API key for authenticating with private endpoints. Should be loaded from an environment variable."
    api_secret:
      desc: "The API secret for signing requests to private endpoints. Should be loaded from an environment variable."
    base_url:
      desc: "The base URL for Kraken's private, authenticated API endpoints."

# --- Application labels ---
app:
  start: "S1mpleTrader is starting..."

plugin_registry:
  scan_start: "Scanning for plugins in path: '{path}'..."
  scan_complete: "Scan complete. Found and registered {count} valid plugins."

loader:
  loading_from: "Loading data from {filename}..."
  load_success: "Data successfully loaded and prepared."

plugin_registry:
  scan_complete: "Scan complete. Found and registered {count} valid plugins."

worker_builder:
  build_success: "Successfully built worker '{name}'."

data_coverage:
  start_time:
    desc: "The timestamp of the earliest trade in the data block."
  end_time:
    desc: "The timestamp of the latest trade in the data block."
  trade_count:
    desc: "The total number of trades within this data block."

history_build_request:
  pair:
    desc: "The trading pair to build the historical archive for (e.g., 'XRP/EUR')."
  start_date:
    desc: "The earliest date (inclusive) to fetch historical data for."

sync_request:
  pair:
    desc: "The trading pair to synchronize the latest data for (e.g., 'BTC/EUR')."

backfill_request:
  pair:
    desc: "The trading pair to backfill the latest data for (e.g., 'ADA/EUR')."


--- END FILE: locales/en.yaml ---

--- START FILE: locales/nl.yaml ---
app:
  start: "S1mpleTrader wordt gestart..."
--- END FILE: locales/nl.yaml ---

--- START FILE: plugins/__init__.py ---

--- END FILE: plugins/__init__.py ---

--- START FILE: plugins/critical_event_detectors/max_drawdown_overlay/plugin_manifest.yaml ---
name: max_drawdown_overlay
type: portfolio_overlay
...
--- END FILE: plugins/critical_event_detectors/max_drawdown_overlay/plugin_manifest.yaml ---

--- START FILE: plugins/critical_event_detectors/max_drawdown_overlay/schema.py ---
# plugins/portfolio_overlays/max_drawdown_overlay/schema.py
"""
Docstring for schema.py.

@layer: TODO
@dependencies: TODO
@responsibilities: TODO
"""

class MaxDrawdownOverlayParams:
    """Docstring for MaxDrawdownOverlayParams."""
    pass

--- END FILE: plugins/critical_event_detectors/max_drawdown_overlay/schema.py ---

--- START FILE: plugins/critical_event_detectors/max_drawdown_overlay/worker.py ---
# plugins/portfolio_overlays/max_drawdown_overlay/worker.py
"""
Docstring for worker.py.

@layer: TODO
@dependencies: TODO
@responsibilities: TODO
"""

class MaxDrawdownOverlay:
    """Docstring for MaxDrawdownOverlay."""
    pass

--- END FILE: plugins/critical_event_detectors/max_drawdown_overlay/worker.py ---

--- START FILE: plugins/critical_event_detectors/max_drawdown_overlay/__init__.py ---

--- END FILE: plugins/critical_event_detectors/max_drawdown_overlay/__init__.py ---

--- START FILE: plugins/critical_event_detectors/max_drawdown_overlay/tests/test_worker.py ---
# plugins/portfolio_overlays/max_drawdown_overlay/tests/test_worker.py
"""
Docstring for test_worker.py.

@layer: TODO
@dependencies: TODO
@responsibilities: TODO
"""

class TestMaxDrawdownOverlay:
    """Docstring for TestMaxDrawdownOverlay."""
    pass

--- END FILE: plugins/critical_event_detectors/max_drawdown_overlay/tests/test_worker.py ---

--- START FILE: plugins/critical_event_detectors/max_drawdown_overlay/tests/__init__.py ---

--- END FILE: plugins/critical_event_detectors/max_drawdown_overlay/tests/__init__.py ---

--- START FILE: plugins/entry_planners/__init__.py ---

--- END FILE: plugins/entry_planners/__init__.py ---

--- START FILE: plugins/entry_planners/liquidity_target_exit/plugin_manifest.yaml ---
name: liquidity_target_exit
type: trade_constructor
...
--- END FILE: plugins/entry_planners/liquidity_target_exit/plugin_manifest.yaml ---

--- START FILE: plugins/entry_planners/liquidity_target_exit/schema.py ---
# plugins/trade_constructors/liquidity_target_exit/schema.py
"""
Docstring for schema.py.

@layer: TODO
@dependencies: TODO
@responsibilities: TODO
"""

class LiquidityTargetExitParams:
    """Docstring for LiquidityTargetExitParams."""
    pass

--- END FILE: plugins/entry_planners/liquidity_target_exit/schema.py ---

--- START FILE: plugins/entry_planners/liquidity_target_exit/worker.py ---
# plugins/trade_constructors/liquidity_target_exit/worker.py
"""
Docstring for worker.py.

@layer: TODO
@dependencies: TODO
@responsibilities: TODO
"""

class LiquidityTargetExitPlanner:
    """Docstring for LiquidityTargetExitPlanner."""
    pass

--- END FILE: plugins/entry_planners/liquidity_target_exit/worker.py ---

--- START FILE: plugins/entry_planners/liquidity_target_exit/__init__.py ---

--- END FILE: plugins/entry_planners/liquidity_target_exit/__init__.py ---

--- START FILE: plugins/entry_planners/liquidity_target_exit/tests/test_worker.py ---
# plugins/trade_constructors/liquidity_target_exit/tests/test_worker.py
"""
Docstring for test_worker.py.

@layer: TODO
@dependencies: TODO
@responsibilities: TODO
"""

class TestLiquidityTargetExitPlanner:
    """Docstring for TestLiquidityTargetExitPlanner."""
    pass

--- END FILE: plugins/entry_planners/liquidity_target_exit/tests/test_worker.py ---

--- START FILE: plugins/entry_planners/liquidity_target_exit/tests/__init__.py ---

--- END FILE: plugins/entry_planners/liquidity_target_exit/tests/__init__.py ---

--- START FILE: plugins/order_routers/__init__.py ---

--- END FILE: plugins/order_routers/__init__.py ---

--- START FILE: plugins/regime_filters/__init__.py ---

--- END FILE: plugins/regime_filters/__init__.py ---

--- START FILE: plugins/regime_filters/adx_trend_filter/plugin_manifest.yaml ---
name: adx_trend_filter
type: regime_filter
...
--- END FILE: plugins/regime_filters/adx_trend_filter/plugin_manifest.yaml ---

--- START FILE: plugins/regime_filters/adx_trend_filter/schema.py ---
# plugins/regime_filters/adx_trend_filter/schema.py
"""
Docstring for schema.py.

@layer: TODO
@dependencies: TODO
@responsibilities: TODO
"""

class AdxTrendFilterParams:
    """Docstring for AdxTrendFilterParams."""
    pass

--- END FILE: plugins/regime_filters/adx_trend_filter/schema.py ---

--- START FILE: plugins/regime_filters/adx_trend_filter/worker.py ---
# plugins/regime_filters/adx_trend_filter/worker.py
"""
Docstring for worker.py.

@layer: TODO
@dependencies: TODO
@responsibilities: TODO
"""

class AdxTrendFilter:
    """Docstring for AdxTrendFilter."""
    pass

--- END FILE: plugins/regime_filters/adx_trend_filter/worker.py ---

--- START FILE: plugins/regime_filters/adx_trend_filter/__init__.py ---

--- END FILE: plugins/regime_filters/adx_trend_filter/__init__.py ---

--- START FILE: plugins/regime_filters/adx_trend_filter/tests/test_worker.py ---
# plugins/regime_filters/adx_trend_filter/tests/test_worker.py
"""
Docstring for test_worker.py.

@layer: TODO
@dependencies: TODO
@responsibilities: TODO
"""

class TestAdxTrendFilter:
    """Docstring for TestAdxTrendFilter."""
    pass

--- END FILE: plugins/regime_filters/adx_trend_filter/tests/test_worker.py ---

--- START FILE: plugins/regime_filters/adx_trend_filter/tests/__init__.py ---

--- END FILE: plugins/regime_filters/adx_trend_filter/tests/__init__.py ---

--- START FILE: plugins/signal_generators/__init__.py ---

--- END FILE: plugins/signal_generators/__init__.py ---

--- START FILE: plugins/signal_generators/fvg_entry_detector/plugin_manifest.yaml ---
name: fvg_entry_detector
type: signal_generator
...
--- END FILE: plugins/signal_generators/fvg_entry_detector/plugin_manifest.yaml ---

--- START FILE: plugins/signal_generators/fvg_entry_detector/schema.py ---
# plugins/signal_generators/fvg_entry_detector/schema.py
"""
Docstring for schema.py.

@layer: TODO
@dependencies: TODO
@responsibilities: TODO
"""

class FvgEntryDetectorParams:
    """Docstring for FvgEntryDetectorParams."""
    pass

--- END FILE: plugins/signal_generators/fvg_entry_detector/schema.py ---

--- START FILE: plugins/signal_generators/fvg_entry_detector/worker.py ---
# plugins/signal_generators/fvg_entry_detector/worker.py
"""
Docstring for worker.py.

@layer: TODO
@dependencies: TODO
@responsibilities: TODO
"""

class FvgEntryDetector:
    """Docstring for FvgEntryDetector."""
    pass

--- END FILE: plugins/signal_generators/fvg_entry_detector/worker.py ---

--- START FILE: plugins/signal_generators/fvg_entry_detector/__init__.py ---

--- END FILE: plugins/signal_generators/fvg_entry_detector/__init__.py ---

--- START FILE: plugins/signal_generators/fvg_entry_detector/tests/test_worker.py ---
# plugins/signal_generators/fvg_entry_detector/tests/test_worker.py
"""
Docstring for test_worker.py.

@layer: TODO
@dependencies: TODO
@responsibilities: TODO
"""

class TestFvgEntryDetector:
    """Docstring for TestFvgEntryDetector."""
    pass

--- END FILE: plugins/signal_generators/fvg_entry_detector/tests/test_worker.py ---

--- START FILE: plugins/signal_generators/fvg_entry_detector/tests/__init__.py ---

--- END FILE: plugins/signal_generators/fvg_entry_detector/tests/__init__.py ---

--- START FILE: plugins/signal_refiners/__init__.py ---

--- END FILE: plugins/signal_refiners/__init__.py ---

--- START FILE: plugins/signal_refiners/volume_spike_refiner/plugin_manifest.yaml ---
name: volume_spike_refiner
type: signal_refiner
...
--- END FILE: plugins/signal_refiners/volume_spike_refiner/plugin_manifest.yaml ---

--- START FILE: plugins/signal_refiners/volume_spike_refiner/schema.py ---
# plugins/signal_refiners/volume_spike_refiner/schema.py
"""
Docstring for schema.py.

@layer: TODO
@dependencies: TODO
@responsibilities: TODO
"""

class VolumeSpikeRefinerParams:
    """Docstring for VolumeSpikeRefinerParams."""
    pass

--- END FILE: plugins/signal_refiners/volume_spike_refiner/schema.py ---

--- START FILE: plugins/signal_refiners/volume_spike_refiner/worker.py ---
# plugins/signal_refiners/volume_spike_refiner/worker.py
"""
Docstring for worker.py.

@layer: TODO
@dependencies: TODO
@responsibilities: TODO
"""

class VolumeSpikeRefiner:
    """Docstring for VolumeSpikeRefiner."""
    pass

--- END FILE: plugins/signal_refiners/volume_spike_refiner/worker.py ---

--- START FILE: plugins/signal_refiners/volume_spike_refiner/__init__.py ---

--- END FILE: plugins/signal_refiners/volume_spike_refiner/__init__.py ---

--- START FILE: plugins/signal_refiners/volume_spike_refiner/tests/test_worker.py ---
# plugins/signal_refiners/volume_spike_refiner/tests/test_worker.py
"""
Docstring for test_worker.py.

@layer: TODO
@dependencies: TODO
@responsibilities: TODO
"""

class TestVolumeSpikeRefiner:
    """Docstring for TestVolumeSpikeRefiner."""
    pass

--- END FILE: plugins/signal_refiners/volume_spike_refiner/tests/test_worker.py ---

--- START FILE: plugins/signal_refiners/volume_spike_refiner/tests/__init__.py ---

--- END FILE: plugins/signal_refiners/volume_spike_refiner/tests/__init__.py ---

--- START FILE: plugins/structural_context/__init__.py ---

--- END FILE: plugins/structural_context/__init__.py ---

--- START FILE: plugins/structural_context/ema_detector/manifest.yaml ---
# plugins/structural_context/ema_detector/plugin_manifest.yaml
name: "ema_detector"
version: "1.0.0"
description: "Calculates and adds an Exponential Moving Average (EMA) to the DataFrame."
type: "structural_context" # Geeft aan dat dit een Fase 2 plugin is

# --- Code Contract ---
entry_class: "EmaDetector"          # De naam van de klasse in worker.py
schema_path: "schema.py"            # Het pad naar het Pydantic-schema
params_class: "EmaDetectorParams"   # De naam van de Pydantic-klasse in schema.py

# --- Data Contract ---
dependencies: ["close"]             # Deze plugin heeft de 'close' kolom nodig
provides: []                        # De output kolomnaam is dynamisch, dus hier leeg

--- END FILE: plugins/structural_context/ema_detector/manifest.yaml ---

--- START FILE: plugins/structural_context/ema_detector/schema.py ---
# plugins/structural_context/ema_detector/schema.py
"""
Contains the Pydantic validation schema for the EmaDetector plugin.

@layer: Plugin
@dependencies: [Pydantic]
@responsibilities:
    - Defines and validates the configuration parameters for the EmaDetector.
"""
from pydantic import BaseModel, Field

class EmaDetectorParams(BaseModel):
    """
    Validation schema for the parameters of the EmaDetector.
    It ensures that the 'period' is a positive integer.
    """
    period: int = Field(
        default=20,
        gt=0,
        description="The lookback period for the EMA calculation."
    )

--- END FILE: plugins/structural_context/ema_detector/schema.py ---

--- START FILE: plugins/structural_context/ema_detector/worker.py ---
# plugins/structural_context/ema_detector/worker.py
"""
Contains the main logic for the EmaDetector plugin.

@layer: Plugin
@dependencies: [pandas, backend.core.interfaces]
@responsibilities:
    - Implements the ContextWorker interface.
    - Calculates an EMA based on the provided period.
    - Adds the calculated EMA as a new column to the DataFrame.
"""
import pandas as pd

from backend.core.interfaces import ContextWorker
from backend.dtos.state.trading_context import TradingContext
from backend.utils.app_logger import LogEnricher

from .schema import EmaDetectorParams


class EmaDetector(ContextWorker):
    """
    A context worker that calculates and adds an EMA column to a DataFrame.
    """

    def __init__(self, name: str, params: EmaDetectorParams, logger: LogEnricher):
        """
        Initializes the EmaDetector.

        Args:
            name (str): The unique name of this worker instance.
            params (EmaDetectorParams): A Pydantic object with validated parameters.
            logger (LogEnricher): The pre-configured logger instance.
        """
        self.name = name
        self.params = params
        self.logger = logger
        self.column_name = f"ema_{self.params.period}"

    # pylint: disable=unused-argument, arguments-differ
    def process(self, df: pd.DataFrame, context: TradingContext) -> pd.DataFrame:
        """
        Calculates the EMA and adds it as a new column to the DataFrame.

        Args:
            df (pd.DataFrame): The input DataFrame with OHLCV data.
            context (TradingContext): The trading context (not used by this worker).

        Returns:
            pd.DataFrame: The DataFrame with the new EMA column added.
        """
        if self.column_name in df.columns:
            return df

        self.logger.info(
            "Calculating EMA with period %s...", self.params.period
        )
        df[self.column_name] = df['close'].ewm(
            span=self.params.period,
            adjust=False
        ).mean()

        return df

--- END FILE: plugins/structural_context/ema_detector/worker.py ---

--- START FILE: plugins/structural_context/ema_detector/__init__.py ---

--- END FILE: plugins/structural_context/ema_detector/__init__.py ---

--- START FILE: plugins/structural_context/ema_detector/tests/test_worker.py ---
# plugins/structural_context/ema_detector/tests/test_worker.py
"""
Unit tests for the EmaDetector worker.

@layer: Plugin
@dependencies: [pandas, pytest]
@responsibilities:
    - Tests that the EmaDetector correctly calculates and adds an EMA column.
"""
import pandas as pd
from unittest.mock import MagicMock

# Import de componenten die we testen
from ..worker import EmaDetector
from ..schema import EmaDetectorParams
from backend.dtos.state.trading_context import TradingContext

def test_ema_detector_adds_correct_column():
    """
    Tests if the worker adds a correctly named and calculated EMA column.
    """
    # Arrange
    params = EmaDetectorParams(period=3)
    logger = MagicMock()
    worker = EmaDetector(name="test_ema", params=params, logger=logger)

    # Maak een mock context object aan
    mock_context = MagicMock(spec=TradingContext)

    # Maak test-data
    data = {'close': [10, 20, 30, 40]}
    df = pd.DataFrame(data)

    # Handmatige berekening van de EMA (span=3 -> alpha=0.5)
    # 1: 10
    # 2: (20 * 0.5) + (10 * 0.5) = 15
    # 3: (30 * 0.5) + (15 * 0.5) = 22.5
    # 4: (40 * 0.5) + (22.5 * 0.5) = 31.25
    expected_ema = [10.0, 15.0, 22.5, 31.25]

    # Act
    result_df = worker.process(df, mock_context)

    # Assert
    expected_col = "ema_3"
    assert expected_col in result_df.columns

    # CORRECTIE: 'check_almost_equal' vervangen door 'check_exact=False' en 'atol'
    pd.testing.assert_series_equal(
        result_df[expected_col],
        pd.Series(expected_ema, name=expected_col),
        check_exact=False,
        atol=0.01  # Absolute tolerance voor floating point vergelijkingen
    )

--- END FILE: plugins/structural_context/ema_detector/tests/test_worker.py ---

--- START FILE: plugins/structural_context/ema_detector/tests/__init__.py ---

--- END FILE: plugins/structural_context/ema_detector/tests/__init__.py ---

--- START FILE: results/20250924_213000_mss_fvg_strategy/result_metrics.yaml ---
# Placeholder for results/20250924_213000_mss_fvg_strategy/result_metrics.yaml

--- END FILE: results/20250924_213000_mss_fvg_strategy/result_metrics.yaml ---

--- START FILE: results/20250924_213000_mss_fvg_strategy/run_config.yaml ---
# Placeholder for results/20250924_213000_mss_fvg_strategy/run_config.yaml

--- END FILE: results/20250924_213000_mss_fvg_strategy/run_config.yaml ---

--- START FILE: services/data_command_service.py ---
# In bestand: services/data_command_service.py
"""
Contains the DataCommandService, responsible for orchestrating actions
that create, update, or backfill historical market data.

This service acts as the "Interactor" or "Command" layer for data persistence.

@layer: Service
@dependencies: [IAPIConnector, IDataPersistor, LogEnricher, DataCollectionLimits]
"""
from typing import Optional
import pandas as pd

from backend.core.interfaces.connectors import IAPIConnector
from backend.core.interfaces.persistors import IDataPersistor
from backend.config.schemas.platform_schema import DataCollectionLimits
from backend.dtos.commands.fetch_period_command import FetchPeriodCommand
from backend.dtos.commands.synchronization_command import SynchronizationCommand
# TODO: Import future command DTOs when they are used
# from backend.dtos.commands.extend_history_command import ExtendHistoryCommand
# from backend.dtos.commands.fill_gaps_command import FillGapsCommand
from backend.utils.app_logger import LogEnricher


class DataCommandService:
    """Orchestrates write-actions for the historical data archive."""

    def __init__(
        self,
        persistor: IDataPersistor,
        connector: IAPIConnector,
        limits: DataCollectionLimits,
        logger: LogEnricher
    ):
        """Initializes the DataCommandService.

        Args:
            persistor (IDataPersistor): The data persistence component.
            connector (IAPIConnector): The data source connector.
            limits (DataCollectionLimits): The configured safety limits for
                data collection operations.
            logger (LogEnricher): The application's configured logger instance.
        """
        self._persistor = persistor
        self._connector = connector
        self._limits = limits
        self._logger = logger

    # --- Private "Motor" ---

    def _fetch_and_save_chunk(
        self, pair: str, since: int, until: Optional[int] = None
    ) -> int:
        """Private helper to fetch and save a single chunk of trade data.

        Args:
            pair (str): The trading pair to process.
            since (int): The start timestamp (nanoseconds) for the data fetch.
            until (Optional[int]): The end timestamp (nanoseconds) for the
                data fetch.

        Returns:
            int: The number of new trades that were successfully saved.
        """
        trades_chunk = self._connector.get_historical_trades(
            pair=pair, since=since, until=until
        )
        if not trades_chunk:
            return 0
        self._persistor.save_trades(pair=pair, trades=trades_chunk)
        return len(trades_chunk)

    # --- Public "Interactors" (Command Handlers) ---

    def fetch_period(self, command: FetchPeriodCommand) -> None:
        """
        Fetches a specific, bounded period of historical data, working backward
        in daily chunks.

        It validates the requested period against platform limits before execution.

        Args:
            command (FetchPeriodCommand): A DTO containing the 'pair',
                'start_date', and optional 'end_date'.

        Raises:
            ValueError: If the requested date range exceeds the configured
                maximum limit in `platform.yaml`.
        """
        self._logger.info(
            'data_command.fetch_period_start',
            values={'pair': command.pair}
        )

        # 1. Determine the start and end dates for the entire operation.
        end_day = (command.end_date or pd.Timestamp.utcnow()).normalize()
        start_day = command.start_date.normalize()

        # 2. Validate the request against the platform's safety limits.
        requested_days = (end_day - start_day).days + 1
        if requested_days > self._limits.max_history_days:
            raise ValueError(
                f"Requested history of {requested_days} days exceeds the "
                f"maximum limit of {self._limits.max_history_days} days."
            )

        # 3. Execute the chunking loop, working backward in time.
        current_day = end_day
        while current_day >= start_day:
            day_str = current_day.strftime('%Y-%m-%d')
            self._logger.info(
                'data_command.fetching_chunk',
                values={'pair': command.pair, 'date': day_str}
            )

            chunk_start_ns = int(current_day.value)

            if current_day == pd.Timestamp.utcnow().normalize():
                chunk_end_ns = int(pd.Timestamp.utcnow().value)
            else:
                next_day_start = current_day + pd.Timedelta(days=1)
                chunk_end_ns = int(next_day_start.value - 1)

            saved_count = self._fetch_and_save_chunk(
                pair=command.pair,
                since=chunk_start_ns,
                until=chunk_end_ns
            )

            if saved_count > 0:
                self._logger.info(
                    'data_command.chunk_saved',
                    values={'count': saved_count, 'date': day_str}
                )
            else:
                self._logger.info(
                    'data_command.no_data_for_chunk',
                    values={'date': day_str}
                )

            current_day -= pd.Timedelta(days=1)

        self._logger.info(
            'data_command.fetch_period_complete',
            values={'pair': command.pair}
        )

    def synchronize(self, command: SynchronizationCommand) -> None:
        """Synchronizes the archive with the latest trades."""
        # TODO: Implement full logic for synchronize
        raise NotImplementedError("synchronize is not yet implemented.")

    def extend_history(self, command: object) -> None:
        """Extends the existing historical archive further into the past."""
        raise NotImplementedError("extend_history is not yet implemented.")

    def fill_gaps(self, command: object) -> None:
        """Fills all identified gaps in the historical data."""
        raise NotImplementedError("fill_gaps is not yet implemented.")

--- END FILE: services/data_command_service.py ---

--- START FILE: services/optimization_service.py ---
# services/optimization_service.py
"""
Docstring for optimization_service.py.

@layer: TODO
@dependencies: TODO
@responsibilities: TODO
"""

class OptimizationService:
    """Docstring for OptimizationService."""
    pass

--- END FILE: services/optimization_service.py ---

--- START FILE: services/parallel_run_service.py ---
# services/parallel_run_service.py
"""
Docstring for parallel_run_service.py.

@layer: TODO
@dependencies: TODO
@responsibilities: TODO
"""

class ParallelRunService:
    """Docstring for ParallelRunService."""
    pass

--- END FILE: services/parallel_run_service.py ---

--- START FILE: services/strategy_operator.py ---
# services/strategy_operator.py
"""
Contains the StrategyOperator, the service responsible for orchestrating a
single, complete strategy run.

@layer: Service
@dependencies:
    - backend.config.schemas.app_schema: To receive the complete run configuration.
    - backend.assembly: To build all necessary components (plugins, engine).
    - backend.environments: To create the world the strategy runs in.
    - backend.core: To instantiate the portfolio and the engine itself.
@responsibilities:
    - Acts as the main "conductor" for a single backtest or trading session.
    - Initializes all necessary backend components based on an AppConfig.
    - Runs the main event loop by calling the StrategyEngine.
    - Delegates the results from the engine (Directives and Events) to the
      appropriate handlers.
"""
from typing import Optional
import pandas as pd

from backend.config.schemas.app_schema import AppConfig
from backend.utils.app_logger import LogEnricher

from backend.assembly import PluginRegistry, WorkerBuilder, ContextBuilder
from backend.assembly.engine_builder import EngineBuilder  # Import the new builder
from backend.core import Portfolio, ContextRecorder
from backend.core.interfaces import Tradable #pyright: ignore[reportUnusedImport], pylint: disable=unused-import
from backend.core.interfaces.execution import ExecutionHandler
from backend.environments.backtest_environment import BacktestEnvironment
from backend.dtos.state.trading_context import TradingContext

# Force Pydantic to resolve any forward-looking type references (like 'Tradable')
# This is crucial for models that use abstract base classes or protocols.
TradingContext.model_rebuild()

class StrategyOperator:  # pylint: disable=too-many-instance-attributes
    """Orchestrates a single, complete strategy run from setup to execution."""

    def __init__(self, app_config: AppConfig, logger: LogEnricher):
        """Initializes the StrategyOperator."""
        self._app_config = app_config
        self._logger = logger
        self._context_recorder = ContextRecorder()

        # Attributes to be initialized in _prepare_components
        self._engine_builder: Optional[EngineBuilder] = None
        self._context_builder: Optional[ContextBuilder] = None
        self._portfolio: Optional[Portfolio] = None
        self._environment: Optional[BacktestEnvironment] = None
        self._execution_handler: Optional[ExecutionHandler] = None

    def _prepare_components(self):
        """Phase 1: Prepares all long-lived components for the run."""
        self._logger.info("operator.setup_start")
        platform_conf = self._app_config.platform

        registry = PluginRegistry(platform_config=platform_conf, logger=self._logger)
        worker_builder = WorkerBuilder(plugin_registry=registry, logger=self._logger)

        # The EngineBuilder is now a dedicated specialist for engine assembly
        self._engine_builder = EngineBuilder(worker_builder=worker_builder)
        self._context_builder = ContextBuilder()

        self._portfolio = Portfolio(
            initial_capital=platform_conf.portfolio.initial_capital,
            fees_pct=platform_conf.portfolio.fees_pct,
            logger=self._logger,
            context_recorder=self._context_recorder
        )

        # TODO: MVP HACK - Environment should be injected by a factory.
        self._environment = BacktestEnvironment(app_config=self._app_config,
                                                tradable=self._portfolio)
        self._execution_handler = self._environment.handler

    def _prepare_data(self) -> pd.DataFrame:
        """Phase 2: Builds and runs the ContextBuilder to create enriched data."""
        assert self._engine_builder is not None, "Components not prepared"
        assert self._context_builder is not None, "Components not prepared"
        assert self._environment is not None, "Components not prepared"

        self._logger.info("operator.context_building_start")
        run_conf = self._app_config.run

        context_pipeline = self._engine_builder.build_context_pipeline(run_conf)

        enriched_df = self._context_builder.build(
            initial_df=self._environment.source.get_data(),
            context_pipeline=context_pipeline
        )
        self._logger.info("operator.context_building_complete")
        return enriched_df

    def _run_operational_cycle(self, enriched_df: pd.DataFrame):
        """Phase 3 & 4: Assembles the engine and runs the main event loop."""
        assert self._engine_builder is not None, "Components not prepared"
        assert self._portfolio is not None, "Components not prepared"
        assert self._environment is not None, "Components not prepared"
        assert self._execution_handler is not None, "Components not prepared"

        run_conf = self._app_config.run

        # --- Assemble Engine ---
        self._logger.info("operator.engine_assembly_start")
        engine = self._engine_builder.build_engine(run_conf)
        self._logger.info("operator.engine_assembly_complete")

        # --- Run Main Loop ---
        trading_context = TradingContext(
            enriched_df=enriched_df,
            portfolio=self._portfolio,
            context_recorder=self._context_recorder
        )
        self._logger.info("operator.engine_start")
        for result in engine.run(trading_context=trading_context, clock=self._environment.clock):
            if result.critical_events:
                # TODO: MVP HACK - Proper event handling belongs in a supervisor.
                self._logger.error("operator.critical_events_detected",
                                   values={'count': len(result.critical_events)})
                break

            if result.execution_directives:
                # TODO: ARCHITECTURE REFACTOR - Directive handling belongs in the caller.
                self._execution_handler.execute_plan(result.execution_directives)

    def run(self):
        """
        Executes the full workflow by composing the private helper methods.
        This method now only describes WHAT happens, not HOW.
        """
        self._logger.info("operator.run_start")

        self._prepare_components()
        enriched_df = self._prepare_data()
        self._run_operational_cycle(enriched_df)

        self._logger.info("operator.run_complete")

--- END FILE: services/strategy_operator.py ---

--- START FILE: services/variant_test_service.py ---
# services/variant_test_service.py
"""
Docstring for variant_test_service.py.

@layer: TODO
@dependencies: TODO
@responsibilities: TODO
"""

class VariantTestService:
    """Docstring for VariantTestService."""
    pass

--- END FILE: services/variant_test_service.py ---

--- START FILE: services/__init__.py ---
# services/__init__.py
"""
Exposes the public API of the Services package.
"""
__all__ = [
    # Top-level services
    "StrategyOperator",
    "OptimizationService",
    "ParallelRunService",
    "VariantTestService",
    # from .api_services
    "PluginQueryService",
    "VisualizationService",
]

# This assumes a file named strategy_operator.py exists with class StrategyOperator
from .strategy_operator import StrategyOperator
from .optimization_service import OptimizationService
from .parallel_run_service import ParallelRunService
from .variant_test_service import VariantTestService
from .api_services import (
    PluginQueryService,
    VisualizationService,
)

--- END FILE: services/__init__.py ---

--- START FILE: services/api_services/plugin_query_service.py ---
# services/api_services/plugin_query_service.py
"""
Docstring for plugin_query_service.py.

@layer: TODO
@dependencies: TODO
@responsibilities: TODO
"""

class PluginQueryService:
    """Docstring for PluginQueryService."""
    pass

--- END FILE: services/api_services/plugin_query_service.py ---

--- START FILE: services/api_services/visualization_service.py ---
# services/api_services/visualization_service.py
"""
Docstring for visualization_service.py.

@layer: TODO
@dependencies: TODO
@responsibilities: TODO
"""

class VisualizationService:
    """Docstring for VisualizationService."""
    pass

--- END FILE: services/api_services/visualization_service.py ---

--- START FILE: services/api_services/__init__.py ---
# services/api_services/__init__.py
"""
Exposes the public API of the API Services sub-package.
"""
__all__ = [
    "PluginQueryService",
    "VisualizationService",
]

from .plugin_query_service import PluginQueryService
from .visualization_service import VisualizationService

--- END FILE: services/api_services/__init__.py ---

--- START FILE: tests/__init__.py ---

--- END FILE: tests/__init__.py ---

--- START FILE: tests/backend/__init__.py ---

--- END FILE: tests/backend/__init__.py ---

--- START FILE: tests/backend/assembly/test_context_builder.py ---
# tests/backend/assembly/test_context_builder.py
"""Unit tests for the ContextBuilder."""

import pandas as pd
import pytest
from pytest_mock import MockerFixture

from backend.assembly.context_builder import ContextBuilder

# --- Test Setup: Maak een paar "nep"-workers ---

class MockWorkerAddColumn:
    """Een nep-worker die simpelweg een kolom toevoegt."""
    def __init__(self, new_col_name: str, value: int):
        self.new_col_name = new_col_name
        self.value = value

    def process(self, df: pd.DataFrame) -> pd.DataFrame:
        df[self.new_col_name] = self.value
        return df

class MockWorkerModifyColumn:
    """Een nep-worker die een bestaande kolom aanpast."""
    def __init__(self, target_col: str, multiplier: int):
        self.target_col = target_col
        self.multiplier = multiplier

    def process(self, df: pd.DataFrame) -> pd.DataFrame:
        df[self.target_col] = df[self.target_col] * self.multiplier
        return df

# --- De Tests ---

def test_context_builder_runs_pipeline_sequentially():
    """
    Tests of de ContextBuilder de workers in de juiste, sequentiële
    volgorde uitvoert.
    """
    # Arrange (De Voorbereiding)
    # 1. Maak een simpele start-DataFrame.
    initial_df = pd.DataFrame({'close': [10, 20, 30]})

    # 2. Maak instanties van onze nep-workers.
    worker1 = MockWorkerAddColumn(new_col_name="EMA_50", value=15)
    worker2 = MockWorkerModifyColumn(target_col="EMA_50", multiplier=2)

    # 3. Stel de ContextBuilder in met deze workers.
    context_builder = ContextBuilder()
    pipeline = [worker1, worker2]

    # Act (De Actie)
    # Voer de pijplijn uit.
    result_df = context_builder.build(initial_df, pipeline)

    # Assert (De Controle)
    # We controleren of het eindresultaat correct is.
    # Worker 1 zou de kolom "EMA_50" met waarde 15 moeten toevoegen.
    # Worker 2 zou die kolom moeten vermenigvuldigen met 2, dus de eindwaarde moet 30 zijn.
    assert "EMA_50" in result_df.columns
    assert result_df["EMA_50"].tolist() == [30, 30, 30]

def test_context_builder_returns_copy_of_dataframe():
    """
    Tests of de ContextBuilder een kopie van de DataFrame retourneert en
    de originele niet aanpast. Dit is belangrijk om onverwachte bijeffecten
    in de rest van de applicatie te voorkomen.
    """
    # Arrange
    initial_df = pd.DataFrame({'close': [10]})
    initial_df_copy = initial_df.copy() # Maak een kopie voor de controle achteraf.

    worker = MockWorkerAddColumn(new_col_name="EMA_50", value=15)
    context_builder = ContextBuilder()

    # Act
    result_df = context_builder.build(initial_df, [worker])

    # Assert
    # Controleer of de geretourneerde DataFrame de nieuwe kolom heeft.
    assert "EMA_50" in result_df.columns
    # Controleer of de *originele* DataFrame ongewijzigd is.
    assert "EMA_50" not in initial_df.columns
    # Vergelijk de originele DataFrame met zijn kopie om zeker te zijn.
    pd.testing.assert_frame_equal(initial_df, initial_df_copy)

--- END FILE: tests/backend/assembly/test_context_builder.py ---

--- START FILE: tests/backend/assembly/test_dependency_validator.py ---
# In bestand: tests/backend/assembly/test_dependency_validator.py

from typing import List, Optional
from pathlib import Path
import pytest
from pytest_mock import MockerFixture

from backend.assembly.dependency_validator import DependencyValidator
from backend.assembly.plugin_registry import PluginRegistry
from backend.config.schemas.plugin_manifest_schema import PluginManifest, CoreIdentity, PluginIdentification, Dependencies, Permissions

def create_mock_manifest(
    name: str,
    dependencies: Optional[List[str]] = None,
    provides: Optional[List[str]] = None
) -> PluginManifest:
    """Creates a mock PluginManifest with the correct nested structure."""
    return PluginManifest(
        core_identity=CoreIdentity(apiVersion="s1mpletrader.io/v1", kind="PluginManifest"),
        identification=PluginIdentification(
            name=name,
            display_name=f"{name.replace('_', ' ').title()}",
            type="structural_context",
            version="1.0.0",
            description="A test plugin.",
            author="Test Author"
        ),
        dependencies=Dependencies(requires=dependencies or [], provides=provides or []),
        permissions=Permissions(network_access=[], filesystem_access=[])
    )

def test_valid_pipeline_succeeds(mocker: MockerFixture):
    """Tests that a logically correct pipeline validates successfully."""
    # Arrange
    mock_registry = mocker.MagicMock(spec=PluginRegistry)
    
    # Gebruik de bijgewerkte create_mock_manifest functie
    mock_registry.get_plugin_data.side_effect = {
        "ema_50": (create_mock_manifest("ema_50", ["close"], ["EMA_50"]), Path("path1")),
        "rsi_14": (create_mock_manifest("rsi_14", ["close"], ["RSI_14"]), Path("path2")),
        "logic": (create_mock_manifest("logic", ["EMA_50", "RSI_14"]), Path("path3"))
    }.get

    validator = DependencyValidator(mock_registry)
    pipeline = ["ema_50", "rsi_14", "logic"]

    # Act & Assert
    assert validator.validate(pipeline) is True

def test_pipeline_with_missing_dependency_fails(mocker: MockerFixture):
    """Tests that a pipeline fails if a dependency is never provided."""
    # Arrange
    mock_registry = mocker.MagicMock(spec=PluginRegistry)
    mock_registry.get_plugin_data.side_effect = {
        "ema_50": (create_mock_manifest("ema_50", ["close"], ["EMA_50"]), Path("path1")),
        "logic": (create_mock_manifest("logic", ["COLUMN_THAT_DOES_NOT_EXIST"]), Path("path2"))
    }.get

    validator = DependencyValidator(mock_registry)
    pipeline = ["ema_50", "logic"]

    # Act & Assert
    error_msg = "Dependency 'COLUMN_THAT_DOES_NOT_EXIST' for plugin 'logic' not met."
    with pytest.raises(ValueError, match=error_msg):
        validator.validate(pipeline)

def test_pipeline_with_incorrect_order_fails(mocker: MockerFixture):
    """Tests that a pipeline fails if dependencies are not met due to wrong order."""
    # Arrange
    mock_registry = mocker.MagicMock(spec=PluginRegistry)
    mock_registry.get_plugin_data.side_effect = {
        "logic": (create_mock_manifest("logic", ["EMA_50"]), Path("path1")),
        "ema_50": (create_mock_manifest("ema_50", ["close"], ["EMA_50"]), Path("path2")),
    }.get

    validator = DependencyValidator(mock_registry)
    pipeline = ["logic", "ema_50"]

    # Act & Assert
    with pytest.raises(ValueError, match="Dependency 'EMA_50' for plugin 'logic' not met."):
        validator.validate(pipeline)

--- END FILE: tests/backend/assembly/test_dependency_validator.py ---

--- START FILE: tests/backend/assembly/test_engine_builder.py ---
# tests/backend/assembly/test_engine_builder.py
"""
Unit tests for the EngineBuilder class.
"""
from unittest.mock import call
from pytest_mock import MockerFixture

from backend.assembly.engine_builder import EngineBuilder
from backend.assembly.worker_builder import WorkerBuilder
from backend.config.schemas.run_schema import RunBlueprint, WorkerDefinition
from backend.core.enums import PipelinePhase

def test_build_context_pipeline(mocker: MockerFixture):
    """
    Tests if the context pipeline is built correctly by only requesting
    workers from the STRUCTURAL_CONTEXT phase.
    """
    # --- Arrange ---
    mock_worker_builder = mocker.MagicMock(spec=WorkerBuilder)
    mock_run_conf = mocker.MagicMock(spec=RunBlueprint)

    # --- DE FIX: Maak de geneste mock-structuur voor de taskboard ---
    mock_taskboard = mocker.MagicMock()
    mock_taskboard.root = {
        PipelinePhase.STRUCTURAL_CONTEXT: ["worker_a", "worker_b"],
        PipelinePhase.SIGNAL_GENERATOR: ["worker_c"]  # Should be ignored
    }
    mock_run_conf.taskboard = mock_taskboard
    # ----------------------------------------------------------------

    # --- DE FIX: Mock ook de workforce met een get methode ---
    mock_workforce = mocker.MagicMock()
    mock_workforce.get.return_value = WorkerDefinition()
    mock_run_conf.workforce = mock_workforce
    # ---------------------------------------------------------

    # --- Act ---
    builder = EngineBuilder(worker_builder=mock_worker_builder)
    builder.build_context_pipeline(run_conf=mock_run_conf)

    # --- Assert ---
    # Verify that 'build' was called only for the context workers
    expected_calls = [
        call(name="worker_a", user_params={}),
        call(name="worker_b", user_params={})
    ]
    mock_worker_builder.build.assert_has_calls(expected_calls, any_order=True)
    assert mock_worker_builder.build.call_count == 2

def test_build_engine(mocker: MockerFixture):
    """
    Tests if the StrategyEngine is assembled correctly with workers from all
    phases EXCEPT the STRUCTURAL_CONTEXT phase.
    """
    # --- Arrange ---
    mock_worker_builder = mocker.MagicMock(spec=WorkerBuilder)
    mock_strategy_engine_class = mocker.patch('backend.assembly.engine_builder.StrategyEngine')
    mock_run_conf = mocker.MagicMock(spec=RunBlueprint)

    # --- DE FIX: Maak de geneste mock-structuur voor de taskboard ---
    mock_taskboard = mocker.MagicMock()
    mock_taskboard.root = {
        PipelinePhase.STRUCTURAL_CONTEXT: ["context_worker"],  # Should be ignored
        PipelinePhase.SIGNAL_GENERATOR: ["signal_worker_1"],
        PipelinePhase.SIGNAL_REFINER: ["refiner_worker_1", "refiner_worker_2"]
    }
    mock_run_conf.taskboard = mock_taskboard
    # ----------------------------------------------------------------

    # --- DE FIX: Mock ook de workforce met een get methode ---
    mock_workforce = mocker.MagicMock()
    mock_workforce.get.return_value = WorkerDefinition()
    mock_run_conf.workforce = mock_workforce
    # ---------------------------------------------------------

    # --- Act ---
    builder = EngineBuilder(worker_builder=mock_worker_builder)
    builder.build_engine(run_conf=mock_run_conf)

    # --- Assert ---
    # Verify the worker_builder was called for the correct, non-context workers
    expected_build_calls = [
        call(name="signal_worker_1", user_params={}),
        call(name="refiner_worker_1", user_params={}),
        call(name="refiner_worker_2", user_params={})
    ]
    mock_worker_builder.build.assert_has_calls(expected_build_calls, any_order=True)
    assert mock_worker_builder.build.call_count == 3

    # Verify the StrategyEngine was instantiated with the correctly grouped workers
    active_workers_arg = mock_strategy_engine_class.call_args[1]['active_workers']

    assert PipelinePhase.SIGNAL_GENERATOR.value in active_workers_arg
    assert len(active_workers_arg[PipelinePhase.SIGNAL_GENERATOR.value]) == 1

    assert PipelinePhase.SIGNAL_REFINER.value in active_workers_arg
    assert len(active_workers_arg[PipelinePhase.SIGNAL_REFINER.value]) == 2

    # Crucially, assert that the context phase was NOT included in the engine's workers
    assert PipelinePhase.STRUCTURAL_CONTEXT.value not in active_workers_arg

--- END FILE: tests/backend/assembly/test_engine_builder.py ---

--- START FILE: tests/backend/assembly/test_plugin_creator.py ---
# tests/backend/assembly/test_plugin_creator.py
"""
Unit tests for the PluginCreator service.

@layer: Tests (Backend/Assembly)
@dependencies: [pytest, unittest.mock, backend.assembly.plugin_creator]
@responsibilities:
    - Verify that the plugin skeleton is generated correctly.
    - Ensure all required files are created in the specified location.
    - Confirm that the creator handles file system operations gracefully.
"""

from pathlib import Path
from unittest.mock import MagicMock

from backend.assembly.plugin_creator import PluginCreator

def test_plugin_creator_generates_correct_skeleton(tmp_path: Path):
    """Tests if the create method successfully generates all required files.

    This test uses a mock logger to isolate the PluginCreator from the actual
    logging infrastructure and verifies that appropriate log messages are called.
    
    Args:
        tmp_path (Path): A temporary directory path provided by the pytest fixture.
    """
    # Arrange
    plugins_root = tmp_path
    plugin_name = "my_test_plugin"
    plugin_type = "signal_generator"
    base_path = plugins_root / plugin_type / plugin_name

    expected_files = [
        base_path / "manifest.yaml",
        base_path / "schema.py",
        base_path / "worker.py",
        base_path / "context_schema.py",
        base_path / "tests/test_worker.py",
    ]

    # Maak een mock object aan voor de LogEnricher
    mock_logger = MagicMock()

    # Injecteer de mock logger in de PluginCreator
    creator = PluginCreator(plugins_root=plugins_root, logger=mock_logger)

    # Act
    success = creator.create(name=plugin_name, plugin_type=plugin_type)

    # Assert
    assert success is True
    assert (base_path / "tests").is_dir()
    for file_path in expected_files:
        assert file_path.is_file(), f"File not found: {file_path}"

    # Verifieer dat de logger correct is gebruikt
    mock_logger.info.assert_any_call(f"Creating plugin '{plugin_name}' at: {base_path}")
    mock_logger.info.assert_any_call(f"Successfully created plugin '{plugin_name}'.")
    mock_logger.error.assert_not_called()

--- END FILE: tests/backend/assembly/test_plugin_creator.py ---

--- START FILE: tests/backend/assembly/test_plugin_registry.py ---
# In bestand: tests/backend/assembly/test_plugin_registry.py

import yaml
from pathlib import Path
from unittest.mock import MagicMock

from backend.assembly.plugin_registry import PluginRegistry
from backend.config.schemas.platform_schema import PlatformConfig

def test_plugin_registry_discovers_and_validates_plugins(tmp_path: Path):
    """
    Tests if the registry correctly finds valid plugins and ignores invalid ones.
    """
    # Arrange
    plugins_root = tmp_path / "plugins"
    
    # Een valide plugin (nu met de correcte, geneste YAML-structuur)
    valid_plugin_path = plugins_root / "structural_context" / "valid_plugin"
    valid_plugin_path.mkdir(parents=True)
    valid_manifest = {
        "core_identity": {
            "apiVersion": "s1mpletrader.io/v1",
            "kind": "PluginManifest"
        },
        "identification": {
            "name": "valid_plugin",
            "display_name": "Valid Plugin",
            "type": "structural_context",
            "version": "1.0.0",
            "description": "A valid test plugin.",
            "author": "Test Author"
        },
        "dependencies": {"requires": [], "provides": []},
        "permissions": {"network_access": [], "filesystem_access": []}
    }
    with open(valid_plugin_path / "plugin_manifest.yaml", "w") as f:
        yaml.dump(valid_manifest, f)

    # Een plugin met een ongeldig manifest (mist 'identification')
    invalid_plugin_path = plugins_root / "structural_context" / "invalid_plugin"
    invalid_plugin_path.mkdir(parents=True)
    invalid_manifest = {
        "core_identity": {
            "apiVersion": "s1mpletrader.io/v1",
            "kind": "PluginManifest"
        }
    }
    with open(invalid_plugin_path / "plugin_manifest.yaml", "w") as f:
        yaml.dump(invalid_manifest, f)
        
    mock_config = MagicMock(spec=PlatformConfig)
    mock_config.plugins_root_path = str(plugins_root)
    mock_logger = MagicMock()

    # Act
    registry = PluginRegistry(platform_config=mock_config, logger=mock_logger)

    # Assert
    all_manifests = registry.get_all_manifests()
    assert len(all_manifests) == 1, "Should only register the one valid plugin."
    assert "valid_plugin" in all_manifests, "The valid plugin should be registered."
    assert "invalid_plugin" not in all_manifests, "The invalid plugin should be ignored."
    assert mock_logger.warning.call_count > 0, "A warning should be logged for the invalid manifest."

--- END FILE: tests/backend/assembly/test_plugin_registry.py ---

--- START FILE: tests/backend/assembly/test_worker_builder.py ---
# tests/backend/assembly/test_worker_builder.py
"""Unit tests for the WorkerBuilder."""

from pathlib import Path
from unittest.mock import MagicMock

from pydantic import BaseModel, ValidationError
from pytest_mock import MockerFixture

from backend.assembly.worker_builder import WorkerBuilder
from backend.assembly.plugin_registry import PluginRegistry

# --- Test Setup: Maak een nep-omgeving ---

class MockParams(BaseModel):
    """Een nep Pydantic-schema voor een plugin."""
    value: int

class MockWorker:
    """Een nep worker-klasse."""
    def __init__(self, name: str, params: MockParams, logger):
        self.name = name
        self.params = params
        self.logger = logger

def test_worker_builder_successfully_builds_worker(mocker: MockerFixture):
    """
    Tests the happy path: building a valid worker with correct parameters.
    """
    # Arrange
    mock_registry = mocker.MagicMock(spec=PluginRegistry)
    mock_logger = mocker.MagicMock()
    
    mock_manifest = mocker.MagicMock(
        params_class="MockParams", entry_class="MockWorker",
        schema_path="schema.py"
    )
    # CORRECTIE: Geef een Path-object terug, geen string.
    mock_registry.get_plugin_data.return_value = (mock_manifest, Path("dummy/path"))
    
    mocker.patch(
        "backend.assembly.worker_builder.load_class_from_module",
        side_effect=[MockParams, MockWorker]
    )
    
    builder = WorkerBuilder(mock_registry, mock_logger)
    
    # Act
    worker_instance = builder.build(name="my_worker", user_params={"value": 123})
    
    # Assert
    assert isinstance(worker_instance, MockWorker)
    assert worker_instance.name == "my_worker"
    assert worker_instance.params.value == 123
    mock_logger.info.assert_called_with("Successfully built worker 'my_worker'.")

def test_worker_builder_fails_on_invalid_params(mocker: MockerFixture):
    """
    Tests if the builder correctly fails when user parameters are invalid.
    """
    # Arrange
    mock_registry = mocker.MagicMock(spec=PluginRegistry)
    mock_logger = mocker.MagicMock()
    
    mock_manifest = mocker.MagicMock(
        params_class="MockParams", entry_class="MockWorker",
        schema_path="schema.py"
    )
    # CORRECTIE: Geef een Path-object terug, geen string.
    mock_registry.get_plugin_data.return_value = (mock_manifest, Path("dummy/path"))

    mocker.patch(
        "backend.assembly.worker_builder.load_class_from_module",
        side_effect=[MockParams, MockWorker]
    )
    
    builder = WorkerBuilder(mock_registry, mock_logger)
    
    # Act
    worker_instance = builder.build(name="my_worker", user_params={"value": "not-an-int"})

    # Assert
    assert worker_instance is None
    mock_logger.error.assert_called_once()
    assert "Invalid parameters for worker 'my_worker'" in mock_logger.error.call_args[0][0]

--- END FILE: tests/backend/assembly/test_worker_builder.py ---

--- START FILE: tests/backend/assembly/__init__.py ---

--- END FILE: tests/backend/assembly/__init__.py ---

--- START FILE: tests/backend/core/test_context_recoreder.py ---
# tests/backend/core/test_context_recorder.py
"""Unit tests for the ContextRecorder."""

import uuid
from datetime import datetime
import pandas as pd
from pydantic import BaseModel

from backend.core.context_recorder import ContextRecorder

# --- Test Setup ---

class MockContextObject(BaseModel):
    """Een simpele Pydantic-klasse om een context-object te simuleren."""
    value: int
    label: str

# --- De Test ---

def test_context_recorder_adds_and_serializes_data():
    """
    Tests if the ContextRecorder correctly adds data, serializes the Pydantic
    model, and structures the log correctly.
    """
    # Arrange (De Voorbereiding)
    recorder = ContextRecorder()
    
    test_timestamp = pd.to_datetime("2023-01-01 10:00:00", utc=True)
    test_specialist = "my_test_plugin"
    test_context_obj = MockContextObject(value=123, label="test")
    test_corr_id = uuid.uuid4()

    # Act (De Actie)
    # Voeg de data toe aan de recorder. Dit is de methode die we testen.
    recorder.add_data(
        correlation_id=test_corr_id,
        timestamp=test_timestamp,
        specialist_name=test_specialist,
        context_object=test_context_obj
    )

    # Assert (De Controle)
    # 1. Haal alle data op uit de recorder.
    all_data = recorder.get_all_data()

    # 2. Controleer de structuur.
    assert test_timestamp in all_data, "Timestamp should be the primary key."
    assert test_specialist in all_data[test_timestamp], "Specialist name should be the secondary key."

    # 3. Controleer de inhoud.
    logged_context = all_data[test_timestamp][test_specialist]
    
    # Is het Pydantic-object correct omgezet naar een dictionary?
    assert isinstance(logged_context, dict)
    assert logged_context['value'] == 123
    assert logged_context['label'] == "test"
    
    # Is de correlation_id correct toegevoegd?
    assert 'correlation_id' in logged_context
    assert logged_context['correlation_id'] == str(test_corr_id)

--- END FILE: tests/backend/core/test_context_recoreder.py ---

--- START FILE: tests/backend/core/test_directive_flattener.py ---
# tests/backend/core/test_directive_flattener.py
"""
Unit tests for the DirectiveFlattener utility.
"""

import uuid
import pandas as pd
from backend.dtos.pipeline.signal import Signal
from backend.dtos.pipeline.entry_signal import EntrySignal
from backend.dtos.pipeline.risk_defined_signal import RiskDefinedSignal
from backend.dtos.pipeline.trade_plan import TradePlan
from backend.dtos.pipeline.routed_trade_plan import RoutedTradePlan
from backend.dtos.execution.execution_directive import ExecutionDirective
from backend.core.directive_flattener import DirectiveFlattener

def test_flatten_routed_trade_plan_to_directive():
    """
    Tests if a deeply nested RoutedTradePlan is correctly flattened into a
    simple ExecutionDirective.
    """
    # --- Arrange (De Voorbereiding) ---
    test_corr_id = uuid.uuid4()
    test_timestamp = pd.Timestamp("2023-01-01 10:00:00", tz='UTC')

    signal = Signal(correlation_id=test_corr_id, timestamp=test_timestamp, asset="BTC/EUR", direction="long", signal_type="fvg_entry_signal")
    entry_signal = EntrySignal(correlation_id=test_corr_id, signal=signal, entry_price=25000.0)
    risk_defined_signal = RiskDefinedSignal(correlation_id=test_corr_id, entry_signal=entry_signal, sl_price=24800.0, tp_price=25500.0)
    trade_plan = TradePlan(correlation_id=test_corr_id, risk_defined_signal=risk_defined_signal, position_value_quote=1000.0, position_size_asset=0.04)
    routed_trade_plan = RoutedTradePlan(correlation_id=test_corr_id, trade_plan=trade_plan, order_type='limit', limit_price=25000.0, time_in_force='GTC', post_only=True)
    
    expected_directive = ExecutionDirective(
        correlation_id=test_corr_id,
        signal_type="fvg_entry_signal",
        asset="BTC/EUR",
        direction="long",
        entry_price=25000.0,
        sl_price=24800.0,
        tp_price=25500.0,
        position_value_quote=1000.0,
        position_size_asset=0.04,
        order_type='limit',
        limit_price=25000.0,
        time_in_force='GTC',
        post_only=True,
        entry_time=test_timestamp
    )

    # --- Act (De Actie) ---
    flattener = DirectiveFlattener()
    actual_directive = flattener.flatten(routed_trade_plan)

    # --- Assert (De Controle) ---
    assert actual_directive == expected_directive

--- END FILE: tests/backend/core/test_directive_flattener.py ---

--- START FILE: tests/backend/core/test_portfolio.py ---
# tests/backend/core/test_portfolio.py
import pytest
import uuid
import pandas as pd

from backend.core.interfaces.portfolio import Tradable
from backend.core.portfolio import Portfolio
from backend.dtos.execution.execution_directive import ExecutionDirective

class MockLogger:
    def info(self, *args, **kwargs): pass
    def trade(self, *args, **kwargs): pass
    def error(self, *args, **kwargs): pass

class MockContextRecorder:
    def add_data(self, *args, **kwargs): pass

@pytest.fixture
def empty_portfolio() -> Portfolio:
    return Portfolio(
        initial_capital=10000.0, fees_pct=0.001,
        logger=MockLogger(), context_recorder=MockContextRecorder()
    )

@pytest.fixture
def sample_directive() -> ExecutionDirective:
    """A fixture for a sample long execution directive."""
    return ExecutionDirective(
        correlation_id=uuid.uuid4(),
        signal_type='TEST_SIGNAL_LONG',
        entry_time=pd.to_datetime("2023-01-01 10:00:00"),
        asset="BTC/EUR",
        direction='long',
        entry_price=20000.0,
        sl_price=19800.0,
        tp_price=20400.0,
        position_value_quote=1000.0,
        position_size_asset=0.05,
        order_type='limit'  # Toegevoegd verplicht veld
    )

@pytest.fixture
def sample_short_directive() -> ExecutionDirective:
    """A fixture for a sample short execution directive."""
    return ExecutionDirective(
        correlation_id=uuid.uuid4(),
        signal_type='TEST_SIGNAL_SHORT',
        entry_time=pd.to_datetime("2023-01-02 10:00:00"),
        asset="ETH/EUR",
        direction='short',
        entry_price=1500.0,
        sl_price=1520.0,
        tp_price=1460.0,
        position_value_quote=1500.0,
        position_size_asset=1.0,
        order_type='market' # Toegevoegd verplicht veld
    )

# --- Test Cases ---

def test_portfolio_fulfills_tradable_contract(empty_portfolio: Portfolio):
    assert isinstance(empty_portfolio, Tradable)

def test_portfolio_initialization(empty_portfolio: Portfolio):
    assert empty_portfolio.balance == 10000.0
    assert empty_portfolio.initial_capital == 10000.0
    assert not empty_portfolio.active_trades
    assert len(empty_portfolio.closed_trades) == 0

def test_open_trade_success(empty_portfolio: Portfolio, sample_directive: ExecutionDirective):
    empty_portfolio.open_trade(sample_directive)
    assert len(empty_portfolio.active_trades) == 1
    active_trade_data = empty_portfolio.active_trades[sample_directive.correlation_id]
    assert active_trade_data['direction'] == 'long'
    assert active_trade_data['entry_price'] == 20000.0
    assert active_trade_data['position_size_asset'] == 0.05

def test_open_multiple_trades_on_different_assets(empty_portfolio: Portfolio, sample_directive: ExecutionDirective):
    eth_directive = sample_directive.model_copy(update={
        'correlation_id': uuid.uuid4(),
        'signal_type': 'TEST_SIGNAL_ETH_LONG',
        'entry_time': pd.to_datetime("2023-01-01 11:00:00"),
        'asset': "ETH/EUR",
        'entry_price': 1500.0,
        'sl_price': 1480.0,
        'tp_price': 1550.0,
        'position_value_quote': 500.0,
        'position_size_asset': 0.33
    })
    empty_portfolio.open_trade(sample_directive)
    empty_portfolio.open_trade(eth_directive)
    
    active_trades_dict = empty_portfolio.active_trades
    assert len(active_trades_dict) == 2
    assert sample_directive.correlation_id in active_trades_dict
    assert eth_directive.correlation_id in active_trades_dict

def test_process_candle_closes_long_trade_on_stop_loss(empty_portfolio: Portfolio, sample_directive: ExecutionDirective):
    empty_portfolio.open_trade(sample_directive)
    assert empty_portfolio.active_trade_count == 1

    candle_timestamp = pd.to_datetime("2023-01-01 11:00:00")
    killer_candle = pd.Series({
        "open": 19900.0, "high": 19950.0,
        "low": 19800.0, "close": 19850.0
    }, name=candle_timestamp)

    empty_portfolio.process_candle(killer_candle)

    assert empty_portfolio.active_trade_count == 0
    assert len(empty_portfolio.closed_trades) == 1

    closed_trade = empty_portfolio.closed_trades[0]
    assert closed_trade.exit_price == 19800.0
    assert closed_trade.pnl_quote < 0

def test_process_candle_closes_long_trade_on_take_profit(empty_portfolio: Portfolio, sample_directive: ExecutionDirective):
    empty_portfolio.open_trade(sample_directive)
    assert empty_portfolio.active_trade_count == 1

    profit_candle_timestamp = pd.to_datetime("2023-01-01 12:00:00")
    profit_candle = pd.Series({
        "open": 20200.0, "high": 20400.0,
        "low": 20150.0, "close": 20350.0
    }, name=profit_candle_timestamp)

    empty_portfolio.process_candle(profit_candle)

    assert empty_portfolio.active_trade_count == 0
    assert len(empty_portfolio.closed_trades) == 1

    closed_trade = empty_portfolio.closed_trades[0]
    assert closed_trade.exit_price == 20400.0
    assert closed_trade.pnl_quote > 0

def test_process_candle_closes_short_trade_on_stop_loss(empty_portfolio: Portfolio, sample_short_directive: ExecutionDirective):
    empty_portfolio.open_trade(sample_short_directive)
    assert empty_portfolio.active_trade_count == 1

    killer_candle_timestamp = pd.to_datetime("2023-01-02 11:00:00")
    killer_candle = pd.Series({
        "open": 1510.0, "high": 1520.0,
        "low": 1505.0, "close": 1515.0
    }, name=killer_candle_timestamp)

    empty_portfolio.process_candle(killer_candle)

    assert empty_portfolio.active_trade_count == 0
    assert len(empty_portfolio.closed_trades) == 1

    closed_trade = empty_portfolio.closed_trades[0]
    assert closed_trade.exit_price == 1520.0
    assert closed_trade.pnl_quote < 0

def test_process_candle_closes_short_trade_on_take_profit(empty_portfolio: Portfolio, sample_short_directive: ExecutionDirective):
    empty_portfolio.open_trade(sample_short_directive)
    assert empty_portfolio.active_trade_count == 1

    profit_candle_timestamp = pd.to_datetime("2023-01-02 12:00:00")
    profit_candle = pd.Series({
        "open": 1470.0, "high": 1475.0,
        "low": 1460.0, "close": 1465.0
    }, name=profit_candle_timestamp)

    empty_portfolio.process_candle(profit_candle)

    assert empty_portfolio.active_trade_count == 0
    assert len(empty_portfolio.closed_trades) == 1

    closed_trade = empty_portfolio.closed_trades[0]
    assert closed_trade.exit_price == 1460.0
    assert closed_trade.pnl_quote > 0

--- END FILE: tests/backend/core/test_portfolio.py ---

--- START FILE: tests/backend/core/test_strategy_engine.py ---
# tests/backend/core/test_strategy_engine.py
"""Unit tests for the StrategyEngine."""

import uuid
import pandas as pd
from pytest_mock import MockerFixture

from backend.dtos.pipeline.signal import Signal
from backend.dtos.pipeline.entry_signal import EntrySignal
from backend.dtos.pipeline.risk_defined_signal import RiskDefinedSignal
from backend.dtos.pipeline.trade_plan import TradePlan
from backend.dtos.pipeline.routed_trade_plan import RoutedTradePlan
from backend.dtos.state.trading_context import TradingContext
from backend.dtos.results.engine_cycle_result import EngineCycleResult
from backend.dtos.execution.execution_directive import ExecutionDirective
from backend.dtos.execution.critical_event import CriticalEvent
from backend.core.interfaces import Clock
from backend.core.strategy_engine import StrategyEngine
from backend.core.directive_flattener import DirectiveFlattener

def test_strategy_engine_yields_correct_result(mocker: MockerFixture):
    """
    Tests that the StrategyEngine correctly processes the full 9-phase pipeline
    and yields a final, complete EngineCycleResult.
    """
    # --- Arrange (De Voorbereiding) ---
    mock_clock = mocker.MagicMock(spec=Clock)
    test_time = pd.Timestamp("2023-01-01 10:00:00", tz='UTC')
    mock_clock.tick.return_value = [(test_time, pd.Series())]
    corr_id = uuid.uuid4()

    # 1. Bouw de volledige, geneste DTO-keten op
    signal_dto = Signal(correlation_id=corr_id, timestamp=test_time, asset="BTC/EUR", direction="long", signal_type="test_signal")
    entry_signal_dto = EntrySignal(correlation_id=corr_id, signal=signal_dto, entry_price=100.0)
    risk_defined_dto = RiskDefinedSignal(correlation_id=corr_id, entry_signal=entry_signal_dto, sl_price=95.0, tp_price=110.0)
    trade_plan_dto = TradePlan(correlation_id=corr_id, risk_defined_signal=risk_defined_dto, position_value_quote=10000.0, position_size_asset=1.0)
    routed_plan_dto = RoutedTradePlan(correlation_id=corr_id, trade_plan=trade_plan_dto, order_type='limit', limit_price=100.0)
    
    expected_directive = DirectiveFlattener().flatten(routed_plan_dto)

    # 2. Mocks voor elke worker, nu geconfigureerd voor de .process() methode
    # --- DE FIX: Gebruik overal .process in de mocks ---
    mock_signal_generator = mocker.MagicMock(process=mocker.Mock(return_value=[signal_dto]))
    mock_signal_refiner = mocker.MagicMock(process=mocker.Mock(return_value=signal_dto))
    mock_entry_planner = mocker.MagicMock(process=mocker.Mock(return_value=entry_signal_dto))
    mock_exit_planner = mocker.MagicMock(process=mocker.Mock(return_value=risk_defined_dto))
    mock_size_planner = mocker.MagicMock(process=mocker.Mock(return_value=trade_plan_dto))
    mock_order_router = mocker.MagicMock(process=mocker.Mock(return_value=routed_plan_dto))
    mock_event_detector = mocker.MagicMock(process=mocker.Mock(return_value=[]))

    active_workers = {
        'signal_generator': [mock_signal_generator],
        'signal_refiner': [mock_signal_refiner],
        'entry_planner': mock_entry_planner,
        'exit_planner': mock_exit_planner,
        'size_planner': mock_size_planner,
        'order_router': [mock_order_router],
        'critical_event_detector': [mock_event_detector]
    }

    # --- Act (De Actie) ---
    engine = StrategyEngine(active_workers=active_workers)
    cycle_results = list(engine.run(
        trading_context=mocker.MagicMock(spec=TradingContext),
        clock=mock_clock
    ))

    # --- Assert (De Controle) ---
    assert len(cycle_results) == 1
    result = cycle_results[0]
    
    assert isinstance(result, EngineCycleResult)
    assert len(result.execution_directives) == 1
    assert len(result.critical_events) == 0
    assert result.execution_directives[0] == expected_directive

    # Valideer dat de correcte methodes zijn aangeroepen
    mock_signal_generator.process.assert_called_once()
    mock_signal_refiner.process.assert_called_once_with(signal_dto, mocker.ANY)
    mock_entry_planner.process.assert_called_once_with(signal_dto, mocker.ANY)
    mock_exit_planner.process.assert_called_once_with(entry_signal_dto, mocker.ANY)
    mock_size_planner.process.assert_called_once_with(risk_defined_dto, mocker.ANY)
    mock_order_router.process.assert_called_once_with(trade_plan_dto, mocker.ANY)
    mock_event_detector.process.assert_called_once()

--- END FILE: tests/backend/core/test_strategy_engine.py ---

--- START FILE: tests/backend/data/test_data_loader.py ---
# tests/backend/data/test_data_loader.py
"""Unit tests for the DataLoader."""

import os
import re # <-- Importeer de 're' module
from pathlib import Path
import pandas as pd
import pytest
from pytest_mock import MockerFixture

from backend.data.loader import DataLoader

def test_data_loader_successfully_loads_csv(tmp_path: Path, mocker: MockerFixture):
    """
    Tests if the DataLoader correctly reads a valid CSV file, sets the index,
    and returns a DataFrame.
    """
    # Arrange
    csv_content = "timestamp,open,high,low,close,volume\n" \
                  "2023-01-01 10:00:00,100,105,99,102,1000\n" \
                  "2023-01-01 10:01:00,102,103,101,102,500"
    
    data_file = tmp_path / "test_data.csv"
    data_file.write_text(csv_content)
    
    mock_logger = mocker.MagicMock()

    # Act
    loader = DataLoader(file_path=str(data_file), logger=mock_logger)
    df = loader.load()

    # Assert
    assert isinstance(df, pd.DataFrame)
    assert len(df) == 2
    assert isinstance(df.index, pd.DatetimeIndex)
    assert df.index.name == "timestamp"
    mock_logger.info.assert_called_with('loader.load_success')

def test_data_loader_raises_error_for_nonexistent_file(mocker: MockerFixture):
    """
    Tests if the DataLoader raises a FileNotFoundError when the file does not exist.
    This test is now platform-independent.
    """
    # Arrange
    # Bouw het pad op een platform-onafhankelijke manier.
    non_existent_path = os.path.join("path", "that", "does", "not", "exist.csv")
    mock_logger = mocker.MagicMock()

    # CORRECTIE: "Escape" de pad-string zodat de regex-engine backslashes
    # als letterlijke tekens behandelt en niet als speciale codes.
    expected_error_pattern = re.escape(non_existent_path)

    # Act & Assert
    with pytest.raises(FileNotFoundError, match=expected_error_pattern):
        DataLoader(file_path=non_existent_path, logger=mock_logger)

--- END FILE: tests/backend/data/test_data_loader.py ---

--- START FILE: tests/backend/data/test_parquet_persistor.py ---
# In bestand: tests/backend/data/test_parquet_persistor.py
"""
Unit tests for the ParquetPersistor implementation.

@layer: Tests (Backend/Data)
@dependencies: [pytest, pandas, pyarrow]
@responsibilities:
    - Verify that trades are correctly saved to a Parquet file.
    - Verify that the last timestamp is correctly retrieved from an existing file.
    - Verify that the component handles non-existent files gracefully.
"""
from pathlib import Path
import pandas as pd
import pytest

from backend.dtos.market.trade_tick import TradeTick
# De klasse die we gaan testen (bestaat nog niet)
from backend.data.persistors.parquet_persistor import ParquetPersistor
from backend.dtos.market.data_coverage import DataCoverage

@pytest.fixture
def sample_trades() -> list[TradeTick]:
    """Provides a list of sample TradeTick DTOs for testing."""
    return [
        TradeTick(price=1.0, volume=10.0, timestamp=pd.to_datetime("2023-01-01 10:00:00", utc=True), side='buy', order_type='market'),
        TradeTick(price=1.1, volume=5.0, timestamp=pd.to_datetime("2023-01-01 10:01:00", utc=True), side='sell', order_type='limit'),
        TradeTick(price=1.2, volume=12.0, timestamp=pd.to_datetime("2023-01-01 10:02:00", utc=True), side='buy', order_type='market'),
    ]

def test_save_trades_and_get_last_timestamp(tmp_path: Path, sample_trades: list[TradeTick]):
    """
    Tests the core workflow: saving trades and then retrieving the last timestamp.
    """
    # Arrange
    data_dir = tmp_path
    pair = "BTC_EUR"
    persistor = ParquetPersistor(data_dir=data_dir)
    
    # Test 1: get_last_timestamp for a non-existent dataset
    assert persistor.get_last_timestamp(pair) == 0, "Should return 0 if no dataset exists."

    # Test 2: save_trades
    persistor.save_trades(pair, sample_trades)

    # <<< DE FIX (DEEL 1): Controleer op de MAP, niet het bestand >>>
    expected_dataset_path = data_dir / pair
    assert expected_dataset_path.is_dir(), "Parquet dataset directory should be created."
    
    # <<< DE FIX (DEEL 2): Lees uit de MAP >>>
    df = pd.read_parquet(expected_dataset_path) # pyright: ignore
    assert len(df) == 3
    # Sorteren is nodig omdat de leesvolgorde van partities niet gegarandeerd is
    df.sort_values('timestamp', inplace=True)
    assert df['price'].iloc[-1] == 1.2

    # Test 3: get_last_timestamp for an existing dataset
    last_ts_ns = sample_trades[-1].timestamp.value
    assert persistor.get_last_timestamp(pair) == last_ts_ns, "Should return the latest timestamp."

def test_get_data_coverage_for_non_existent_file(tmp_path: Path):
    """
    Tests that get_data_coverage returns an empty list for a non-existent pair.
    """
    # Arrange
    persistor = ParquetPersistor(data_dir=tmp_path)

    # Act
    coverage = persistor.get_data_coverage("NON_EXISTENT_PAIR")

    # Assert
    assert coverage == []

def test_get_data_coverage_for_single_block(tmp_path: Path, sample_trades: list[TradeTick]):
    """
    Tests that get_data_coverage correctly identifies a single contiguous block.
    """
    # Arrange
    pair = "BTC_EUR"
    persistor = ParquetPersistor(data_dir=tmp_path)
    persistor.save_trades(pair, sample_trades)

    # Act
    coverage = persistor.get_data_coverage(pair)

    # Assert
    assert len(coverage) == 1
    assert isinstance(coverage[0], DataCoverage)
    assert coverage[0].start_time == sample_trades[0].timestamp
    assert coverage[0].end_time == sample_trades[-1].timestamp
    assert coverage[0].trade_count == len(sample_trades)

def test_get_data_coverage_with_gap(tmp_path: Path):
    """
    Tests that get_data_coverage correctly identifies two separate blocks of data.
    """
    # Arrange
    pair = "ETH_EUR"
    persistor = ParquetPersistor(data_dir=tmp_path)

    trades_part1 = [
        TradeTick(price=1.0, volume=10.0, timestamp=pd.to_datetime("2023-01-01 10:00:00", utc=True), side='buy', order_type='market'),
        TradeTick(price=1.1, volume=5.0, timestamp=pd.to_datetime("2023-01-01 10:01:00", utc=True), side='sell', order_type='limit'),
    ]
    
    # Een "gat" in de data van meer dan een minuut
    trades_part2 = [
        TradeTick(price=1.2, volume=12.0, timestamp=pd.to_datetime("2023-01-01 10:03:00", utc=True), side='buy', order_type='market'),
        TradeTick(price=1.3, volume=8.0, timestamp=pd.to_datetime("2023-01-01 10:04:00", utc=True), side='sell', order_type='limit'),
    ]
    
    persistor.save_trades(pair, trades_part1 + trades_part2)

    # Act
    coverage = persistor.get_data_coverage(pair)

    # Assert
    assert len(coverage) == 2, "Should detect two separate blocks of data."
    
    # Controleer Block 1
    assert coverage[0].start_time == trades_part1[0].timestamp
    assert coverage[0].end_time == trades_part1[-1].timestamp
    assert coverage[0].trade_count == 2
    
    # Controleer Block 2
    assert coverage[1].start_time == trades_part2[0].timestamp
    assert coverage[1].end_time == trades_part2[-1].timestamp
    assert coverage[1].trade_count == 2

--- END FILE: tests/backend/data/test_parquet_persistor.py ---

--- START FILE: tests/backend/data/__init__.py ---

--- END FILE: tests/backend/data/__init__.py ---

--- START FILE: tests/backend/environments/test_backtest_environment.py ---
# tests/backend/environments/test_backtest_environment.py
"""Unit tests for the BacktestEnvironment."""

import pandas as pd
from pytest_mock import MockerFixture

from backend.environments.backtest_environment import (
    BacktestEnvironment, CSVDataSource, SimulatedClock, BacktestExecutionHandler
)
from backend.core.interfaces import Tradable
from backend.config.schemas.app_schema import AppConfig
from backend.config.schemas.platform_schema import PlatformConfig, DataConfig
from backend.config.schemas.run_schema import RunBlueprint, RunDataConfig

def test_backtest_environment_initialization(mocker: MockerFixture):
    """
    Tests if the BacktestEnvironment correctly initializes its
    specialized sub-components.
    """
    # Arrange
    mock_df = pd.DataFrame({'close': [1, 2, 3]})
    mocker.patch("backend.data.loader.DataLoader.load", return_value=mock_df)
    mocker.patch("backend.data.loader.DataLoader.__init__", return_value=None)

    # --- DE FIX: Bouw een correcte, geneste mock AppConfig ---
    mock_app_config = mocker.MagicMock(spec=AppConfig)
    mock_app_config.platform = mocker.MagicMock(spec=PlatformConfig)
    mock_app_config.run = mocker.MagicMock(spec=RunBlueprint)

    # Definieer de geneste data-objecten
    mock_app_config.platform.data = mocker.MagicMock(spec=DataConfig)
    mock_app_config.run.data = mocker.MagicMock(spec=RunDataConfig)

    # Wijs de waarden toe op de juiste, geneste locatie
    mock_app_config.platform.data.source_dir = "mock_data_dir"
    mock_app_config.run.data.trading_pair = "BTC/EUR"
    mock_app_config.run.data.timeframe = "1h"

    mock_portfolio = mocker.MagicMock(spec=Tradable)

    # Act
    environment = BacktestEnvironment(
        app_config=mock_app_config,
        tradable=mock_portfolio
    )

    # Assert
    assert isinstance(environment.source, CSVDataSource)
    assert isinstance(environment.clock, SimulatedClock)
    assert isinstance(environment.handler, BacktestExecutionHandler)

--- END FILE: tests/backend/environments/test_backtest_environment.py ---

--- START FILE: tests/backend/environments/test_kraken_connector.py ---
# tests/backend/environments/api_connectors/test_kraken_connector.py
"""
Unit tests for the KrakenAPIConnector.

@layer: Tests (Backend/Environments/APIConnectors)
@dependencies: [pytest, pytest-mock, requests]
@responsibilities:
    - Verify that the connector correctly calls the Kraken API endpoints.
    - Verify that the connector correctly parses valid API responses into DTOs.
    - Verify that the connector handles API errors and invalid data gracefully.
"""
from unittest.mock import MagicMock, call
from pytest_mock import MockerFixture
import requests

import pandas as pd

from backend.environments.api_connectors.kraken_connector import KrakenAPIConnector
from backend.dtos.market.trade_tick import TradeTick
from backend.config.schemas.connectors.kraken_schema import KrakenPublicConfig, KrakenAPIRetryConfig

MOCK_KRAKEN_SUCCESS_RESPONSE = {
    "error": [],
    "result": {
        "XXBTZEUR": [
            ["60000.10", "0.1", 1617225600.1234, "b", "m", ""],
            ["60000.20", "0.05", 1617225601.5678, "s", "l", ""]
        ],
        "last": "1617225601567800000"
    }
}

MOCK_KRAKEN_OHLCV_SUCCESS_RESPONSE = {
    "error": [],
    "result": {
        "XXBTZEUR": [
            [1617225600, "60000.1", "60005.0", "59990.5", "60002.0", "60001.0", "10.5", 150],
            [1617226500, "60002.1", "60010.0", "60000.0", "60008.0", "60007.0", "12.0", 180],
        ],
        "last": 1617226500
    }
}

MOCK_KRAKEN_ERROR_RESPONSE = {
    "error": ["EQuery:Invalid pair"]
}


def test_get_historical_trades_success(mocker: MockerFixture):
    mock_logger = MagicMock()
    mock_config = KrakenPublicConfig()
    
    # Mock de eerste call met data, en de tweede zonder nieuwe data
    mock_response_page1 = MagicMock()
    mock_response_page1.json.return_value = MOCK_KRAKEN_SUCCESS_RESPONSE
    mock_response_page2 = MagicMock()
    mock_response_page2.json.return_value = {
        "error": [], 
        "result": {
            "XXBTZEUR": [["60000.20", "0.05", 1617225601.5678, "s", "l", ""]],
            "last": MOCK_KRAKEN_SUCCESS_RESPONSE["result"]["last"]
        }
    }
    mocker.patch('requests.Session.get', side_effect=[mock_response_page1, mock_response_page2])
    mocker.patch('time.sleep')

    connector = KrakenAPIConnector(logger=mock_logger, config=mock_config)
    trades = connector.get_historical_trades(pair="XXBTZEUR", since=0)

    assert len(trades) == 2
    assert trades[0].price == 60000.10
    assert trades[1].price == 60000.20


def test_get_historical_trades_handles_api_error(mocker: MockerFixture):
    mock_logger = MagicMock()
    mock_config = KrakenPublicConfig()
    
    mocker.patch('requests.Session.get', return_value=MagicMock(json=MagicMock(return_value=MOCK_KRAKEN_ERROR_RESPONSE)))

    connector = KrakenAPIConnector(logger=mock_logger, config=mock_config)
    trades = connector.get_historical_trades(pair="INVALIDPAIR", since=0)

    assert trades == []
    # FIX: De assert controleert nu de correcte, specifiekere foutmelding.
    mock_logger.error.assert_called_once_with("Kraken API error for endpoint '/Trades': ['EQuery:Invalid pair']")


def test_get_historical_trades_respects_time_window(mocker: MockerFixture):
    mock_logger = MagicMock()
    mock_config = KrakenPublicConfig()
    
    SINCE_NS = 1000 * 1_000_000_000
    UNTIL_NS = 3000 * 1_000_000_000

    # Pagina 1 bevat een valide trade en een trade die we al hebben
    mock_response_page1 = MagicMock()
    mock_response_page1.json.return_value = { "error": [], "result": { "XXBTZEUR": [
        ["1000.0", "1.0", 1000.0, "b", "m", ""], 
        ["1500.0", "1.5", 1500.0, "b", "m", ""]
    ], "last": str(1500 * 1_000_000_000) } }
    
    # Pagina 2 bevat een trade NA het venster
    mock_response_page2 = MagicMock()
    mock_response_page2.json.return_value = { "error": [], "result": { "XXBTZEUR": [["3500.0", "3.5", 3500.0, "s", "l", ""]], "last": str(3500 * 1_000_000_000) } }

    mock_get = mocker.patch(
        'requests.Session.get', side_effect=[mock_response_page1, mock_response_page2])
    mocker.patch('time.sleep')

    connector = KrakenAPIConnector(logger=mock_logger, config=mock_config)
    trades = connector.get_historical_trades(pair="XXBTZEUR", since=SINCE_NS, until=UNTIL_NS)

    assert len(trades) == 1
    assert trades[0].price == 1500.0
    assert mock_get.call_count == 2


def test_get_historical_trades_retries_on_network_error(mocker: MockerFixture):
    mock_logger = MagicMock()
    mock_config = KrakenPublicConfig(
        retries=KrakenAPIRetryConfig(max_attempts=3, delay_seconds=1))

    mock_success_response = MagicMock()
    mock_success_response.json.return_value = MOCK_KRAKEN_SUCCESS_RESPONSE
    
    mocked_get = mocker.patch('requests.Session.get', side_effect=[
        requests.RequestException("First network error"),
        requests.RequestException("Second network error"),
        mock_success_response,
        MagicMock(json=MagicMock(
            return_value={"error": [],
                          "result": {"last": MOCK_KRAKEN_SUCCESS_RESPONSE["result"]["last"]}}))
    ])
    mocker.patch('time.sleep')

    connector = KrakenAPIConnector(logger=mock_logger, config=mock_config)
    trades = connector.get_historical_trades(pair="XXBTZEUR", since=0)

    assert mocked_get.call_count == 4
    assert len(trades) == 2
    assert mock_logger.error.call_count == 2

def test_get_historical_ohlcv_success(mocker: MockerFixture):
    """
    Tests if the connector correctly fetches and parses OHLCV data into a DataFrame.
    """
    # Arrange
    mock_logger = MagicMock()
    mock_config = KrakenPublicConfig()

    mock_response = MagicMock()
    mock_response.json.return_value = MOCK_KRAKEN_OHLCV_SUCCESS_RESPONSE
    mocker.patch('requests.Session.get', return_value=mock_response)

    connector = KrakenAPIConnector(logger=mock_logger, config=mock_config)

    # Act
    df = connector.get_historical_ohlcv(pair="XXBTZEUR", timeframe="15m", since=0)

    # Assert
    assert isinstance(df, pd.DataFrame)
    assert len(df) == 2
    assert list(df.columns) == ["open", "high", "low", "close", "vwap", "volume", "count"]
    assert isinstance(df.index, pd.DatetimeIndex)
    assert df.index.name == "timestamp"
    assert df.iloc[0]['open'] == 60000.1
    assert df.iloc[1]['close'] == 60008.0

--- END FILE: tests/backend/environments/test_kraken_connector.py ---

--- START FILE: tests/backend/environments/__init__.py ---

--- END FILE: tests/backend/environments/__init__.py ---

--- START FILE: tests/services/test_data_command_service.py ---
# In bestand: tests/services/test_data_command_service.py
"""
Unit tests for the DataCommandService's interactors.
"""
from unittest.mock import MagicMock, call
import pandas as pd
import pytest
from pytest_mock import MockerFixture

from backend.core.interfaces.connectors import IAPIConnector
from backend.core.interfaces.persistors import IDataPersistor
from backend.config.schemas.platform_schema import DataCollectionLimits
from backend.dtos.market.trade_tick import TradeTick
from backend.dtos.commands.fetch_period_command import FetchPeriodCommand

# De klasse die we gaan testen
from services.data_command_service import DataCommandService

def test_fetch_period_works_backward_in_chunks(mocker: MockerFixture):
    """
    Tests the happy path for fetch_period: fetching a valid period backward
    in daily, sequential chunks.
    """
    # --- Arrange ---
    # Request om 2 dagen aan historie op te halen
    command = FetchPeriodCommand(
        pair="BTC/EUR",
        start_date=pd.to_datetime("2025-10-05", utc=True)
    )

    MOCK_NOW = pd.to_datetime("2025-10-06 15:00:00", utc=True)
    mocker.patch('pandas.Timestamp.utcnow', return_value=MOCK_NOW)

    mock_persistor = mocker.MagicMock(spec=IDataPersistor)
    mock_connector = mocker.MagicMock(spec=IAPIConnector)
    mock_logger = mocker.MagicMock()
    # Stel een ruime limiet in die niet wordt overschreden
    mock_limits = DataCollectionLimits(max_history_days=365)

    # Definieer de data die de connector zal "teruggeven"
    trades_today = [TradeTick(price=1.0, volume=1, timestamp=MOCK_NOW, side='buy', order_type='market')]
    trades_yesterday = [TradeTick(price=2.0, volume=2, timestamp=MOCK_NOW - pd.Timedelta(days=1), side='buy', order_type='market')]

    def get_trades_side_effect(pair, since, until=None):
        if since == int(pd.to_datetime("2025-10-06 00:00:00", utc=True).value): return trades_today
        if since == int(pd.to_datetime("2025-10-05 00:00:00", utc=True).value): return trades_yesterday
        return []
    mock_connector.get_historical_trades.side_effect = get_trades_side_effect

    # --- Act ---
    service = DataCommandService(persistor=mock_persistor, connector=mock_connector, logger=mock_logger, limits=mock_limits)
    service.fetch_period(command=command)

    # --- Assert ---
    # De connector moet exact 2 keer zijn aangeroepen (voor 6 en 5 oktober)
    assert mock_connector.get_historical_trades.call_count == 2
    calls = mock_connector.get_historical_trades.call_args_list
    assert calls[0] == call(pair=command.pair, since=int(pd.to_datetime("2025-10-06 00:00:00", utc=True).value), until=int(MOCK_NOW.value))
    assert calls[1] == call(pair=command.pair, since=int(pd.to_datetime("2025-10-05 00:00:00", utc=True).value), until=int(pd.to_datetime("2025-10-05 23:59:59.999999999", utc=True).value))


def test_fetch_period_raises_error_if_period_exceeds_max_limit(mocker: MockerFixture):
    """
    Tests that fetch_period correctly validates the requested period against
    the centrally configured safety limit and raises a ValueError if exceeded.
    """
    # --- Arrange ---
    # Stel een strikte limiet in van slechts 30 dagen
    mock_limits = DataCollectionLimits(max_history_days=30)
    
    # Een request voor 31 dagen aan data, wat de limiet overschrijdt
    start_date_too_far = pd.Timestamp.utcnow() - pd.Timedelta(days=31)
    command = FetchPeriodCommand(
        pair="BTC/EUR",
        start_date=start_date_too_far
    )

    mock_persistor = mocker.MagicMock(spec=IDataPersistor)
    mock_connector = mocker.MagicMock(spec=IAPIConnector)
    mock_logger = mocker.MagicMock()

    # --- Act & Assert ---
    service = DataCommandService(persistor=mock_persistor, connector=mock_connector, logger=mock_logger, limits=mock_limits)
    
    # We verwachten dat de methode een ValueError gooit met een duidelijke melding
    with pytest.raises(ValueError, match="Requested history of 32 days exceeds the maximum limit of 30 days."):
        service.fetch_period(command=command)

    # Verifieer dat er GEEN calls zijn gedaan naar de connector of persistor
    mock_connector.get_historical_trades.assert_not_called()

--- END FILE: tests/services/test_data_command_service.py ---

--- START FILE: tests/services/test_strategy_operator.py ---
# tests/services/test_strategy_operator.py
"""
Unit tests for the refactored StrategyOperator service.
"""
from pytest_mock import MockerFixture

# De klasse die we gaan testen
from services.strategy_operator import StrategyOperator

# De klassen die we gaan mocken
from backend.config.schemas.app_schema import AppConfig

def test_strategy_operator_run_orchestration(mocker: MockerFixture):
    """
    Tests if the main `run` method correctly calls the private helper
    methods in the correct order, confirming its role as a pure orchestrator.
    """
    # --- Arrange ---
    # Mock the private helper methods that contain the actual logic
    mock_prepare_components = mocker.patch(
        'services.strategy_operator.StrategyOperator._prepare_components'
    )
    mock_prepare_data = mocker.patch('services.strategy_operator.StrategyOperator._prepare_data')
    mock_run_cycle = mocker.patch(
        'services.strategy_operator.StrategyOperator._run_operational_cycle'
    )

    # Create dummy config and logger for instantiation
    mock_app_config = mocker.MagicMock(spec=AppConfig)
    mock_logger = mocker.MagicMock()

    # --- Act ---
    operator = StrategyOperator(app_config=mock_app_config, logger=mock_logger)
    operator.run()

    # --- Assert ---
    # Verify that the main orchestration methods were called exactly once
    mock_prepare_components.assert_called_once()
    mock_prepare_data.assert_called_once()
    mock_run_cycle.assert_called_once()

--- END FILE: tests/services/test_strategy_operator.py ---

--- START FILE: tools/bootstrap_v2.py ---
# bootstrap_v2.py
"""
This script generates the complete directory and file skeleton for the S1mpleTrader V2 architecture.

It creates all necessary directories and populates them with initial files,
including boilerplate content like file headers and class definitions, adhering
to the project's coding standards.

@layer: Tool
@dependencies:
    - os
    - pathlib
@responsibilities:
    - Creates the main directory structure for the V2 application.
    - Generates placeholder files with correct headers and initial content.
    - Establishes a clean, consistent starting point for V2 development.
@inputs: None
@outputs: A complete directory structure on the filesystem.
"""

import os
from pathlib import Path

# --- Configuration ---
ROOT_DIR = Path("S1mpleTrader_V2")

# The full directory and file structure based on our architecture design
STRUCTURE = {
    "config": {
        "runs": {"mss_fvg_strategy.yaml": "# V2 Strategy Blueprint: MSS with FVG Entry"},
        "optimizations": {"optimize_atr_params.yaml": "# V2 Optimization Blueprint: Tune ATR Exit Parameters"},
        "variants": {"robustness_test.yaml": "# V2 Variant Test: Test strategy across multiple markets"},
        "overrides": {"use_eth_pair.yaml": "# V2 Override: Switch trading pair to ETH/EUR"},
        "index.yaml": "# Central index mapping short names to blueprint file paths",
        "platform.yaml": "# Global platform settings: portfolio, logging, etc.",
    },
    "plugins": {
        "regime_filters": {
            "adx_trend_filter": {
                "__init__.py": None,
                "plugin_manifest.yaml": "name: adx_trend_filter\ntype: regime_filter\n...",
                "worker.py": "AdxTrendFilter",
                "schema.py": "AdxTrendFilterParams",
                "tests": {"test_worker.py": "TestAdxTrendFilter"},
            }
        },
        "structural_context": {
            "market_structure_detector": {
                "__init__.py": None,
                "plugin_manifest.yaml": "name: market_structure_detector\ntype: structural_context\n...",
                "worker.py": "MarketStructureDetector",
                "schema.py": "MarketStructureDetectorParams",
                "tests": {"test_worker.py": "TestMarketStructureDetector"},
            }
        },
        "signal_generators": {
            "fvg_entry_detector": {
                "__init__.py": None,
                "plugin_manifest.yaml": "name: fvg_entry_detector\ntype: signal_generator\n...",
                "worker.py": "FvgEntryDetector",
                "schema.py": "FvgEntryDetectorParams",
                "tests": {"test_worker.py": "TestFvgEntryDetector"},
            }
        },
        "signal_refiners": {
            "volume_spike_refiner": {
                "__init__.py": None,
                "plugin_manifest.yaml": "name: volume_spike_refiner\ntype: signal_refiner\n...",
                "worker.py": "VolumeSpikeRefiner",
                "schema.py": "VolumeSpikeRefinerParams",
                "tests": {"test_worker.py": "TestVolumeSpikeRefiner"},
            }
        },
        "trade_constructors": {
            "liquidity_target_exit": {
                "__init__.py": None,
                "plugin_manifest.yaml": "name: liquidity_target_exit\ntype: trade_constructor\n...", # This would likely be an exit planner
                "worker.py": "LiquidityTargetExitPlanner",
                "schema.py": "LiquidityTargetExitParams",
                "tests": {"test_worker.py": "TestLiquidityTargetExitPlanner"},
            }
        },
        "portfolio_overlays": {
            "max_drawdown_overlay": {
                "__init__.py": None,
                "plugin_manifest.yaml": "name: max_drawdown_overlay\ntype: portfolio_overlay\n...",
                "worker.py": "MaxDrawdownOverlay",
                "schema.py": "MaxDrawdownOverlayParams",
                "tests": {"test_worker.py": "TestMaxDrawdownOverlay"},
            }
        },
    },
    "backend": {
        "config": {"schemas": {"__init__.py": None, "app_schema.py": "AppConfig", "blueprint_schema.py": "BlueprintConfig"}},
        "assembly": {
            "__init__.py": None,
            "plugin_registry.py": "PluginRegistry",
            "worker_builder.py": "WorkerBuilder",
            "context_pipeline_runner.py": "ContextPipelineRunner",
        },
        "core": {
            "__init__.py": None,
            "portfolio.py": "Portfolio",
            "execution.py": "ExecutionHandler",
            "performance_analyzer.py": "PerformanceAnalyzer",
            "interfaces.py": "# Contains abstract base classes like Tradable",
            "constants.py": "# Application-wide constants",
        },
        "environments": {
            "__init__.py": None,
            "base_environment.py": "ExecutionEnvironment",
            "backtest_environment.py": "BacktestEnvironment",
            "paper_environment.py": "PaperTradeEnvironment",
            "live_environment.py": "LiveTradeEnvironment",
        },
        "dtos": {
            "__init__.py": None,
            "signal.py": "# Signal DTO dataclass",
            "trade.py": "# Trade DTO dataclass",
            "closed_trade.py": "# ClosedTrade DTO dataclass",
            "backtest_result.py": "# BacktestResult DTO dataclass",
        },
        "utils": {
            "__init__.py": None,
            "app_logger.py": "LogEnricher",
            "translator.py": "Translator",
            "data_utils.py": "# Utility functions for data manipulation",
        },
    },
    "services": {
        "__init__.py": None,
        "strategy_operator.py": "StrategyOperator",
        "optimization_service.py": "OptimizationService",
        "variant_test_service.py": "VariantTestService",
        "parallel_run_service.py": "ParallelRunService",
        "api_services": {
            "__init__.py": None,
            "plugin_query_service.py": "PluginQueryService",
            "visualization_service.py": "VisualizationService",
        },
    },
    "frontends": {
        "web": {
            "api": {
                "__init__.py": None,
                "main.py": "# FastAPI application entry point",
                "routers": {
                    "__init__.py": None,
                    "plugins_router.py": "# API endpoints for plugins",
                    "backtest_router.py": "# API endpoints for running backtests",
                },
            },
            "ui": {
                "src": {"components": {}, "services": {}, "App.tsx": "// Main React/TypeScript component"},
                "package.json": '{\n  "name": "s1mpletrader-ui",\n  "version": "0.1.0"\n}',
            },
        },
        "cli": {
            "presenters": {"optimization_presenter.py": "OptimizationPresenter"},
            "reporters": {"cli_reporter.py": "CliReporter"},
        },
    },
    "locales": {"en.yaml": "app:\n  start: \"S1mpleTrader is starting...\"", "nl.yaml": "app:\n  start: \"S1mpleTrader wordt gestart...\""},
    "tools": {"generate_structure.py": None, "plugin_creator.py": "# A helper script to bootstrap a new plugin directory"},
    "source_data": {"BTC_EUR_15m.csv": "timestamp,open,high,low,close,volume\n..."},
    "results": {
        "20250924_213000_mss_fvg_strategy": {
            "run_config.yaml": None,
            "result_trades.csv": None,
            "result_metrics.yaml": None,
            "run.log.json": None,
        }
    },
    "run_web.py": "# Entrypoint: Starts the Web UI and API",
    "run_supervisor.py": "# Entrypoint: Starts the live trading supervisor",
    "run_backtest_cli.py": "# Entrypoint: For automated (headless) runs",
    "pyproject.toml": '[tool.poetry]\nname = "s1mpletrader-v2"\nversion = "2.0.0"\ndescription = ""\nauthors = ["Your Name <you@example.com>"]',
}

def create_py_content(file_path_str: str, class_name: str) -> str:
    """Generates standard Python file content based on coding standards."""
    return f'# {file_path_str}\n"""\nDocstring for {os.path.basename(file_path_str)}.\n\n@layer: TODO\n@dependencies: TODO\n@responsibilities: TODO\n"""\n\nclass {class_name}:\n    """Docstring for {class_name}."""\n    pass\n'

def create_file_content(file_path: Path, content_instruction: str) -> str:
    """Creates the appropriate boilerplate content for a given file type."""
    file_path_str = str(file_path.relative_to(ROOT_DIR)).replace("\\", "/")
    
    if content_instruction and file_path.suffix == ".py":
        if content_instruction.startswith("#"): # It's a comment, not a class name
             return f'# {file_path_str}\n"""\n{content_instruction[2:]}\n"""\n'
        return create_py_content(file_path_str, content_instruction)
    
    if file_path.name == "__init__.py":
        return "" # Empty init file
        
    if content_instruction and not content_instruction.startswith("#"):
        return content_instruction
    
    return f"# Placeholder for {file_path_str}\n"


def build_structure(current_path: Path, structure_dict: dict):
    """Recursively builds the directory and file structure."""
    current_path.mkdir(exist_ok=True)
    for name, content in structure_dict.items():
        new_path = current_path / name
        if isinstance(content, dict):
            build_structure(new_path, content)
        else:
            print(f"Creating file: {new_path}")
            file_content = create_file_content(new_path, content)
            with open(new_path, "w", encoding="utf-8") as f:
                f.write(file_content)

if __name__ == "__main__":
    if ROOT_DIR.exists():
        print(f"Directory '{ROOT_DIR}' already exists. Please remove it first to start fresh.")
    else:
        print(f"--- 🚀 Bootstrapping S1mpleTrader V2 Structure in '{ROOT_DIR}' ---")
        build_structure(ROOT_DIR, STRUCTURE)
        print("\n--- ✅ Structure created successfully! ---")

--- END FILE: tools/bootstrap_v2.py ---

--- START FILE: tools/combine_docs.py ---
import os

def merge_markdown_files(input_folder, output_file):
    with open(output_file, 'w', encoding='utf-8') as outfile:
        for filename in os.listdir(input_folder):
            if filename.endswith(".md"):
                file_path = os.path.join(input_folder, filename)
                with open(file_path, 'r', encoding='utf-8') as infile:
                    outfile.write(f"# {filename}\n\n")  # optioneel: titel toevoegen
                    outfile.write(infile.read())
                    outfile.write("\n\n---\n\n")  # scheiding tussen bestanden

if __name__ == "__main__":
    input_folder = "./docs/system"       # vervang door jouw map
    output_file = "docs123.md" # naam van het resultaat
    merge_markdown_files(input_folder, output_file)
    print(f"Alle bestanden samengevoegd in: {output_file}")
--- END FILE: tools/combine_docs.py ---

--- START FILE: tools/generate_structure.py ---
# tools/generate_structure.py
"""
Generates a text file representing the project's directory and file structure.

This script scans the project from the root directory, respects the rules
in the .gitignore file, and outputs a clean, indented tree structure to a
specified text file.

@layer: Tool
"""

# 1. Standard Library Imports
import os
import fnmatch
from pathlib import Path

# --- CONFIGURATION ---
PROJECT_ROOT = Path(__file__).resolve().parent.parent
OUTPUT_PATH = PROJECT_ROOT / "docs" / "folder_file_structure.txt"
GITIGNORE_PATH = PROJECT_ROOT / ".gitignore"

# Default patterns to always ignore, even if not in .gitignore
DEFAULT_IGNORE = {".git", ".vscode", "*.pyc", "*__pycache__*", ".DS_Store"}


def read_gitignore() -> set:
    """
    Reads and parses the .gitignore file.

    Returns:
        set: A set of patterns to be ignored.
    """
    if not GITIGNORE_PATH.is_file():
        print("Warning: .gitignore file not found at project root.")
        return set()

    with open(GITIGNORE_PATH, 'r', encoding='utf-8') as f:
        patterns = {
            line.strip() for line in f
            if line.strip() and not line.startswith('#')
        }
    return patterns


def should_ignore(path: Path, ignore_patterns: set) -> bool:
    """
    Checks if a path should be ignored using a simplified interpretation
    of .gitignore-style patterns.

    @inputs:
        path (Path): The path object relative to the project root.
        ignore_patterns (set): A set of patterns from .gitignore and defaults.

    @outputs:
        bool: True if the path should be ignored, False otherwise.
    """
    for pattern in ignore_patterns:
        # Handle directory-only patterns (e.g., 'build/', '__pycache__/')
        if pattern.endswith('/'):
            if pattern.rstrip('/') in path.parts:
                return True
        # Handle file/general patterns (e.g., '*.pyc', '.DS_Store')
        elif fnmatch.fnmatch(path.name, pattern):
            return True
    return False


def generate_structure(directory: Path, ignore_patterns: set, prefix: str = "") -> str:
    """
    Recursively generates the directory structure string.
    """
    structure = ""
    # Create a sorted list of items to process
    try:
        items = sorted(list(directory.iterdir()), key=lambda p: (p.is_file(), p.name.lower()))
    except FileNotFoundError:
        return "" # Directory might have been deleted mid-run

    # Create a list of non-ignored items to correctly determine the last element
    valid_items = [p for p in items if not should_ignore(p.relative_to(PROJECT_ROOT), ignore_patterns)]

    for i, path in enumerate(valid_items):
        is_last = (i == len(valid_items) - 1)
        connector = "└── " if is_last else "├── "
        structure += f"{prefix}{connector}{path.name}\n"

        if path.is_dir():
            extension = "    " if is_last else "│   "
            structure += generate_structure(
                path, ignore_patterns, prefix + extension
            )

    return structure


def main():
    """Main function to generate and save the structure file."""
    print("Generating project structure...")

    ignore_patterns = DEFAULT_IGNORE.union(read_gitignore())
    project_name = PROJECT_ROOT.name
    tree_string = generate_structure(PROJECT_ROOT, ignore_patterns)
    full_output = f"{project_name}/\n{tree_string}"

    OUTPUT_PATH.parent.mkdir(parents=True, exist_ok=True)

    with open(OUTPUT_PATH, "w", encoding="utf-8") as f:
        f.write(full_output)

    print(f"✅ Project structure saved to: {OUTPUT_PATH}")


if __name__ == "__main__":
    main()
--- END FILE: tools/generate_structure.py ---

--- START FILE: tools/__init__.py ---

--- END FILE: tools/__init__.py ---

